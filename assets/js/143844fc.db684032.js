"use strict";(self.webpackChunkdocusaurus_site=self.webpackChunkdocusaurus_site||[]).push([[8884],{5653:(e,s,t)=>{t.r(s),t.d(s,{assets:()=>c,contentTitle:()=>i,default:()=>o,frontMatter:()=>r,metadata:()=>l,toc:()=>m});var a=t(4848),n=t(8453);const r={slug:"intro-to-qnn",title:"Introduction to Quantum Neural Networks",authors:"eason",tags:["intermediate","quantum","qnn","pytorch","tutorial"]},i=void 0,l={permalink:"/blog/intro-to-qnn",source:"@site/blog/2024-2-9-intro-to-qnn/index.md",title:"Introduction to Quantum Neural Networks",description:"Make a Quantum Neural Network (QNN) in PyTorch.",date:"2024-02-09T00:00:00.000Z",tags:[{label:"intermediate",permalink:"/blog/tags/intermediate"},{label:"quantum",permalink:"/blog/tags/quantum"},{label:"qnn",permalink:"/blog/tags/qnn"},{label:"pytorch",permalink:"/blog/tags/pytorch"},{label:"tutorial",permalink:"/blog/tags/tutorial"}],readingTime:9.51,hasTruncateMarker:!0,authors:[{name:"Eason Xie",title:"Website Owner",url:"https://easonoob.github.io",imageURL:"https://avatars.githubusercontent.com/u/100521878?v=4",key:"eason"}],frontMatter:{slug:"intro-to-qnn",title:"Introduction to Quantum Neural Networks",authors:"eason",tags:["intermediate","quantum","qnn","pytorch","tutorial"]},unlisted:!1,nextItem:{title:"Introduction to Quantum Computing",permalink:"/blog/intro-to-quantum-computing"}},c={authorsImageUrls:[void 0]},m=[{value:"Prerequisites",id:"prerequisites",level:2},{value:"Simulation",id:"simulation",level:2},{value:"Define Gate Matrices",id:"define-gate-matrices",level:3},{value:"Angle Encoding",id:"angle-encoding",level:3},{value:"Measurement",id:"measurement",level:3},{value:"Model",id:"model",level:3},{value:"Dataset",id:"dataset",level:3},{value:"Training Loop",id:"training-loop",level:3}];function h(e){const s={a:"a",annotation:"annotation",code:"code",h2:"h2",h3:"h3",li:"li",math:"math",mi:"mi",mn:"mn",mo:"mo",mrow:"mrow",msup:"msup",mtext:"mtext",munder:"munder",p:"p",path:"path",pre:"pre",semantics:"semantics",span:"span",svg:"svg",ul:"ul",...(0,n.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(s.p,{children:"Make a Quantum Neural Network (QNN) in PyTorch."}),"\n",(0,a.jsxs)(s.p,{children:["In the ",(0,a.jsx)(s.a,{href:"https://easonoob.github.io/blog/intro-to-quantum-computing",children:"last blog post"}),", we talked about the very basics of quantum computing, and tried a few examples with qiskit. In this post, we will try to build a QNN completely with PyTorch only, without quantum simulation / machine learning libraries like PennyLane or TorchQuantum, so we can know what really happens inside the simulation. Let's get started!"]}),"\n",(0,a.jsx)(s.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,a.jsxs)(s.ul,{children:["\n",(0,a.jsx)(s.li,{children:"Read and Understand the last post"}),"\n",(0,a.jsx)(s.li,{children:"Basic Python & PyTorch"}),"\n"]}),"\n",(0,a.jsx)(s.h2,{id:"simulation",children:"Simulation"}),"\n",(0,a.jsx)(s.p,{children:"Import necessary libraries:"}),"\n",(0,a.jsx)(s.pre,{children:(0,a.jsx)(s.code,{className:"language-python",children:"import torch, torch.nn as nn, torch.nn.functional as F\r\nfrom typing import Union, List\r\nimport numpy as np\r\nfrom torchvision import datasets, transforms\r\nfrom torch.utils.data import DataLoader\r\nimport matplotlib.pyplot as plt\n"})}),"\n",(0,a.jsxs)(s.p,{children:["We know a quantum state vector of a single qubit can be represented as ",(0,a.jsxs)(s.span,{className:"katex",children:[(0,a.jsx)(s.span,{className:"katex-mathml",children:(0,a.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,a.jsxs)(s.semantics,{children:[(0,a.jsxs)(s.mrow,{children:[(0,a.jsx)(s.mi,{mathvariant:"normal",children:"\u2223"}),(0,a.jsx)(s.mi,{children:"\u03c8"}),(0,a.jsx)(s.mo,{stretchy:"false",children:"\u27e9"}),(0,a.jsx)(s.mo,{children:"="}),(0,a.jsx)(s.mi,{children:"\u03b1"}),(0,a.jsx)(s.mi,{mathvariant:"normal",children:"\u2223"}),(0,a.jsx)(s.mn,{children:"0"}),(0,a.jsx)(s.mo,{stretchy:"false",children:"\u27e9"}),(0,a.jsx)(s.mo,{children:"+"}),(0,a.jsx)(s.mi,{children:"\u03b2"}),(0,a.jsx)(s.mi,{mathvariant:"normal",children:"\u2223"}),(0,a.jsx)(s.mn,{children:"1"}),(0,a.jsx)(s.mo,{stretchy:"false",children:"\u27e9"})]}),(0,a.jsx)(s.annotation,{encoding:"application/x-tex",children:"|\\psi\\rangle = \\alpha |0\\rangle + \\beta |1\\rangle"})]})})}),(0,a.jsxs)(s.span,{className:"katex-html","aria-hidden":"true",children:[(0,a.jsxs)(s.span,{className:"base",children:[(0,a.jsx)(s.span,{className:"strut",style:{height:"1em",verticalAlign:"-0.25em"}}),(0,a.jsx)(s.span,{className:"mord",children:"\u2223"}),(0,a.jsx)(s.span,{className:"mord mathnormal",style:{marginRight:"0.03588em"},children:"\u03c8"}),(0,a.jsx)(s.span,{className:"mclose",children:"\u27e9"}),(0,a.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2778em"}}),(0,a.jsx)(s.span,{className:"mrel",children:"="}),(0,a.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2778em"}})]}),(0,a.jsxs)(s.span,{className:"base",children:[(0,a.jsx)(s.span,{className:"strut",style:{height:"1em",verticalAlign:"-0.25em"}}),(0,a.jsx)(s.span,{className:"mord mathnormal",style:{marginRight:"0.0037em"},children:"\u03b1"}),(0,a.jsx)(s.span,{className:"mord",children:"\u22230"}),(0,a.jsx)(s.span,{className:"mclose",children:"\u27e9"}),(0,a.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}}),(0,a.jsx)(s.span,{className:"mbin",children:"+"}),(0,a.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}})]}),(0,a.jsxs)(s.span,{className:"base",children:[(0,a.jsx)(s.span,{className:"strut",style:{height:"1em",verticalAlign:"-0.25em"}}),(0,a.jsx)(s.span,{className:"mord mathnormal",style:{marginRight:"0.05278em"},children:"\u03b2"}),(0,a.jsx)(s.span,{className:"mord",children:"\u22231"}),(0,a.jsx)(s.span,{className:"mclose",children:"\u27e9"})]})]})]}),", and numerous qubits can be combined into a large state of ",(0,a.jsxs)(s.span,{className:"katex",children:[(0,a.jsx)(s.span,{className:"katex-mathml",children:(0,a.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,a.jsxs)(s.semantics,{children:[(0,a.jsx)(s.mrow,{children:(0,a.jsxs)(s.msup,{children:[(0,a.jsx)(s.mn,{children:"2"}),(0,a.jsx)(s.mi,{children:"n"})]})}),(0,a.jsx)(s.annotation,{encoding:"application/x-tex",children:"2^n"})]})})}),(0,a.jsx)(s.span,{className:"katex-html","aria-hidden":"true",children:(0,a.jsxs)(s.span,{className:"base",children:[(0,a.jsx)(s.span,{className:"strut",style:{height:"0.6644em"}}),(0,a.jsxs)(s.span,{className:"mord",children:[(0,a.jsx)(s.span,{className:"mord",children:"2"}),(0,a.jsx)(s.span,{className:"msupsub",children:(0,a.jsx)(s.span,{className:"vlist-t",children:(0,a.jsx)(s.span,{className:"vlist-r",children:(0,a.jsx)(s.span,{className:"vlist",style:{height:"0.6644em"},children:(0,a.jsxs)(s.span,{style:{top:"-3.063em",marginRight:"0.05em"},children:[(0,a.jsx)(s.span,{className:"pstrut",style:{height:"2.7em"}}),(0,a.jsx)(s.span,{className:"sizing reset-size6 size3 mtight",children:(0,a.jsx)(s.span,{className:"mord mathnormal mtight",children:"n"})})]})})})})})]})]})})]})," propabilities. For the simulation, we will create a single large state vector ",(0,a.jsxs)(s.span,{className:"katex",children:[(0,a.jsx)(s.span,{className:"katex-mathml",children:(0,a.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,a.jsxs)(s.semantics,{children:[(0,a.jsx)(s.mrow,{children:(0,a.jsx)(s.mi,{children:"h"})}),(0,a.jsx)(s.annotation,{encoding:"application/x-tex",children:"h"})]})})}),(0,a.jsx)(s.span,{className:"katex-html","aria-hidden":"true",children:(0,a.jsxs)(s.span,{className:"base",children:[(0,a.jsx)(s.span,{className:"strut",style:{height:"0.6944em"}}),(0,a.jsx)(s.span,{className:"mord mathnormal",children:"h"})]})})]})," which stores all qubits in the system (not very efficient but this is the easiest) with ",(0,a.jsxs)(s.span,{className:"katex",children:[(0,a.jsx)(s.span,{className:"katex-mathml",children:(0,a.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,a.jsxs)(s.semantics,{children:[(0,a.jsxs)(s.mrow,{children:[(0,a.jsx)(s.mi,{children:"h"}),(0,a.jsx)(s.mo,{children:"\u2208"}),(0,a.jsxs)(s.msup,{children:[(0,a.jsx)(s.mi,{mathvariant:"double-struck",children:"R"}),(0,a.jsxs)(s.mrow,{children:[(0,a.jsx)(s.mi,{children:"b"}),(0,a.jsx)(s.mo,{children:"\xd7"}),(0,a.jsxs)(s.munder,{children:[(0,a.jsxs)(s.munder,{children:[(0,a.jsxs)(s.mrow,{children:[(0,a.jsx)(s.mn,{children:"2"}),(0,a.jsx)(s.mo,{children:"\xd7"}),(0,a.jsx)(s.mn,{children:"2"}),(0,a.jsx)(s.mo,{children:"\xd7"}),(0,a.jsx)(s.mo,{children:"\u22ef"}),(0,a.jsx)(s.mo,{children:"\xd7"}),(0,a.jsx)(s.mn,{children:"2"})]}),(0,a.jsx)(s.mo,{stretchy:"true",children:"\u23df"})]}),(0,a.jsxs)(s.mrow,{children:[(0,a.jsx)(s.mi,{children:"n"}),(0,a.jsx)(s.mtext,{children:"\xa0times"})]})]})]})]})]}),(0,a.jsx)(s.annotation,{encoding:"application/x-tex",children:"h \\in \\mathbb{R}^{b \\times \\underbrace{2 \\times 2 \\times \\cdots \\times 2}_{n \\text{ times}}}"})]})})}),(0,a.jsxs)(s.span,{className:"katex-html","aria-hidden":"true",children:[(0,a.jsxs)(s.span,{className:"base",children:[(0,a.jsx)(s.span,{className:"strut",style:{height:"0.7335em",verticalAlign:"-0.0391em"}}),(0,a.jsx)(s.span,{className:"mord mathnormal",children:"h"}),(0,a.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2778em"}}),(0,a.jsx)(s.span,{className:"mrel",children:"\u2208"}),(0,a.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2778em"}})]}),(0,a.jsxs)(s.span,{className:"base",children:[(0,a.jsx)(s.span,{className:"strut",style:{height:"1.5797em"}}),(0,a.jsxs)(s.span,{className:"mord",children:[(0,a.jsx)(s.span,{className:"mord mathbb",children:"R"}),(0,a.jsx)(s.span,{className:"msupsub",children:(0,a.jsx)(s.span,{className:"vlist-t",children:(0,a.jsx)(s.span,{className:"vlist-r",children:(0,a.jsx)(s.span,{className:"vlist",style:{height:"1.5797em"},children:(0,a.jsxs)(s.span,{style:{top:"-4.0936em",marginRight:"0.05em"},children:[(0,a.jsx)(s.span,{className:"pstrut",style:{height:"3em"}}),(0,a.jsx)(s.span,{className:"sizing reset-size6 size3 mtight",children:(0,a.jsxs)(s.span,{className:"mord mtight",children:[(0,a.jsx)(s.span,{className:"mord mathnormal mtight",children:"b"}),(0,a.jsx)(s.span,{className:"mbin mtight",children:"\xd7"}),(0,a.jsx)(s.span,{className:"mord munder mtight",children:(0,a.jsxs)(s.span,{className:"vlist-t vlist-t2",children:[(0,a.jsxs)(s.span,{className:"vlist-r",children:[(0,a.jsxs)(s.span,{className:"vlist",style:{height:"0.6444em"},children:[(0,a.jsxs)(s.span,{style:{top:"-1.5916em"},children:[(0,a.jsx)(s.span,{className:"pstrut",style:{height:"3em"}}),(0,a.jsx)(s.span,{className:"sizing reset-size3 size1 mtight",children:(0,a.jsxs)(s.span,{className:"mord mtight",children:[(0,a.jsx)(s.span,{className:"mord mathnormal mtight",children:"n"}),(0,a.jsx)(s.span,{className:"mord text mtight",children:(0,a.jsx)(s.span,{className:"mord mtight",children:"\xa0times"})})]})})]}),(0,a.jsxs)(s.span,{style:{top:"-3em"},children:[(0,a.jsx)(s.span,{className:"pstrut",style:{height:"3em"}}),(0,a.jsx)(s.span,{className:"mord munder mtight",children:(0,a.jsxs)(s.span,{className:"vlist-t vlist-t2",children:[(0,a.jsxs)(s.span,{className:"vlist-r",children:[(0,a.jsxs)(s.span,{className:"vlist",style:{height:"0.6444em"},children:[(0,a.jsxs)(s.span,{className:"svg-align",style:{top:"-2.2687em"},children:[(0,a.jsx)(s.span,{className:"pstrut",style:{height:"3em"}}),(0,a.jsxs)(s.span,{className:"stretchy mtight",style:{height:"0.548em",minWidth:"1.6em"},children:[(0,a.jsx)(s.span,{className:"brace-left mtight",style:{height:"0.548em"},children:(0,a.jsx)(s.svg,{xmlns:"http://www.w3.org/2000/svg",width:"400em",height:"0.548em",viewBox:"0 0 400000 548",preserveAspectRatio:"xMinYMin slice",children:(0,a.jsx)(s.path,{d:"M0 6l6-6h17c12.688 0 19.313.3 20 1 4 4 7.313 8.3 10 13\n 35.313 51.3 80.813 93.8 136.5 127.5 55.688 33.7 117.188 55.8 184.5 66.5.688\n 0 2 .3 4 1 18.688 2.7 76 4.3 172 5h399450v120H429l-6-1c-124.688-8-235-61.7\n-331-161C60.687 138.7 32.312 99.3 7 54L0 41V6z"})})}),(0,a.jsx)(s.span,{className:"brace-center mtight",style:{height:"0.548em"},children:(0,a.jsx)(s.svg,{xmlns:"http://www.w3.org/2000/svg",width:"400em",height:"0.548em",viewBox:"0 0 400000 548",preserveAspectRatio:"xMidYMin slice",children:(0,a.jsx)(s.path,{d:"M199572 214\nc100.7 8.3 195.3 44 280 108 55.3 42 101.7 93 139 153l9 14c2.7-4 5.7-8.7 9-14\n 53.3-86.7 123.7-153 211-199 66.7-36 137.3-56.3 212-62h199568v120H200432c-178.3\n 11.7-311.7 78.3-403 201-6 8-9.7 12-11 12-.7.7-6.7 1-18 1s-17.3-.3-18-1c-1.3 0\n-5-4-11-12-44.7-59.3-101.3-106.3-170-141s-145.3-54.3-229-60H0V214z"})})}),(0,a.jsx)(s.span,{className:"brace-right mtight",style:{height:"0.548em"},children:(0,a.jsx)(s.svg,{xmlns:"http://www.w3.org/2000/svg",width:"400em",height:"0.548em",viewBox:"0 0 400000 548",preserveAspectRatio:"xMaxYMin slice",children:(0,a.jsx)(s.path,{d:"M399994 0l6 6v35l-6 11c-56 104-135.3 181.3-238 232-57.3\n 28.7-117 45-179 50H-300V214h399897c43.3-7 81-15 113-26 100.7-33 179.7-91 237\n-174 2.7-5 6-9 10-13 .7-1 7.3-1 20-1h17z"})})})]})]}),(0,a.jsxs)(s.span,{style:{top:"-3em"},children:[(0,a.jsx)(s.span,{className:"pstrut",style:{height:"3em"}}),(0,a.jsxs)(s.span,{className:"mord",children:[(0,a.jsx)(s.span,{className:"mord",children:"2"}),(0,a.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}}),(0,a.jsx)(s.span,{className:"mbin",children:"\xd7"}),(0,a.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}}),(0,a.jsx)(s.span,{className:"mord",children:"2"}),(0,a.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}}),(0,a.jsx)(s.span,{className:"mbin",children:"\xd7"}),(0,a.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}}),(0,a.jsx)(s.span,{className:"minner",children:"\u22ef"}),(0,a.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}}),(0,a.jsx)(s.span,{className:"mbin",children:"\xd7"}),(0,a.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}}),(0,a.jsx)(s.span,{className:"mord",children:"2"})]})]})]}),(0,a.jsx)(s.span,{className:"vlist-s",children:"\u200b"})]}),(0,a.jsx)(s.span,{className:"vlist-r",children:(0,a.jsx)(s.span,{className:"vlist",style:{height:"0.7313em"},children:(0,a.jsx)(s.span,{})})})]})})]})]}),(0,a.jsx)(s.span,{className:"vlist-s",children:"\u200b"})]}),(0,a.jsx)(s.span,{className:"vlist-r",children:(0,a.jsx)(s.span,{className:"vlist",style:{height:"1.4084em"},children:(0,a.jsx)(s.span,{})})})]})})]})})]})})})})})]})]})]})]})," which ",(0,a.jsxs)(s.span,{className:"katex",children:[(0,a.jsx)(s.span,{className:"katex-mathml",children:(0,a.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,a.jsxs)(s.semantics,{children:[(0,a.jsx)(s.mrow,{children:(0,a.jsx)(s.mi,{children:"b"})}),(0,a.jsx)(s.annotation,{encoding:"application/x-tex",children:"b"})]})})}),(0,a.jsx)(s.span,{className:"katex-html","aria-hidden":"true",children:(0,a.jsxs)(s.span,{className:"base",children:[(0,a.jsx)(s.span,{className:"strut",style:{height:"0.6944em"}}),(0,a.jsx)(s.span,{className:"mord mathnormal",children:"b"})]})})]})," is batch size, ",(0,a.jsxs)(s.span,{className:"katex",children:[(0,a.jsx)(s.span,{className:"katex-mathml",children:(0,a.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,a.jsxs)(s.semantics,{children:[(0,a.jsx)(s.mrow,{children:(0,a.jsx)(s.mi,{children:"n"})}),(0,a.jsx)(s.annotation,{encoding:"application/x-tex",children:"n"})]})})}),(0,a.jsx)(s.span,{className:"katex-html","aria-hidden":"true",children:(0,a.jsxs)(s.span,{className:"base",children:[(0,a.jsx)(s.span,{className:"strut",style:{height:"0.4306em"}}),(0,a.jsx)(s.span,{className:"mord mathnormal",children:"n"})]})})]})," is number of qubits so that for each batch, there are ",(0,a.jsxs)(s.span,{className:"katex",children:[(0,a.jsx)(s.span,{className:"katex-mathml",children:(0,a.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,a.jsxs)(s.semantics,{children:[(0,a.jsx)(s.mrow,{children:(0,a.jsxs)(s.msup,{children:[(0,a.jsx)(s.mn,{children:"2"}),(0,a.jsx)(s.mi,{children:"n"})]})}),(0,a.jsx)(s.annotation,{encoding:"application/x-tex",children:"2^n"})]})})}),(0,a.jsx)(s.span,{className:"katex-html","aria-hidden":"true",children:(0,a.jsxs)(s.span,{className:"base",children:[(0,a.jsx)(s.span,{className:"strut",style:{height:"0.6644em"}}),(0,a.jsxs)(s.span,{className:"mord",children:[(0,a.jsx)(s.span,{className:"mord",children:"2"}),(0,a.jsx)(s.span,{className:"msupsub",children:(0,a.jsx)(s.span,{className:"vlist-t",children:(0,a.jsx)(s.span,{className:"vlist-r",children:(0,a.jsx)(s.span,{className:"vlist",style:{height:"0.6644em"},children:(0,a.jsxs)(s.span,{style:{top:"-3.063em",marginRight:"0.05em"},children:[(0,a.jsx)(s.span,{className:"pstrut",style:{height:"2.7em"}}),(0,a.jsx)(s.span,{className:"sizing reset-size6 size3 mtight",children:(0,a.jsx)(s.span,{className:"mord mathnormal mtight",children:"n"})})]})})})})})]})]})})]})," propabilities."]}),"\n",(0,a.jsx)(s.p,{children:"To apply the gate to a specific qubit in the state vector, we can simply permute the target qubit dimension to the back, then multiply it with the gate matrix. For double qubits gates, we can permute the control and target to the back, combine the two dimensions, multiply the gate matrix, and reshape back. Here is a function that applied the gate to the state:"}),"\n",(0,a.jsx)(s.pre,{children:(0,a.jsx)(s.code,{className:"language-python",children:'def apply_gate(state, mat, wires: Union[int, List[int]]):\r\n    """\r\n    Apply the gate matrix/matrices to the state vector using torch.bmm method.\r\n    \r\n    Args:\r\n        state (torch.Tensor): The state vector.\r\n        mat (torch.Tensor): The gate matrix/matrices.\r\n        wires (int or List[int]): Which qubit(s) the operation is applied to.\r\n        \r\n    Returns:\r\n        torch.Tensor: The updated state vector.\r\n    """\r\n    # Handle input for single qubit as a list for uniformity\r\n    if isinstance(wires, int):\r\n        wires = [wires]\r\n    \r\n    # Ensure the matrix is on the same device and dtype as the state\r\n    mat = mat.to(state.device).to(state.dtype)\r\n    \r\n    # Calculate the new order of dimensions for the state to match matrix multiplication needs\r\n    num_qubits = len(state.shape) - 1\r\n    permute_order = list(range(1, num_qubits + 1))  # Start from 1 to account for batch dimension\r\n    for index, wire in enumerate(wires):\r\n        permute_order.remove(wire + 1)  # Remove wire from its current place\r\n        permute_order.insert(index, wire + 1)  # Insert wire right after the batch dimension\r\n\r\n    # Permute the state tensor to bring the target wire dimensions next to the batch dimension\r\n    permuted_state = state.permute([0] + permute_order)  # Batch dimension remains the first\r\n    reshaped_state = permuted_state.reshape(state.shape[0], -1, mat.size(-1))\r\n    \r\n    # Apply the gate using matrix multiplication\r\n    new_state = torch.bmm(reshaped_state, mat) if len(mat.shape) == 3 else reshaped_state @ mat\r\n    \r\n    # Reshape and permute back to the original shape and order\r\n    final_state = new_state.view(state.shape).permute(list(np.argsort([0] + permute_order)))\r\n    \r\n    return final_state\n'})}),"\n",(0,a.jsx)(s.p,{children:"The reason for the input to accept batched gate matrices is for the angle encoding later, which encodes inputs into the state."}),"\n",(0,a.jsx)(s.h3,{id:"define-gate-matrices",children:"Define Gate Matrices"}),"\n",(0,a.jsx)(s.p,{children:"Now the gate matrix functions:"}),"\n",(0,a.jsx)(s.pre,{children:(0,a.jsx)(s.code,{className:"language-python",children:"def h_matrix():\r\n    return 1 / np.sqrt(2) * torch.tensor([[1, 1], [1, -1]], dtype=torch.complex64, requires_grad=True)\r\n\r\ndef cnot_matrix():\r\n    return torch.tensor([[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 0, 1], [0, 0, 1, 0]], dtype=torch.complex64, requires_grad=True)\r\n\r\ndef pauli_x_matrix():\r\n    return torch.tensor([[0, 1], [1, 0]], dtype=torch.complex64, requires_grad=True)\r\n\r\ndef pauli_y_matrix():\r\n    return torch.tensor([[0, -1j], [1j, 0]], dtype=torch.complex64, requires_grad=True)\r\n\r\ndef pauli_z_matrix():\r\n    return torch.tensor([[1, 0], [0, -1]], dtype=torch.complex64, requires_grad=True)\r\n\r\ndef rx_matrix(theta):\r\n    return torch.tensor([[torch.cos(theta / 2), -1j * torch.sin(theta / 2)],\r\n                         [-1j * torch.sin(theta / 2), torch.cos(theta / 2)]], dtype=torch.complex64, requires_grad=True)\r\n\r\ndef ry_matrix(theta):\r\n    return torch.tensor([[torch.cos(theta / 2), -torch.sin(theta / 2)],\r\n                         [torch.sin(theta / 2), torch.cos(theta / 2)]], dtype=torch.complex64, requires_grad=True)\r\n\r\ndef rz_matrix(theta):\r\n    return torch.tensor([[torch.exp(-1j * theta / 2), 0],\r\n                         [0, torch.exp(1j * theta / 2)]], dtype=torch.complex64, requires_grad=True)\r\n\r\ndef u3_matrix(theta, phi, lam):\r\n    return torch.tensor([[torch.cos(theta / 2), -torch.exp(1j * lam) * torch.sin(theta / 2)],\r\n                         [torch.exp(1j * phi) * torch.sin(theta / 2), torch.exp(1j * (phi + lam)) * torch.cos(theta / 2)]], dtype=torch.complex64, requires_grad=True)\r\n\r\ndef crx_matrix(theta):\r\n    return torch.tensor([[1, 0, 0, 0],\r\n                         [0, 1, 0, 0],\r\n                         [0, 0, torch.cos(theta / 2), -1j * torch.sin(theta / 2)],\r\n                         [0, 0, -1j * torch.sin(theta / 2), torch.cos(theta / 2)]], dtype=torch.complex64, requires_grad=True)\r\n\r\ndef cry_matrix(theta):\r\n    return torch.tensor([[1, 0, 0, 0],\r\n                         [0, 1, 0, 0],\r\n                         [0, 0, torch.cos(theta / 2), -torch.sin(theta / 2)],\r\n                         [0, 0, torch.sin(theta / 2), torch.cos(theta / 2)]], dtype=torch.complex64, requires_grad=True)\r\n\r\ndef crz_matrix(theta):\r\n    return torch.tensor([[1, 0, 0, 0],\r\n                         [0, 1, 0, 0],\r\n                         [0, 0, torch.exp(-1j * theta / 2), 0],\r\n                         [0, 0, 0, torch.exp(1j * theta / 2)]], dtype=torch.complex64, requires_grad=True)\r\n\r\ndef cu3_matrix(theta, phi, lam):\r\n    return torch.tensor([[1, 0, 0, 0],\r\n                         [0, 1, 0, 0],\r\n                         [0, 0, torch.cos(theta / 2), -torch.exp(1j * lam) * torch.sin(theta / 2)],\r\n                         [0, 0, torch.exp(1j * phi) * torch.sin(theta / 2), torch.exp(1j * (phi + lam)) * torch.cos(theta / 2)]], dtype=torch.complex64, requires_grad=True)\n"})}),"\n",(0,a.jsx)(s.p,{children:"Now the Gate class, which stores the parameters (if any) and qubits:"}),"\n",(0,a.jsx)(s.pre,{children:(0,a.jsx)(s.code,{className:"language-python",children:"class Gate(nn.Module):\r\n    def __init__(self, gate_matrix_fn, wires: list, n_params: int):\r\n        super(Gate, self).__init__()\r\n        self.wires = wires\r\n        self.n_params = n_params\r\n        self.gate_matrix_fn = gate_matrix_fn\r\n        self.params = nn.Parameter(torch.randn(n_params, dtype=torch.float32)).uniform_(-np.pi, np.pi) # Unifrom distribution between -pi and pi\r\n    \r\n    def forward(self, state):\r\n        gate_matrix = self.gate_matrix_fn(*self.params)\r\n        return apply_gate(state, gate_matrix, self.wires)\n"})}),"\n",(0,a.jsx)(s.h3,{id:"angle-encoding",children:"Angle Encoding"}),"\n",(0,a.jsx)(s.p,{children:"Angle encoding, as mentioned above, encodes the input tensor into the state vector using rotation gates (rx, ry, u3, etc)."}),"\n",(0,a.jsx)(s.pre,{children:(0,a.jsx)(s.code,{className:"language-python",children:'class AngleEncoding(nn.Module):\r\n    """\r\n    Example functions list:\r\n    [{"gate": \'rx\', "wires": 0, "input_idx": 0},\r\n    {"gate": \'ry\', "wires": 1, "input_idx": 1},\r\n    {"gate": \'rz\', "wires": 2, "input_idx": 2},]\r\n    or\r\n    [{"gate": \'u3\', "wires": 0, "input_idx": [0, 1, 2]},\r\n    {"gate": \'rx\', "wires": 1, "input_idx": 1},\r\n    {"gate": \'cu3\', "wires": [1, 2], "input_idx": [0, 1, 2]},]\r\n    """\r\n    def __init__(self, func_list):\r\n        super().__init__()\r\n        self.func_list = func_list\r\n\r\n    def forward(self, state, x):\r\n        for info in self.func_list:\r\n            params = x[:, [info["input_idx"]]] if len(x.shape) > 1 else x[info["input_idx"]]\r\n            gate = info["gate"] + \'_matrix\'\r\n            fn = globals()[gate] # Get the function from the globals\r\n            mat = torch.stack([fn(*p) for p in params], dim=0) if len(x.shape) > 1 else fn(*params) # Gate matrix\r\n            state = apply_gate(\r\n                state,\r\n                mat=mat,\r\n                wires=info["wires"],\r\n            )\r\n        return state\n'})}),"\n",(0,a.jsx)(s.h3,{id:"measurement",children:"Measurement"}),"\n",(0,a.jsxs)(s.p,{children:["For the measurement, we will measure the expected values of all qubits based on a observable matrix of size ",(0,a.jsxs)(s.span,{className:"katex",children:[(0,a.jsx)(s.span,{className:"katex-mathml",children:(0,a.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,a.jsxs)(s.semantics,{children:[(0,a.jsxs)(s.mrow,{children:[(0,a.jsxs)(s.msup,{children:[(0,a.jsx)(s.mn,{children:"2"}),(0,a.jsx)(s.mi,{children:"n"})]}),(0,a.jsx)(s.mo,{children:"\xd7"}),(0,a.jsxs)(s.msup,{children:[(0,a.jsx)(s.mn,{children:"2"}),(0,a.jsx)(s.mi,{children:"n"})]})]}),(0,a.jsx)(s.annotation,{encoding:"application/x-tex",children:"2^n \\times 2^n"})]})})}),(0,a.jsxs)(s.span,{className:"katex-html","aria-hidden":"true",children:[(0,a.jsxs)(s.span,{className:"base",children:[(0,a.jsx)(s.span,{className:"strut",style:{height:"0.7477em",verticalAlign:"-0.0833em"}}),(0,a.jsxs)(s.span,{className:"mord",children:[(0,a.jsx)(s.span,{className:"mord",children:"2"}),(0,a.jsx)(s.span,{className:"msupsub",children:(0,a.jsx)(s.span,{className:"vlist-t",children:(0,a.jsx)(s.span,{className:"vlist-r",children:(0,a.jsx)(s.span,{className:"vlist",style:{height:"0.6644em"},children:(0,a.jsxs)(s.span,{style:{top:"-3.063em",marginRight:"0.05em"},children:[(0,a.jsx)(s.span,{className:"pstrut",style:{height:"2.7em"}}),(0,a.jsx)(s.span,{className:"sizing reset-size6 size3 mtight",children:(0,a.jsx)(s.span,{className:"mord mathnormal mtight",children:"n"})})]})})})})})]}),(0,a.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}}),(0,a.jsx)(s.span,{className:"mbin",children:"\xd7"}),(0,a.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}})]}),(0,a.jsxs)(s.span,{className:"base",children:[(0,a.jsx)(s.span,{className:"strut",style:{height:"0.6644em"}}),(0,a.jsxs)(s.span,{className:"mord",children:[(0,a.jsx)(s.span,{className:"mord",children:"2"}),(0,a.jsx)(s.span,{className:"msupsub",children:(0,a.jsx)(s.span,{className:"vlist-t",children:(0,a.jsx)(s.span,{className:"vlist-r",children:(0,a.jsx)(s.span,{className:"vlist",style:{height:"0.6644em"},children:(0,a.jsxs)(s.span,{style:{top:"-3.063em",marginRight:"0.05em"},children:[(0,a.jsx)(s.span,{className:"pstrut",style:{height:"2.7em"}}),(0,a.jsx)(s.span,{className:"sizing reset-size6 size3 mtight",children:(0,a.jsx)(s.span,{className:"mord mathnormal mtight",children:"n"})})]})})})})})]})]})]})]}),", which typically is either Pauli-X, Pauli-Y, or Pauli-Z. Mathematically it is ",(0,a.jsxs)(s.span,{className:"katex",children:[(0,a.jsx)(s.span,{className:"katex-mathml",children:(0,a.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,a.jsxs)(s.semantics,{children:[(0,a.jsxs)(s.mrow,{children:[(0,a.jsx)(s.mo,{stretchy:"false",children:"\u27e8"}),(0,a.jsx)(s.mi,{children:"\u03c8"}),(0,a.jsx)(s.mi,{mathvariant:"normal",children:"\u2223"}),(0,a.jsx)(s.mi,{children:"O"}),(0,a.jsx)(s.mi,{mathvariant:"normal",children:"\u2223"}),(0,a.jsx)(s.mi,{children:"\u03c8"}),(0,a.jsx)(s.mo,{stretchy:"false",children:"\u27e9"})]}),(0,a.jsx)(s.annotation,{encoding:"application/x-tex",children:"\\langle\\psi|O|\\psi\\rangle"})]})})}),(0,a.jsx)(s.span,{className:"katex-html","aria-hidden":"true",children:(0,a.jsxs)(s.span,{className:"base",children:[(0,a.jsx)(s.span,{className:"strut",style:{height:"1em",verticalAlign:"-0.25em"}}),(0,a.jsx)(s.span,{className:"mopen",children:"\u27e8"}),(0,a.jsx)(s.span,{className:"mord mathnormal",style:{marginRight:"0.03588em"},children:"\u03c8"}),(0,a.jsx)(s.span,{className:"mord",children:"\u2223"}),(0,a.jsx)(s.span,{className:"mord mathnormal",style:{marginRight:"0.02778em"},children:"O"}),(0,a.jsx)(s.span,{className:"mord",children:"\u2223"}),(0,a.jsx)(s.span,{className:"mord mathnormal",style:{marginRight:"0.03588em"},children:"\u03c8"}),(0,a.jsx)(s.span,{className:"mclose",children:"\u27e9"})]})})]}),"."]}),"\n",(0,a.jsx)(s.pre,{children:(0,a.jsx)(s.code,{className:"language-python",children:"def measure_pauli_expectations(state, observable_matrix):\r\n    num_qubits = len(state.shape) - 1\r\n    batch_size = state.shape[0]\r\n    expected_values = torch.empty((batch_size, num_qubits), dtype=state.real.dtype, device=state.device)\r\n    observable_matrix = observable_matrix.to(state.device).to(state.dtype)\r\n\r\n    # Compute the expectation value for each qubit\r\n    for qubit in range(num_qubits):\r\n        # Permute to bring the qubit of interest to the last position\r\n        dims = list(range(1, num_qubits + 1))\r\n        dims.append(dims.pop(qubit))  # Move the qubit index to the end\r\n        permuted_state = state.permute([0] + dims)\r\n        \r\n        # Reshape to combine all other dimensions except the last two\r\n        reshaped_state = permuted_state.reshape(batch_size, -1, 2)\r\n        \r\n        # Apply observable and calculate expectation value\r\n        # Here we calculate <psi|O|psi> for the current qubit\r\n        measured_state = torch.matmul(reshaped_state, observable_matrix)\r\n        probabilities = torch.matmul(measured_state, reshaped_state.transpose(-2, -1)).diagonal(dim1=-2, dim2=-1)\r\n        expected_value = probabilities.sum(dim=-1)  # Sum over the states to get the expectation\r\n\r\n        # Store the computed expected value for the current qubit\r\n        expected_values[:, qubit] = expected_value.real\r\n\r\n    return expected_values\n"})}),"\n",(0,a.jsx)(s.h3,{id:"model",children:"Model"}),"\n",(0,a.jsxs)(s.p,{children:["We will use the MNIST dataset for the testing. We will only use the first 5 numbers (0, 1, 2, 3, 4) and the image size will be reduced to ",(0,a.jsxs)(s.span,{className:"katex",children:[(0,a.jsx)(s.span,{className:"katex-mathml",children:(0,a.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,a.jsxs)(s.semantics,{children:[(0,a.jsxs)(s.mrow,{children:[(0,a.jsx)(s.mn,{children:"5"}),(0,a.jsx)(s.mo,{children:"\xd7"}),(0,a.jsx)(s.mn,{children:"5"})]}),(0,a.jsx)(s.annotation,{encoding:"application/x-tex",children:"5 \\times 5"})]})})}),(0,a.jsxs)(s.span,{className:"katex-html","aria-hidden":"true",children:[(0,a.jsxs)(s.span,{className:"base",children:[(0,a.jsx)(s.span,{className:"strut",style:{height:"0.7278em",verticalAlign:"-0.0833em"}}),(0,a.jsx)(s.span,{className:"mord",children:"5"}),(0,a.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}}),(0,a.jsx)(s.span,{className:"mbin",children:"\xd7"}),(0,a.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}})]}),(0,a.jsxs)(s.span,{className:"base",children:[(0,a.jsx)(s.span,{className:"strut",style:{height:"0.6444em"}}),(0,a.jsx)(s.span,{className:"mord",children:"5"})]})]})]})," to reduce complexity. The Variational Quantum Circuit (VQC) consists 4 blocks of single and double qubits parameterized gates."]}),"\n",(0,a.jsx)(s.pre,{children:(0,a.jsx)(s.code,{className:"language-python",children:'class VQC(nn.Module):\r\n    def __init__(self):\r\n        super(VQC, self).__init__()\r\n        self.n_wires = 5 # Number of qubits in the circuit\r\n        self.encoding = AngleEncoding([\r\n            {"input_idx": [0], "gate": "ry", "wires": [0]},\r\n            {"input_idx": [1], "gate": "ry", "wires": [1]},\r\n            {"input_idx": [2], "gate": "ry", "wires": [2]},\r\n            {"input_idx": [3], "gate": "ry", "wires": [3]},\r\n            {"input_idx": [4], "gate": "ry", "wires": [4]},\r\n            {"input_idx": [5], "gate": "rz", "wires": [0]},\r\n            {"input_idx": [6], "gate": "rz", "wires": [1]},\r\n            {"input_idx": [7], "gate": "rz", "wires": [2]},\r\n            {"input_idx": [8], "gate": "rz", "wires": [3]},\r\n            {"input_idx": [9], "gate": "rz", "wires": [4]},\r\n            {"input_idx": [10], "gate": "rx", "wires": [0]},\r\n            {"input_idx": [11], "gate": "rx", "wires": [1]},\r\n            {"input_idx": [12], "gate": "rx", "wires": [2]},\r\n            {"input_idx": [13], "gate": "rx", "wires": [3]},\r\n            {"input_idx": [14], "gate": "rx", "wires": [4]},\r\n            {"input_idx": [15], "gate": "ry", "wires": [0]},\r\n            {"input_idx": [16], "gate": "ry", "wires": [1]},\r\n            {"input_idx": [17], "gate": "ry", "wires": [2]},\r\n            {"input_idx": [18], "gate": "ry", "wires": [3]},\r\n            {"input_idx": [19], "gate": "ry", "wires": [4]},\r\n            {"input_idx": [20], "gate": "rz", "wires": [0]},\r\n            {"input_idx": [21], "gate": "rz", "wires": [1]},\r\n            {"input_idx": [22], "gate": "rz", "wires": [2]},\r\n            {"input_idx": [23], "gate": "rz", "wires": [3]},\r\n            {"input_idx": [24], "gate": "rz", "wires": [4]},\r\n        ]) # Encode 16 features into 4 qubits\r\n        \r\n        layers = []\r\n        for _ in range(4):\r\n            layers.extend([\r\n                Gate(cnot_matrix, [0, 1], 0),\r\n                Gate(cnot_matrix, [1, 2], 0),\r\n                Gate(cnot_matrix, [2, 3], 0),\r\n                Gate(cnot_matrix, [3, 4], 0),\r\n                Gate(cnot_matrix, [4, 0], 0),\r\n                Gate(rx_matrix, [0], 1),\r\n                Gate(rx_matrix, [1], 1),\r\n                Gate(rx_matrix, [2], 1),\r\n                Gate(rx_matrix, [3], 1),\r\n                Gate(rx_matrix, [4], 1),\r\n                Gate(cu3_matrix, [1, 0], 3),\r\n                Gate(cu3_matrix, [2, 1], 3),\r\n                Gate(cu3_matrix, [3, 2], 3),\r\n                Gate(cu3_matrix, [4, 3], 3),\r\n                Gate(cu3_matrix, [0, 4], 3),\r\n                Gate(u3_matrix, [0], 3),\r\n                Gate(u3_matrix, [1], 3),\r\n                Gate(u3_matrix, [2], 3),\r\n                Gate(u3_matrix, [3], 3),\r\n                Gate(u3_matrix, [4], 3),\r\n            ])\r\n        self.qnn = nn.Sequential(*layers)\r\n    \r\n    def forward(self, x):\r\n        state = torch.zeros(x.shape[0], 2**self.n_wires, dtype=torch.complex64, device=x.device).reshape(x.shape[0], 2, 2, 2, 2, 2)\r\n        state[:, 0, 0, 0, 0, 0] = 1  # Initialize the state to |0001>\r\n        state = self.encoding(state, x.view(x.shape[0], -1)) # Encode the input features\r\n        state = self.qnn(state)\r\n        measured = measure_pauli_expectations(state, pauli_z_matrix())\r\n        return F.log_softmax(measured, dim=-1)\n'})}),"\n",(0,a.jsx)(s.h3,{id:"dataset",children:"Dataset"}),"\n",(0,a.jsxs)(s.p,{children:["We will process the MNIST to include only first 5 digits and resize it to ",(0,a.jsxs)(s.span,{className:"katex",children:[(0,a.jsx)(s.span,{className:"katex-mathml",children:(0,a.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,a.jsxs)(s.semantics,{children:[(0,a.jsxs)(s.mrow,{children:[(0,a.jsx)(s.mn,{children:"5"}),(0,a.jsx)(s.mo,{children:"\xd7"}),(0,a.jsx)(s.mn,{children:"5"})]}),(0,a.jsx)(s.annotation,{encoding:"application/x-tex",children:"5 \\times 5"})]})})}),(0,a.jsxs)(s.span,{className:"katex-html","aria-hidden":"true",children:[(0,a.jsxs)(s.span,{className:"base",children:[(0,a.jsx)(s.span,{className:"strut",style:{height:"0.7278em",verticalAlign:"-0.0833em"}}),(0,a.jsx)(s.span,{className:"mord",children:"5"}),(0,a.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}}),(0,a.jsx)(s.span,{className:"mbin",children:"\xd7"}),(0,a.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}})]}),(0,a.jsxs)(s.span,{className:"base",children:[(0,a.jsx)(s.span,{className:"strut",style:{height:"0.6444em"}}),(0,a.jsx)(s.span,{className:"mord",children:"5"})]})]})]}),"."]}),"\n",(0,a.jsx)(s.pre,{children:(0,a.jsx)(s.code,{className:"language-python",children:"class MNISTDigitsDataset(datasets.MNIST):\r\n    def __init__(self, root, train=True, transform=None, target_transform=None, download=False):\r\n        super().__init__(root, train=train, transform=transform, target_transform=target_transform, download=download)\r\n        # Filter indices for digits 0 to 3\r\n        indices = [i for i, label in enumerate(self.targets) if label in [0, 1, 2, 3, 4]]\r\n        self.data = self.data[indices]\r\n        self.targets = self.targets[indices]\r\n\r\n    def __getitem__(self, index):\r\n        with torch.no_grad():\r\n            # Get the image and target label\r\n            img, target = self.data[index], int(self.targets[index])\r\n            \r\n            # Convert image to PIL for transformations\r\n            img = transforms.functional.to_pil_image(img)\r\n            \r\n            # Apply transformations if any\r\n            if self.transform:\r\n                img = self.transform(img)\r\n            \r\n            if self.target_transform:\r\n                target = self.target_transform(target)\r\n        \r\n        return img, target\r\n\r\n    def __len__(self):\r\n        return len(self.data)\r\n\r\ntransform = transforms.Compose([\r\n    transforms.Resize((5, 5)),  # Resize the image to 5x5\r\n    transforms.ToTensor()       # Convert the image to a PyTorch tensor\r\n])\r\n\r\n# Initialize the dataset\r\ntrain_dataset = MNISTDigitsDataset(root='./data', train=True, transform=transform, download=True)\r\nvalid_dataset = MNISTDigitsDataset(root='./data', train=False, transform=transform, download=True)\r\n\r\n# Create the DataLoader\r\nbatch_size = 32\r\ntrain_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\r\nvalid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n"})}),"\n",(0,a.jsx)(s.h3,{id:"training-loop",children:"Training Loop"}),"\n",(0,a.jsx)(s.pre,{children:(0,a.jsx)(s.code,{className:"language-python",children:"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\nmodel = VQC().to(device)\r\n\r\nloss_fn = nn.NLLLoss()\r\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\r\n\r\n# Training loop\r\nnum_epochs = 5\r\ntrain_losses = []\r\nvalid_losses = []\r\nvalid_accuracies = []\r\nfor epoch in range(num_epochs):\r\n    for batch, (images, labels) in enumerate(train_dataloader):\r\n        images, labels = images.to(device), labels.to(device)\r\n        optimizer.zero_grad()\r\n        outputs = model(images)\r\n        loss = loss_fn(outputs, labels)\r\n        loss.backward()\r\n        optimizer.step()\r\n        train_losses.append(loss.item())\r\n        \r\n        if (batch + 1) % 50 == 0:\r\n            print(f\"Epoch {epoch + 1}/{num_epochs}, Iteration: {batch+1}, Loss: {loss.item()}\")\r\n        if batch == 500:\r\n            break # Stop early to speed up the training process\r\n    \r\n    # Validation loop\r\n    model.eval()\r\n    with torch.no_grad():\r\n        total = 0\r\n        correct = 0\r\n        for batch, (images, labels) in enumerate(valid_dataloader):\r\n            images, labels = images.to(device), labels.to(device)\r\n            outputs = model(images)\r\n            loss = loss_fn(outputs, labels)\r\n            valid_losses.append(loss.item())\r\n            \r\n            _, predicted = torch.max(outputs, 1)\r\n            total += labels.size(0)\r\n            correct += (predicted == labels).sum().item()\r\n\r\n            if batch == 100:\r\n                break # Stop early to speed up the validation process\r\n        \r\n        accuracy = correct / total\r\n        valid_accuracies.append(accuracy)\r\n        print(f\"Validation accuracy: {accuracy}, Loss: {loss.item()}\")\r\n    model.train()\n"})}),"\n",(0,a.jsx)(s.p,{children:"And finally plot the result:"}),"\n",(0,a.jsx)(s.pre,{children:(0,a.jsx)(s.code,{className:"language-python",children:"# plot\r\nplt.plot(train_losses, label='Training loss')\r\nplt.plot(valid_losses, label='Validation loss')\r\nplt.xlabel('Iterations')\r\nplt.ylabel('Loss')\r\nplt.legend()\r\nplt.title('Training and Validation Loss')\r\nplt.show()\r\n\r\nplt.plot(valid_accuracies)\r\nplt.xlabel('Iterations')\r\nplt.ylabel('Accuracy')\r\nplt.title('Validation Accuracy')\r\nplt.show()\n"})})]})}function o(e={}){const{wrapper:s}={...(0,n.R)(),...e.components};return s?(0,a.jsx)(s,{...e,children:(0,a.jsx)(h,{...e})}):h(e)}},8453:(e,s,t)=>{t.d(s,{R:()=>i,x:()=>l});var a=t(6540);const n={},r=a.createContext(n);function i(e){const s=a.useContext(r);return a.useMemo((function(){return"function"==typeof e?e(s):{...s,...e}}),[s,e])}function l(e){let s;return s=e.disableParentContext?"function"==typeof e.components?e.components(n):e.components||n:i(e.components),a.createElement(r.Provider,{value:s},e.children)}}}]);