"use strict";(self.webpackChunkdocusaurus_site=self.webpackChunkdocusaurus_site||[]).push([[5894],{6042:e=>{e.exports=JSON.parse('{"blogPosts":[{"id":"post-hackathon-analysis","metadata":{"permalink":"/blog/post-hackathon-analysis","source":"@site/blog/2025-3-6-post-hackathon-analysis/index.mdx","title":"Post-hackathon analysis of Efficient Gaussian State Preparation","description":"Post-hackathon analysis of Our Solution at MIT iQuHACK 2025","date":"2025-03-06T00:00:00.000Z","tags":[{"label":"advanced","permalink":"/blog/tags/advanced"},{"label":"quantum","permalink":"/blog/tags/quantum"},{"label":"state-preparation","permalink":"/blog/tags/state-preparation"},{"label":"hackathon","permalink":"/blog/tags/hackathon"}],"readingTime":5.465,"hasTruncateMarker":true,"authors":[{"name":"Eason Xie","title":"Some Random Tree","url":"https://easonoob.github.io","imageURL":"https://avatars.githubusercontent.com/u/100521878?v=4","key":"eason"}],"frontMatter":{"slug":"post-hackathon-analysis","title":"Post-hackathon analysis of Efficient Gaussian State Preparation","authors":"eason","tags":["advanced","quantum","state-preparation","hackathon"]},"unlisted":false,"nextItem":{"title":"Preparing a Gaussian State in the Symmetrical Domain","permalink":"/blog/mit-iquhack-solution"}},"content":"import ReactPlayer from \'react-player\'\\n\\nPost-hackathon analysis of Our Solution at MIT iQuHACK 2025\\n\\n\x3c!--truncate--\x3e\\n\\n## Introduction\\n\\nIn a [previous blog post](./mit-iquhack-solution), I presented our MIT iQuHACK 2025 solution for preparing a Gaussian state on a quantum computer using a two-step approach:\\n\\n1. **Bitwise (product) state preparation** via RY rotations with angles chosen to induce exponential decay per qubit.\\n2. **Approximate Quantum Fourier Transform (QFT)** with pruning of small-angle controlled-phase gates to keep the circuit cost nearly linear in the number of qubits.\\n\\nWe demonstrated that this method yields an acceptably small mean-squared error (MSE) while maintaining scalability. After the hackathon, we continued investigating this technique\u2014particularly, how well it accommodates different resolutions (number of qubits) and how the shape of the Gaussian (controlled by the parameter $$\\\\sigma$$ or `EXP_RATE`) affects fidelity. \\n\\nThis follow-up blog post summarizes these explorations, includes additional visualizations, and discusses next steps such as parameter optimization and potential integration into quantum machine learning (QML) tasks.\\n\\n\\n## Testing at Different Resolutions\\n\\nOne of the first post-hackathon experiments was to test the same circuit design at different resolutions $$(r=5 \\\\text{ to } 16)$$. Here is are the results of these runs:\\n\\n![Different Resolutions](different_resolution.jpg)\\n\\nThe circuit structure and parameters (apart from changing the qubit count) remain the same for each experiment. As expected, as $$r$$ grows, the MSE typically improves because a higher-resolution grid captures the discrete Gaussian more finely. This is consistent with our original hackathon findings: the approximate QFT approach benefits from increased dimensionality, so long as we can handle the gate overhead (mitigated by pruning small-angle controlled-phase gates).\\n\\n\\n## Varying $$\\\\sigma$$ and the \u201cShoulder\u201d Phenomenon\\n\\nAnother goal was to see how well the circuit adapts to different Gaussian widths, controlled by $$\\\\sigma$$. Recall that in the hackathon, we mostly focused on a single value $$\\\\sigma$$. Extending that to arbitrary $$\\\\sigma$$ values unveiled an interesting \u201cshoulder\u201d phenomenon on both tails of the Gaussian for certain $$\\\\sigma$$ ranges. \\n\\nBelow is a short video ([different_sigma.mp4](/different_sigma.mp4)) illustrating how the amplitude distribution transitions as $$\\\\sigma$$ varies from $$1 \\\\text{ to } 5$$. Notice that for significantly different $$\\\\sigma$$ from unity, the circuit\u2019s approximation is less accurate at the tails:\\n\\n<ReactPlayer playing controls url=\'/different_sigma.mp4\' />\\n\\nWe suspect the cause is that our initial RY-based product state and the single global QFT are not fully flexible in matching the shape of a \u201ctrue\u201d Gaussian when $$\\\\sigma$$ deviates substantially from 1. This prompts us to investigate the deeper mathematical models involved in the quantum circuit in order to fully understand the \\"shoulder\\" problem.\\n\\n\\n## Plotting MSE as a Function of $$\\\\sigma$$\\n\\nTo quantify how the circuit\u2019s fidelity changes with $$\\\\sigma$$, we let MSE as a function of $$\\\\sigma$$ and measured the MSE for a range of $$\\\\sigma$$ values. Surprisingly, the region where MSE changes the least seems to lie around $$\\\\sigma \\\\approx e$$. We also calculated $$\\\\frac{d\\\\,\\\\mathrm{MSE}}{d\\\\,\\\\sigma}$$ and observed that its behavior is most favorable near $$\\\\sigma=e$$. The reason might be tied to how the approximate QFT\u2019s discrete frequencies interact with the bitwise angles in the initial state, further investigation is required.\\n\\nBelow is the plot of $$\\\\mathrm{MSE}(\\\\sigma)$$:\\n\\n![MSE vs. Sigma Plot](different_sigma.png)\\n\\n> *Figure:* MSE versus $$\\\\sigma$$. At $$\\\\sigma = e$$, the slope $$\\\\frac{d\\\\,\\\\mathrm{MSE}}{d\\\\,\\\\sigma}$$ is notably small, suggesting a \u201csweet spot\u201d for this particular parameterization.\\n\\n## Preliminary Real-Hardware Tests on IBM Brisbane\\n\\nTo explore the robustness of our approach under realistic conditions, we ran the circuit on IBM\'s *Brisbane* quantum device.\\n\\nWe employed 5 qubits and collected 5000 shots. Despite hardware noise and inevitable gate errors, the measured distribution still resembled a Gaussian-like shape in the histogram of outcomes. Below is an image depicting the measured distribution:\\n\\n![IBM Brisbane 5-Qubit Results](ibm_result.png)\\n\\n> *Figure:* Measurement outcomes from IBM Brisbane with 5 qubits. A semblance of the Gaussian shape persists, though slightly distorted.\\n\\nWhen we extended the circuit to 6 qubits or more, however, the noise effects amplified, and the Gaussian shape effectively vanished. This significant distortion suggests that higher qubit counts on current hardware require either more robust error mitigation or further refinements of the circuit to tolerate noise. We plan to investigate specialized noise-adaptive strategies or gate-error mitigation to see if we can preserve the Gaussian profile beyond 5 qubits.\\n\\n\\n## Ongoing Directions and Next Steps\\n\\n### 1. Adaptive Parameter Tuning\\n\\nOur circuit still uses the same bitwise angle formula from the hackathon:\\n$$\\n\\\\theta_i = 2\\\\,\\\\arctan\\\\bigl(e^{-i^2/(\\\\frac{2\\\\sigma^2}{5})}\\\\bigr).\\n$$\\nHowever, it might be possible to add a small correction term per qubit or introduce global phase factors such that the QFT outcome better fits a broader range of $$\\\\sigma$$. This approach would entail optimizing angles for a given \u201ctarget $$\\\\sigma$$\u201d to minimize MSE, akin to a small classical optimization loop.\\n\\n### 2. Further Mathematical Analysis\\n\\nFurther mathematical analysis and modelling is required to understand the nature of the quantum circuit more completely, which is a fundamental step in solving problems like the \\"shoulder\\" phenomenom for arbitrary $$\\\\sigma$$ and also explaning parameters like dividing by $$\\\\sqrt{5}$$ in angle calculation.\\n\\n### 3. Improved Approximate QFT Pruning\\n\\nWhile the approximate QFT prunes CPhase gates with angles below a certain threshold, that threshold is static. We might allow a flexible threshold that depends on the qubit index (or a local cost function), further improving trade-offs between circuit depth and fidelity.\\n\\n### 4. Integration into QML\\n\\nA key motivation for Gaussian state preparation is quantum machine learning\u2014e.g., embedding classical data or forming distribution-based feature maps. We performed a preliminary integration into a QML algorithm and observed some notable gains. Refinements, or a deeper synergy with QML-based cost functions, may achieve bigger improvements.\\n\\n### 5. Real-Hardware Trials\\n\\nAll these simulations were performed in ideal, noiseless environments. Our initial tests on IBM Brisbane (5 qubits) demonstrate partial success, but the shape deteriorates at 6 qubits. Continued investigation of error mitigation, calibration, or noise-adaptive circuit designs could help preserve the Gaussian profile at larger scales.\\n\\n### 6. Library Integration\\n\\nWe plan to provide a tutorial or example for approximate-QFT plus product-state preparation in Classiq, demonstrating how to reduce gate counts for large resolutions.\\n\\n\\n## Conclusion\\n\\nOur post-hackathon exploration reaffirms that approximate QFT-based Gaussian state preparation is both scalable and flexible\u2014but it also highlights interesting parameter-dependent artifacts that warrant deeper analysis. By plotting $$\\\\mathrm{MSE}(\\\\sigma)$$ and visualizing amplitude profiles across resolutions and $$\\\\sigma$$ values, we have pinpointed where the method remains robust and where further refinements are needed.\\n\\nI look forward to further collaborations with Classiq and more thorough mathematical investigations to refine this approach. Stay tuned for additional results on parameter optimization, possible integration into Classiq\u2019s library, and explorations of real-device performance. As always, questions and feedback are welcome!"},{"id":"mit-iquhack-solution","metadata":{"permalink":"/blog/mit-iquhack-solution","source":"@site/blog/2025-2-16-mit-iquhack-solution/index.md","title":"Preparing a Gaussian State in the Symmetrical Domain","description":"Explanation of Our Solution at MIT iQuHACK","date":"2025-02-16T00:00:00.000Z","tags":[{"label":"advanced","permalink":"/blog/tags/advanced"},{"label":"quantum","permalink":"/blog/tags/quantum"},{"label":"state-preparation","permalink":"/blog/tags/state-preparation"},{"label":"hackathon","permalink":"/blog/tags/hackathon"}],"readingTime":8.835,"hasTruncateMarker":true,"authors":[{"name":"Eason Xie","title":"Some Random Tree","url":"https://easonoob.github.io","imageURL":"https://avatars.githubusercontent.com/u/100521878?v=4","key":"eason"}],"frontMatter":{"slug":"mit-iquhack-solution","title":"Preparing a Gaussian State in the Symmetrical Domain","authors":"eason","tags":["advanced","quantum","state-preparation","hackathon"]},"unlisted":false,"prevItem":{"title":"Post-hackathon analysis of Efficient Gaussian State Preparation","permalink":"/blog/post-hackathon-analysis"},"nextItem":{"title":"Introduction to Quantum Computing","permalink":"/blog/intro-to-quantum-computing"}},"content":"Explanation of Our Solution at MIT iQuHACK\\n\\n\x3c!--truncate--\x3e\\n\\n## Introduction\\n\\nIn this blog post, I will explain the solution of our team to the Classiq track at MIT iQuHACK 2025, including why it works and how it scales.\\n\\nGaussian state preparation is essential for simulating physical systems and tackling problems in quantum chemistry, machine learning, and optimization. Gaussian states, characterized by their Gaussian-shaped wavefunctions, are powerful tools for encoding probability distributions and modeling quantum systems.\\n\\nWith the scaling of quantum hardware, achieving efficient and precise Gaussian state preparation could improve the costs of quantum algorithms and enhance impactful applications like option pricing in finance, molecular simulations in quantum chemistry, and data analysis in machine learning, among others.\\n\\nAt the competition, our challenge was to prepare a Gaussian state in the symmetrical domain  \\n\\n$$\\nx \\\\in [-2, 2)\\n$$\\n\\nusing a quantum circuit. The target state is defined as:\\n\\n$$\\n|x_0\\\\rangle_N = |0\\\\rangle_N \\\\longrightarrow \\\\sum_{x\\\\in \\\\text{domain}} \\\\sqrt{G(x)}\\\\,|x\\\\rangle_N,\\n$$\\n\\nwith\\n\\n$$\\nG(x) = \\\\frac{\\\\exp(-\\\\lambda x^2)}{\\\\sum_{x\'\\\\in \\\\text{domain}} \\\\exp(-\\\\lambda {x\'}^2)}.\\n$$\\n\\nHere, $\\\\lambda$ (represented by `EXP_RATE` in our code) controls the decay of the Gaussian, and the domain discretization is determined by the `resolution` variable. Our task was to design a quantum circuit that not only achieves a small mean squared error (MSE) compared to the ideal Gaussian state but also scales efficiently as the resolution increases.\\n\\nThere are of course many solutions to this problem. For example, the easiest way is to first prepare a Gaussian state as an amplitude list of length $2^n$ (where $n$ is the number of qubits or `resolution`), then encode this list of amplitudes into the state vector using amplitude encoding. However, this method is costly, requiring $\\\\mathcal{O}(2^n)$ complexity in the worst case scenario. Another method might be using Hamiltonian simulation with trotterization to calculate the exponentila part of the Gaussian, although it results in a true Gaussian, it is also pretty costly. Our solution is not perfect \u2014 it only approximates the Gaussian to an extent, but this approximation is good enough for the MSE error and most importantly, it scales almost linearly to the `resolution` or number of qubits, i.e. $\\\\mathcal{O}(n)$ complexity.\\n\\n## Our Solution\\n\\nOur solution is based on two key steps: first, preparing a state using RY rotations that encodes an exponential factor on each qubit, and second, applying the Quantum Fourier Transform (QFT) to convert that state into one with a Gaussian amplitude distribution. \\n\\n### First Step\\n\\nWhen we apply an RY gate with angle $\\\\theta$ to a qubit initially in $|0\\\\rangle$, we obtain\\n\\n$$\\n\\\\text{RY}(\\\\theta)|0\\\\rangle = \\\\cos\\\\Bigl(\\\\frac{\\\\theta}{2}\\\\Bigr)|0\\\\rangle + \\\\sin\\\\Bigl(\\\\frac{\\\\theta}{2}\\\\Bigr)|1\\\\rangle.\\n$$\\n\\n:::info\\nThe unitary gate matrix of the RY gate is $R_y(\\\\theta) = \\\\begin{bmatrix}\\n\\\\cos\\\\left(\\\\frac{\\\\theta}{2}\\\\right) & -\\\\sin\\\\left(\\\\frac{\\\\theta}{2}\\\\right) \\\\\\\\\\n\\\\sin\\\\left(\\\\frac{\\\\theta}{2}\\\\right) & \\\\cos\\\\left(\\\\frac{\\\\theta}{2}\\\\right)\\n\\\\end{bmatrix}$\\n:::\\n\\nIn our code, for each qubit indexed by $i$ we set\\n\\n$$\\n\\\\theta_i = 2\\\\,\\\\arctan\\\\Bigl(e^{-\\\\frac{i^2}{2\\\\sigma^2}}\\\\Bigr),\\n$$\\n\\nwith\\n\\n$$\\n\\\\sigma = \\\\frac{\\\\text{EXP\\\\_RATE}}{\\\\sqrt{5}}.\\n$$\\n\\nThus, for the $i$th qubit,\\n\\n$$\\n\\\\frac{\\\\theta_i}{2} = \\\\arctan\\\\Bigl(e^{-\\\\frac{i^2}{2\\\\sigma^2}}\\\\Bigr).\\n$$\\n\\nUsing the trigonometric identities $ \\\\cos(\\\\arctan(x)) = \\\\frac{1}{\\\\sqrt{1+x^2}} $ and $ \\\\sin(\\\\arctan(x)) = \\\\frac{x}{\\\\sqrt{1+x^2}} $, with $x = e^{-\\\\frac{i^2}{2\\\\sigma^2}}$, we find\\n\\n$$\\n\\\\cos\\\\Bigl(\\\\frac{\\\\theta_i}{2}\\\\Bigr) = \\\\frac{1}{\\\\sqrt{1+e^{-\\\\frac{i^2}{\\\\sigma^2}}}}, \\\\quad \\\\sin\\\\Bigl(\\\\frac{\\\\theta_i}{2}\\\\Bigr) = \\\\frac{e^{-\\\\frac{i^2}{2\\\\sigma^2}}}{\\\\sqrt{1+e^{-\\\\frac{i^2}{\\\\sigma^2}}}}.\\n$$\\n\\nFor large $i$, $e^{-\\\\frac{i^2}{2\\\\sigma^2}}$ is very small, so the amplitude in $|1\\\\rangle$ is approximately\\n\\n$$\\n\\\\sin\\\\Bigl(\\\\frac{\\\\theta_i}{2}\\\\Bigr) \\\\approx e^{-\\\\frac{i^2}{2\\\\sigma^2}}.\\n$$\\n\\nBy applying these rotations to each of the $n$ qubits, the overall state becomes the tensor product\\n\\n$$\\n|\\\\psi\\\\rangle = \\\\bigotimes_{i=0}^{n-1}\\\\left(\\\\frac{1}{\\\\sqrt{1+e^{-\\\\frac{i^2}{\\\\sigma^2}}}}|0\\\\rangle_i + \\\\frac{e^{-\\\\frac{i^2}{2\\\\sigma^2}}}{\\\\sqrt{1+e^{-\\\\frac{i^2}{\\\\sigma^2}}}}|1\\\\rangle_i\\\\right).\\n$$\\n\\nWhen this state is expressed in the computational basis, a basis state $|j\\\\rangle$ (with $j$ written in binary as $j=\\\\sum_{i=0}^{n-1}j_i2^i$ where each $j_i\\\\in\\\\{0,1\\\\}$) has amplitude\\n\\n$$\\na_j = \\\\prod_{i=0}^{n-1}\\\\left[\\\\cos\\\\Bigl(\\\\frac{\\\\theta_i}{2}\\\\Bigr)\\\\right]^{1-j_i}\\\\left[\\\\sin\\\\Bigl(\\\\frac{\\\\theta_i}{2}\\\\Bigr)\\\\right]^{j_i}.\\n$$\\n\\nBecause $\\\\sin(\\\\frac{\\\\theta_i}{2})\\\\approx e^{-\\\\frac{i^2}{2\\\\sigma^2}}$, the amplitude $a_j$ is a product of factors that decay exponentially with $i$. This product structure means that the probability distribution over $j$ appears as a steep exponential drop in the contributions of higher-index qubits rather than as a smooth Gaussian curve. This is shown in the figure below (only the first few amplitudes are shown):\\n\\n![initial_state](exp_state.png)\\n\\n### Second Step\\n\\nThe next step is to apply the QFT, which acts on computational basis states as\\n\\n$$\\n\\\\text{QFT}\\\\,|j\\\\rangle = \\\\frac{1}{\\\\sqrt{N}}\\\\sum_{k=0}^{N-1} e^{2\\\\pi i\\\\,jk/N}\\\\,|k\\\\rangle,\\n$$\\n\\nwith $N=2^n$. After applying the QFT, the final state becomes\\n\\n$$\\n\\\\text{QFT}\\\\,|\\\\psi\\\\rangle = \\\\sum_{k=0}^{N-1} b_k\\\\,|k\\\\rangle,\\n$$\\n\\nwhere\\n\\n$$\\nb_k = \\\\frac{1}{\\\\sqrt{N}}\\\\sum_{j=0}^{N-1} a_j\\\\,e^{2\\\\pi i\\\\,jk/N}.\\n$$\\n\\nWe can express the sum over $j$ in terms of the binary digits $j_i$. Since $a_j$ is a product over qubit contributions, the sum factorizes, and we have\\n\\n$$\\nb_k = \\\\frac{1}{\\\\sqrt{N}}\\\\prod_{i=0}^{n-1}\\\\left[\\\\cos\\\\Bigl(\\\\frac{\\\\theta_i}{2}\\\\Bigr) + e^{\\\\frac{2\\\\pi i\\\\,2^ik}{N}}\\\\sin\\\\Bigl(\\\\frac{\\\\theta_i}{2}\\\\Bigr)\\\\right].\\n$$\\n\\nSubstituting the approximate forms for $\\\\cos(\\\\frac{\\\\theta_i}{2})$ and $\\\\sin(\\\\frac{\\\\theta_i}{2})$, this becomes\\n\\n$$\\nb_k \\\\approx \\\\frac{1}{\\\\sqrt{N}}\\\\prod_{i=0}^{n-1}\\\\frac{1}{\\\\sqrt{1+e^{-\\\\frac{i^2}{\\\\sigma^2}}}}\\\\left[1 + e^{-\\\\frac{i^2}{2\\\\sigma^2}}e^{\\\\frac{2\\\\pi i\\\\,2^ik}{N}}\\\\right].\\n$$\\n\\nIn the large-$N$ limit and with appropriate scaling of $\\\\sigma$, the sum over $j$ approximates the discrete Fourier transform of a Gaussian function. It is a well-known fact that the Fourier transform of a Gaussian is another Gaussian:\\n\\n$$\\n\\\\int_{-\\\\infty}^{\\\\infty} e^{-\\\\frac{x^2}{2\\\\sigma^2}} e^{2\\\\pi i x k/N}\\\\,dx \\\\propto e^{-2\\\\pi^2 \\\\sigma^2 k^2/N^2},\\n$$\\n\\nwhich implies that the amplitudes $b_k$ in the Fourier basis take the form\\n\\n$$\\nb_k \\\\sim e^{-\\\\alpha k^2},\\n$$\\n\\nup to normalization and a rescaling of parameters, where $\\\\alpha>0$. Thus, although the original state $|\\\\psi\\\\rangle$ in the computational basis exhibits an exponential decay coming from the product of bitwise factors, the QFT\u2019s property of transforming Gaussians into Gaussians yields a smooth Gaussian distribution in the computational basis after applying QFT. Showing this transformation visually, the amplitudes after QFT are:\\n\\n![amplitudes_after_qft](qft_state.png)\\n\\nNote that the Gaussian is \\"inverted\\". To turn it back to a normal Gaussian, we can apply the X gate on the 0th qubit. The final result is:\\n\\n![final_state](final_state.png)\\n\\nUsing the method described above, we can reach a MSE error of $2.230\\\\times10^{-8}$.\\n\\nThe overall circuit is:\\n\\n![overall_circuit](overall_circuit.png)\\n\\n### Scalability of Our Solution\\n\\nThe Phase 2 of the challenge is to maximize the scalability of our solution, i.e. make our solution works at the highest resolution with the least number of gates possible. The bottleneck of our solution described above was the QFT part, the number of gates in a regular QFT grows quadratically as the number of qubits increases:\\n\\n$$\\n\\\\mathcal{O}\\\\left(\\\\frac{n^2+n}{2}\\\\right) \\\\approx \\\\mathcal{O}(n^2).\\n$$\\n\\nWe observed that the gate angles of the CPhase gates used in QFT decreases exponentially! The gate angle for the CPhase gate with layer number $j$ and qubit number $k$ is\\n\\n$$\\n\\\\frac{2\\\\pi}{2^{k-j+1}}.\\n$$\\n\\nWhen there are many qubits, the majority of the gate angles often become very small (e.g. less than $1\\\\times10^{-3}$), which have negligible effects on the overall state vector. Therefore, we pruned any gates with angle smaller than a threshold $\\\\theta$ as shown in the diagram below:\\n\\n![gate_prune](gate_prune.png)\\n\\nNow the number of gates in the approximated QFT with qubit number $n$ and $\\\\theta=1\\\\times10^{-2}$ is\\n\\n$$\\n\\\\mathcal{O}\\\\bigl(n+\\\\sum_{j=1}^{n}\\\\text{min}(8,n-j)\\\\bigr) \\\\approx \\\\mathcal{O}(n).\\n$$\\n\\nWe did an experiment to compare the effect of different choices of $\\\\theta$ with 18 qubits (`resolution = 18`), below is a table summarising the results:\\n\\n| $\\\\theta$         | Number of CX Gates | MSE error             |\\n| ---------------- | ------------------ | --------------------- |\\n| $1\\\\times10^{-3}$ | 293                | $1.564\\\\times10^{-14}$ |\\n| $1\\\\times10^{-2}$ | 245                | $1.601\\\\times10^{-14}$ |\\n| $1\\\\times10^{-1}$ | 153                | $1.098\\\\times10^{-13}$ |\\n\\nWe chose $\\\\theta=1\\\\times10^{-2}$ in our final submission for a balance of complexity and accuracy. The figure below shows the ratio between the number of CX gates and the resolution, which can be seen that the relation becomes linear if the resolution is large enough.\\n\\n![scalability](https://docs.google.com/spreadsheets/d/e/2PACX-1vRSAaAXzWFhnhRiy2nwQ4QAfic7GpYxCQGUsp-dkcqI05yOARGQ_1d5sF5zOA_qDN43n2GGEPpsXER3/pubchart?oid=941513203&format=image)\\n\\nHere is an additional table summarising another experiment that compares the difference in MSE error when we change the resolution $n$ from 5 to 18 ($\\\\theta=1\\\\times10^{-2}$):\\n\\n| $n$  | Number of CX Gates | MSE error             |\\n| ---- | ------------------ | --------------------- |\\n| $5$  | 28                 | $2.515\\\\times10^{-5}$  |\\n| $8$  | 70                 | $2.230\\\\times10^{-8}$  |\\n| $12$ | 140                | $6.526\\\\times10^{-11}$ |\\n| $15$ | 191                | $1.024\\\\times10^{-12}$ |\\n| $18$ | 245                | $1.601\\\\times10^{-14}$ |\\n\\nThe MSE decreases as the resolution increases. This is because the overall probability scale is smaller when resolution is higher, leading to decreasing MSE. The Gaussian is also more fine-grained as resolution increases, as shown in the following figure:\\n\\n![r8and12](r8and12.png)\\n\\n### Code of Our Solution\\n\\nBelow is the code we used for submission. It implements the two-step approach (product-state preparation + approximate QFT) and demonstrates how gate pruning is applied:\\n\\n```python\\ndef create_solution(resolution: int):\\n    fraction_digits = resolution - 2\\n    EXP_RATE = 1\\n    \\n    import math\\n    \\n    @qfunc\\n    def single_X(x_arr: QArray[QBit]):\\n        X(x_arr[0])\\n\\n    @qfunc\\n    def prepare_state(q: QArray[QBit]) :\\n        for i in range(resolution):\\n            exponent = (i ** 2)/(-2 * (EXP_RATE / math.sqrt(5)) ** 2)\\n            angle = 2.0 * math.atan(math.exp(exponent))\\n            if angle > 1e-6:\\n                RY(angle, q[i])\\n        CRY(-math.pi/42, q[0], q[1]) # a small \\"hand-tuned tweak\\"\\n\\n    @qfunc\\n    def approx_qft(target: QArray[QBit]):\\n        for i in range(resolution // 2):\\n            SWAP(target[i], target[resolution - i - 1])\\n            \\n        for j in range(resolution):\\n            H(target[j])\\n            \\n            for k in range(j+1, resolution):\\n                theta = 2 * math.pi / (2 ** (k - j + 1))\\n                if theta > 1e-2: # 9 qubits\\n                    CPHASE(theta, target[k], target[j])\\n    \\n    @qfunc\\n    def prepare_gaussian(x: QNum):\\n        prepare_state(x)\\n        approx_qft(x)\\n        single_X(x)\\n        \\n    return prepare_gaussian\\n```\\n\\n## Discussion and Future Work\\n\\nOur pruning strategy for the QFT threshold $\\\\theta$ was largely heuristic. Further adjusting $\\\\theta$ (either making it adaptive to local circuit conditions or tuning it per qubit layer) may unlock even better trade-offs between gate count and fidelity. An in-depth exploration of different pruning schedules could systematically find the best thresholds for specific MSE targets.\\n\\nAdditionally, while our approach was tested in noiseless simulations, real devices inevitably introduce errors. **Testing under realistic noise models** (e.g., depolarizing or amplitude-damping channels) will help identify how robust the approximate QFT and the initial product-state preparation are in practice. We plan to **run the circuit on a real quantum computer** to gauge the impact of gate errors, crosstalk, and qubit decoherence.\\n\\nFinally, Gaussian states have broad relevance to **quantum machine learning (QML)**, e.g., for encoding continuous data distributions, generating feature maps, or performing kernel-based methods on near-term hardware. A natural next step is to investigate how well our circuit-based Gaussian preparation integrates into QML workflows, particularly in tasks such as clustering or dimensionality reduction. Improved scalability and resilience against hardware noise would make this approach more appealing for real-world QML applications.\\n\\n## Conclusion\\n\\nWe have presented an approach that efficiently prepares a Gaussian state in the symmetrical domain using a circuit of nearly linear complexity. By applying a simple product\u2010state preparation followed by an approximate QFT with pruning, we achieve a low MSE while keeping the gate count under control. This technique can be extended or refined for many practical tasks that rely on Gaussian states, especially as quantum hardware scales to higher qubit numbers.\\n\\n## Acknowledgements\\n\\nWe thank MIT and Classiq for organizing MIT iQuHACK 2025, and we extend our gratitude to the mentors and judges for their invaluable guidance and support throughout the competition."},{"id":"intro-to-quantum-computing","metadata":{"permalink":"/blog/intro-to-quantum-computing","source":"@site/blog/2024-1-5-intro-to-quantum-computing/index.md","title":"Introduction to Quantum Computing","description":"The basics of quantum computing and some hands-on trying.","date":"2024-01-05T00:00:00.000Z","tags":[{"label":"beginners","permalink":"/blog/tags/beginners"},{"label":"quantum","permalink":"/blog/tags/quantum"},{"label":"qiskit","permalink":"/blog/tags/qiskit"},{"label":"pytorch","permalink":"/blog/tags/pytorch"},{"label":"tutorial","permalink":"/blog/tags/tutorial"}],"readingTime":11.275,"hasTruncateMarker":true,"authors":[{"name":"Eason Xie","title":"Some Random Tree","url":"https://easonoob.github.io","imageURL":"https://avatars.githubusercontent.com/u/100521878?v=4","key":"eason"}],"frontMatter":{"slug":"intro-to-quantum-computing","title":"Introduction to Quantum Computing","authors":"eason","tags":["beginners","quantum","qiskit","pytorch","tutorial"]},"unlisted":false,"prevItem":{"title":"Preparing a Gaussian State in the Symmetrical Domain","permalink":"/blog/mit-iquhack-solution"},"nextItem":{"title":"Introduction to PyTorch","permalink":"/blog/intro-to-pytorch"}},"content":"The basics of quantum computing and some hands-on trying.\\n\\n\x3c!--truncate--\x3e\\n\\n## Prerequisites\\n- Basic Maths &mdash; linear algebra\\n- Basic quantum mechanics concepts &mdash; Superposition, the Uncertainty Principle, and Entanglement\\n- Basic Python\\n- Imagination power\\n\\n## Quantum Mechanics\\n> \u201cThe mathematical framework of quantum theory has passed countless successful tests and is now universally accepted as a consistent and accurate description of all atomic phenomena.\u201d ~ Erwin Schr\xf6dinger\\n\\nWe will not be talking about the advanced maths or concepts of quantum mechanics in this blog post, such as the Uncertainty Principle or the Schr\xf6dinger Equation, but I\'ll assume you know the main principles of Quantum Mechanics, including Superposition, Wave-Particle Duality, the Uncertainty Principle, and Entanglement. The concept of qubits and quantum circuits gates will be explained below.\\n\\n### Qubits\\nIn normal semiconductor computers, each \\"bit\\" can only represent two values: 0 and 1, which can be used in calculations as low and high voltage. However, in quantum computers, \\"qubits\\" (quantum bit) are used. Now instead of representing only 0 **or** 1, it can now represent 0 **and** 1 simultaneously. It carries a \\"state\\" $|\\\\psi\\\\rangle$ (written in [Bra-ket notation](https://en.wikipedia.org/wiki/Bra%E2%80%93ket_notation)), which is a vector defined as followed in a qubit:\\n\\n$$\\n|\\\\psi\\\\rangle = \\\\alpha |0\\\\rangle + \\\\beta |1\\\\rangle\\n$$\\n\\nWhere\\n\\n$$\\n|\\\\alpha|^2 + |\\\\beta|^2 = 1\\n$$\\n\\nto ensure the state is normalized. $\\\\alpha$ and $\\\\beta$ represents the **amplitudes** of the qubit in the $|0\\\\rangle$ and $|1\\\\rangle$ states respectively, and are **complex numbers**. To get the probability of measuring a certain state $|0\\\\rangle$ or $|1\\\\rangle$ from $|\\\\psi\\\\rangle$, we can apply the Born rule (given that $|0\\\\rangle$ and $|1\\\\rangle$ are orthonormal, which means orthogonal and normalized):\\n\\n$$\\nP(0) = |\\\\langle0|\\\\psi\\\\rangle|^2\\n$$\\nand\\n$$\\nP(1) = |\\\\langle1|\\\\psi\\\\rangle|^2.\\n$$\\n\\nNow, if we have two qubits entangled, we can represent $2^2 = 4$ states ($00, 01, 10, 11$), each associated with a probability.\\n\\n$$\\n|\\\\psi\\\\rangle = c_1 |0\\\\rangle + c_2 |1\\\\rangle\\n$$\\n$$\\n|\\\\phi\\\\rangle = c_3 |0\\\\rangle + c_4 |1\\\\rangle\\n$$\\n$$\\n|combined\\\\rangle = c_1c_3 |00\\\\rangle + c_1c_4 |01\\\\rangle + c_2c_3 |10\\\\rangle + c_2c_4 |11\\\\rangle\\n$$\\n\\nThe power of quantum computing is you can represent $2^n$ states if you have $n$ qubits, which is an exponential growth. For example, if you have 20 qubits, you can encode a superposition over all $2^{20} = 1048576$ basis states simultaneously, but in a classical computer, 20 bits can only be in 1 of the $2^{20}$ possible configurations at any given time. Similarly, a system with 266 qubits would have a state space with a dimensionality on the order of $10^{80}$, which is roughly equivalent to the estimated number of atoms in the observable universe! When qubits are entangled, quantum gates (which are unitary operations) act on the entire state vector, modifying the amplitudes of all basis states via quantum interference. This ability to manipulate a vast, exponentially large state space is central to the potential power of quantum computing. Now we will talk about these quantum gates.\\n\\n### Gates\\n\\nTo visualize a single qubit, you can use the Bloch Sphere:\\n![Bloch Sphere](bloch_sphere.png)\\nThe z-axis corresponds to the probability in being measured in $|0\\\\rangle$ and $|1\\\\rangle$. X-axis and y-axis also corresponds to being measured in $|+\\\\rangle$, $|-\\\\rangle$, $|+i\\\\rangle$, and $|-i\\\\rangle$ respectively.\\n\\n:::info\\n$$\\n|+\\\\rangle = \\\\frac{1}{\\\\sqrt{2}}(|0\\\\rangle + |1\\\\rangle)\\n$$\\n$$\\n|-\\\\rangle = \\\\frac{1}{\\\\sqrt{2}}(|0\\\\rangle - |1\\\\rangle)\\n$$\\n$$\\n|+i\\\\rangle = \\\\frac{1}{\\\\sqrt{2}}(|0\\\\rangle + i|1\\\\rangle)\\n$$\\n$$\\n|-i\\\\rangle = \\\\frac{1}{\\\\sqrt{2}}(|0\\\\rangle - i|1\\\\rangle)\\n$$\\n:::\\n\\nQuantum circuit gates can be visualized as rotating the state vector (the red arrow) in the bloch sphere. Here are some examples:\\n\\n1. **Pauli-X Gate (NOT Gate)**:\\n   - Rotation: $$ \\\\pi $$ radians (180 degrees) around the X-axis.\\n   - Effect: Transforms the state $$ |0\\\\rangle $$ to $$ |1\\\\rangle $$ and vice versa. On the Bloch sphere, it flips the state from the north pole to the south pole and vice versa.\\n\\n2. **Pauli-Y Gate**:\\n   - Rotation: $$ \\\\pi $$ radians (180 degrees) around the Y-axis.\\n   - Effect: Applies a complex phase and swaps the states $$ |0\\\\rangle $$ and $$ |1\\\\rangle $$. For example, $$ |0\\\\rangle $$ becomes $$ i|1\\\\rangle $$ and $$ |1\\\\rangle $$ becomes $$ -i|0\\\\rangle $$.\\n\\n3. **Pauli-Z Gate**:\\n   - Rotation: $$ \\\\pi $$ radians (180 degrees) around the Z-axis.\\n   - Effect: Leaves the state $$ |0\\\\rangle $$ unchanged and adds a phase of $$ \\\\pi $$ (equivalent to a factor of -1) to the state $$ |1\\\\rangle $$.\\n\\n4. **Hadamard Gate (H Gate)**:\\n   - Rotation: This gate performs a more complex transformation, equivalent to rotating around the axis at 45 degrees from both X and Z axes, followed by a $$ \\\\pi $$ radian rotation around the Y-axis.\\n   - Effect: Creates superpositions from basis states, turning $$ |0\\\\rangle $$ into $$ \\\\frac{|0\\\\rangle + |1\\\\rangle}{\\\\sqrt{2}} $$ and $$ |1\\\\rangle $$ into $$ \\\\frac{|0\\\\rangle - |1\\\\rangle}{\\\\sqrt{2}} $$, which is the $|+\\\\rangle$ state.\\n\\n5. **S Gate (Phase Gate)**:\\n   - Rotation: $$ \\\\frac{\\\\pi}{2} $$ radians (90 degrees) around the Z-axis.\\n   - Effect: Leaves the state $$ |0\\\\rangle $$ unchanged and multiplies the state $$ |1\\\\rangle $$ by $$ i $$.\\n\\n6. **T Gate**:\\n   - Rotation: $$ \\\\frac{\\\\pi}{4} $$ radians (45 degrees) around the Z-axis.\\n   - Effect: Leaves the state $$ |0\\\\rangle $$ unchanged and multiplies the state $$ |1\\\\rangle $$ by $$ e^{i\\\\pi/4} $$, which is a more subtle phase shift than the S gate.\\n\\nAnd some parameterized gates:\\n\\n7. **RX Gate**:\\n    - Rotation: $$\\\\theta$$ radians around the X-axis. It can be represented as a matrix multiplication between the unitary gate matrix and the quantum states, the gate matrix is:\\n\\n$$\\nR_x(\\\\theta) = \\\\begin{bmatrix}\\n\\\\cos\\\\left(\\\\frac{\\\\theta}{2}\\\\right) & -i\\\\sin\\\\left(\\\\frac{\\\\theta}{2}\\\\right) \\\\\\\\\\n-i\\\\sin\\\\left(\\\\frac{\\\\theta}{2}\\\\right) & \\\\cos\\\\left(\\\\frac{\\\\theta}{2}\\\\right)\\n\\\\end{bmatrix}\\n$$\\n\\n8. **RY Gate**:\\n    - Rotation: $$\\\\theta$$ radians around the y-axis:\\n$$\\nR_y(\\\\theta) = \\\\begin{bmatrix}\\n\\\\cos\\\\left(\\\\frac{\\\\theta}{2}\\\\right) & -\\\\sin\\\\left(\\\\frac{\\\\theta}{2}\\\\right) \\\\\\\\\\n\\\\sin\\\\left(\\\\frac{\\\\theta}{2}\\\\right) & \\\\cos\\\\left(\\\\frac{\\\\theta}{2}\\\\right)\\n\\\\end{bmatrix}\\n$$\\n\\n8. **RZ Gate**:\\n    - Rotation: $$\\\\theta$$ radians around the z-axis:\\n$$\\nR_z(\\\\theta) = \\\\begin{bmatrix}\\ne^{-i\\\\theta/2} & 0 \\\\\\\\\\n0 & e^{i\\\\theta/2}\\n\\\\end{bmatrix}\\n$$\\n\\nAnd much more.\\n\\n:::danger[INFO]\\nEvery gate is represented as an unitary gate matrix, e.g. the Hadamard Gate:\\n$$\\nH = \\\\frac{1}{\\\\sqrt{2}}\\\\begin{bmatrix}\\n1 & 1 \\\\\\\\\\n1 & -1\\n\\\\end{bmatrix}\\n$$\\n:::\\n\\nAlso, there are double-qubits gate, for example the CNOT (Controlled-NOT) gate:\\n$$\\nCNOT = \\\\begin{bmatrix}\\n1 & 0 & 0 & 0 \\\\\\\\\\n0 & 1 & 0 & 0 \\\\\\\\\\n0 & 0 & 0 & 1 \\\\\\\\\\n0 & 0 & 1 & 0\\n\\\\end{bmatrix}\\n$$\\nControlled gates mean if the first qubit is $|1\\\\rangle$, then the gate (NOT in this case) on second qubit will be performed. There are even triple-qubits gates, like the Toffoli gate (a.k.a. CCNOT), which has a $2^n \\\\times 2^n = 8 \\\\times 8$ size gate matrix.\\n\\nIf you have a quantum states, say 10 qubits, and you want to apply a specific gate on a specific qubit, you can use the Kronecker product to construct a $2^n \\\\times 2^n$ gate matrix and apply it to the quantum states, for example you want to apply the Hadamard gate on the 4th qubit:\\n\\n$$\\n|\\\\psi_{t+1}\\\\rangle = |\\\\psi_t\\\\rangle \\\\times (I \\\\otimes I \\\\otimes I \\\\otimes \\\\frac{1}{\\\\sqrt{2}}\\\\begin{bmatrix}1 & 1 \\\\\\\\ 1 & -1\\\\end{bmatrix} \\\\otimes I \\\\otimes I \\\\otimes I \\\\otimes I \\\\otimes I \\\\otimes I)\\n$$\\n\\n:::tip\\n$I$ is the identity matrix, i.e. $\\\\begin{bmatrix}1 & 0 \\\\\\\\ 0 & 1\\\\end{bmatrix}$\\n:::\\n\\nThese quantum gates are crucial in constructing quantum circuits for algorithms, where each type of gate contributes to manipulating the qubit\'s state in a controlled manner to achieve various computational goals.\\n\\n## Simulation with Qiskit\\n\\nQiskit is an open-source framework for quantum computing. It provides the tools for creating quantum circuits, simulating them, and running them on real quantum hardware through IBM\'s cloud services.\\n\\n### Setting Up Your Environment\\n\\nTo start using Qiskit, make sure Python is installed on your machine, then install Qiskit and Qiskit Aer, which includes simulators that run on your local machine.\\n\\n```bash\\npip install qiskit qiskit-aer pylatexenc\\n```\\n\\n### Single Qubit Operations\\n\\n#### Building the Circuit\\n\\nWe\'ll begin with a basic single-qubit circuit to demonstrate sequential gate operations that include rotations and a phase shift.\\n\\n```python\\nfrom qiskit import QuantumCircuit, transpile\\nfrom qiskit_aer import Aer\\nfrom qiskit.visualization import plot_bloch_multivector\\n\\n# Initialize a Quantum Circuit with 1 qubit\\nqc_single = QuantumCircuit(1)\\n\\n# Apply rotation around the X-axis\\nqc_single.rx(3.14159 / 4, 0)  # Pi/4 rotation\\n\\n# Apply rotation around the Y-axis\\nqc_single.ry(3.14159 / 2, 0)  # Pi/2 rotation\\n\\n# Apply a Z gate\\nqc_single.z(0)\\n\\n# Visualize the circuit\\nprint(\\"Single Qubit Circuit:\\")\\nprint(qc_single.draw(output=\'mpl\'))\\n```\\n\\n#### Calculating the Expected State Vector\\n\\nThe state vector undergoes a rotation by $$\\\\pi/4$$ radians around the X-axis, followed by $$\\\\pi/2$$ around the Y-axis, and finally, a phase shift due to the Z gate.\\n\\n#### Simulation and Output\\n\\n```python\\n# Prepare the simulator\\nsimulator = Aer.get_backend(\'statevector_simulator\')\\n\\n# Transpile the circuit for the simulator\\ntranspiled_qc = transpile(qc_single, simulator)\\n\\n# Run the simulation\\njob = simulator.run(transpiled_qc)\\nresult = job.result()\\n\\n# Get the final state vector\\nstatevector = result.get_statevector()\\nprint(statevector)\\n\\n# Plot the state on the Bloch sphere\\nplot_bloch_multivector(statevector)\\n```\\n\\n### Multi-Qubit Operations\\n\\n#### Building the Circuit\\n\\nWe will construct a five-qubit circuit with a combination of entanglement, rotations, and phase shifts to demonstrate the interplay of different quantum gates.\\n\\n```python\\nfrom qiskit import QuantumCircuit, transpile\\nfrom qiskit_aer import Aer\\nfrom qiskit.visualization import plot_bloch_multivector\\nfrom math import pi\\n\\n# Initialize a Quantum Circuit with 5 qubits\\nqc_multi = QuantumCircuit(5)\\n\\n# Apply a Hadamard gate to qubit 0\\nqc_multi.h(0)\\n\\n# Apply a CNOT gate between qubit 0 and qubit 1\\nqc_multi.cx(0, 1)\\n\\n# Apply a rotation around the X-axis by pi/2 on qubit 1\\nqc_multi.rx(pi / 2, 1)\\n\\n# Apply a T gate to qubit 2\\nqc_multi.t(2)\\n\\n# Apply a CNOT gate between qubit 1 and qubit 2\\nqc_multi.cx(1, 2)\\n\\n# Apply a rotation around the Z-axis by pi/4 on qubit 3\\nqc_multi.rz(pi / 4, 3)\\n\\n# Apply a Hadamard gate to qubit 4\\nqc_multi.h(4)\\n\\n# Apply a CNOT gate between qubit 4 and qubit 3\\nqc_multi.cx(4, 3)\\n\\n# Apply a CNOT gate between qubit 1 and qubit 4\\nqc_multi.cx(1, 4)\\n\\n# Visualize the Circuit\\nprint(\\"Five-Qubit Circuit:\\")\\nprint(qc_multi.draw(output=\'mpl\'))\\n```\\n\\n#### Calculating the Expected State Vector\\n\\nNow let\'s calculate the expected state vector after applying the specified sequence of gates:\\n\\n1. **Initial State**  \\n   **Operation**: All qubits start in $\\\\lvert 00000\\\\rangle$.  \\n   **State now**:  \\n   $$\\n     \\\\lvert 00000\\\\rangle.\\n   $$\\n\\n2. **After $H(0)$**  \\n   **Operation**: A Hadamard on qubit\u202f0 creates an equal superposition of $\\\\lvert 0\\\\rangle$ and $\\\\lvert 1\\\\rangle$ (for that qubit).  \\n   **State now**:\\n   $$\\n     \\\\frac{1}{\\\\sqrt{2}}\\n     \\\\bigl(\\\\lvert 00000\\\\rangle + \\\\lvert 10000\\\\rangle\\\\bigr).\\n   $$\\n\\n3. **After $\\\\mathrm{CNOT}(0,1)$**  \\n   **Operation**: Qubit\u202f0 is the control, qubit\u202f1 is the target. If qubit\u202f0=1, flip qubit\u202f1.  \\n   **State now**:\\n   $$\\n     \\\\frac{1}{\\\\sqrt{2}}\\n     \\\\bigl(\\\\lvert 00000\\\\rangle + \\\\lvert 11000\\\\rangle\\\\bigr).\\n   $$\\n\\n4. **After $\\\\mathrm{RX}\\\\bigl(1,\\\\tfrac{\\\\pi}{2}\\\\bigr)$**  \\n   **Operation**: A rotation by $\\\\theta=\\\\frac{\\\\pi}{2}$ around the $x$\u2010axis acts on qubit\u202f1. Recall:\\n   $$\\n     R_x\\\\!\\\\Bigl(\\\\tfrac{\\\\pi}{2}\\\\Bigr)\\n     \\\\lvert 0\\\\rangle\\n       = \\\\tfrac{1}{\\\\sqrt{2}}\\\\bigl(\\\\lvert 0\\\\rangle - i\\\\,\\\\lvert 1\\\\rangle\\\\bigr), \\n     \\\\quad\\n     R_x\\\\!\\\\Bigl(\\\\tfrac{\\\\pi}{2}\\\\Bigr)\\n     \\\\lvert 1\\\\rangle\\n       = \\\\tfrac{1}{\\\\sqrt{2}}\\\\bigl(-\\\\,i\\\\,\\\\lvert 0\\\\rangle + \\\\lvert 1\\\\rangle\\\\bigr).\\n   $$\\n   Since the state so far is $\\\\tfrac{1}{\\\\sqrt{2}}(\\\\lvert 00000\\\\rangle + \\\\lvert 11000\\\\rangle)$, applying $\\\\mathrm{RX}(\\\\pi/2)$ to qubit\u202f1 in each term yields four basis states:  \\n   **State now**:\\n   $$\\n     \\\\frac{1}{2}\\n     \\\\Bigl(\\n       \\\\lvert 00000\\\\rangle\\n       \\\\;-\\\\; i\\\\,\\\\lvert 01000\\\\rangle\\n       \\\\;-\\\\; i\\\\,\\\\lvert 10000\\\\rangle\\n       \\\\;+\\\\; \\\\lvert 11000\\\\rangle\\n     \\\\Bigr).\\n   $$\\n\\n5. **After $T(2)$**  \\n   **Operation**: The $T$ gate adds a phase $e^{i\\\\pi/4}$ if qubit\u202f2 is $\\\\lvert 1\\\\rangle$. Here, in all four terms above, qubit\u202f2=$\\\\lvert 0\\\\rangle$, so this gate has **no effect**.  \\n   **State now** (unchanged):\\n   $$\\n     \\\\frac{1}{2}\\n     \\\\Bigl(\\n       \\\\lvert 00000\\\\rangle\\n       \\\\;-\\\\; i\\\\,\\\\lvert 01000\\\\rangle\\n       \\\\;-\\\\; i\\\\,\\\\lvert 10000\\\\rangle\\n       \\\\;+\\\\; \\\\lvert 11000\\\\rangle\\n     \\\\Bigr).\\n   $$\\n\\n6. **After $\\\\mathrm{CNOT}(1,2)$**  \\n   **Operation**: Qubit\u202f1 is the control, qubit\u202f2 is the target. If qubit\u202f1=1, flip qubit\u202f2.  \\n   - $\\\\lvert 01000\\\\rangle \\\\to \\\\lvert 01100\\\\rangle$.  \\n   - $\\\\lvert 11000\\\\rangle \\\\to \\\\lvert 11100\\\\rangle$.  \\n   **State now**:\\n   $$\\n     \\\\frac{1}{2}\\n     \\\\Bigl(\\n       \\\\lvert 00000\\\\rangle\\n       \\\\;-\\\\; i\\\\,\\\\lvert 01100\\\\rangle\\n       \\\\;-\\\\; i\\\\,\\\\lvert 10000\\\\rangle\\n       \\\\;+\\\\; \\\\lvert 11100\\\\rangle\\n     \\\\Bigr).\\n   $$\\n\\n7. **After $\\\\mathrm{RZ}\\\\bigl(3,\\\\tfrac{\\\\pi}{4}\\\\bigr)$**  \\n   **Operation**: A rotation by $\\\\tfrac{\\\\pi}{4}$ about $Z$ on qubit\u202f3 adds a phase to the $\\\\lvert 1\\\\rangle$ state of that qubit. However, qubit\u202f3=$\\\\lvert 0\\\\rangle$ for all four terms, so only a global phase results (which can be ignored).  \\n   **State now** (unchanged up to global phase):\\n   $$\\n     \\\\frac{1}{2}\\n     \\\\Bigl(\\n       \\\\lvert 00000\\\\rangle\\n       \\\\;-\\\\; i\\\\,\\\\lvert 01100\\\\rangle\\n       \\\\;-\\\\; i\\\\,\\\\lvert 10000\\\\rangle\\n       \\\\;+\\\\; \\\\lvert 11100\\\\rangle\\n     \\\\Bigr).\\n   $$\\n\\n8. **After $H(4)$**  \\n   **Operation**: A Hadamard on qubit\u202f4 (the rightmost qubit) transforms $\\\\lvert 0\\\\rangle\\\\mapsto(\\\\lvert 0\\\\rangle+\\\\lvert 1\\\\rangle)/\\\\sqrt{2}$. Each existing term with qubit\u202f4=0 splits into two terms.  \\n   **State now**:\\n   $$\\n     \\\\frac{1}{2\\\\sqrt{2}}\\n     \\\\Bigl(\\n       \\\\lvert 00000\\\\rangle + \\\\lvert 00001\\\\rangle\\n       \\\\;-\\\\; i\\\\,\\\\lvert 01100\\\\rangle - i\\\\,\\\\lvert 01101\\\\rangle\\n       \\\\;-\\\\; i\\\\,\\\\lvert 10000\\\\rangle - i\\\\,\\\\lvert 10001\\\\rangle\\n       \\\\;+\\\\; \\\\lvert 11100\\\\rangle + \\\\lvert 11101\\\\rangle\\n     \\\\Bigr).\\n   $$\\n\\n9. **After $\\\\mathrm{CNOT}(4,3)$**  \\n   **Operation**: Qubit\u202f4 is the control, qubit\u202f3 is the target. If qubit\u202f4=1, flip qubit\u202f3.  \\n   **State now**:\\n   $$\\n     \\\\frac{1}{2\\\\sqrt{2}}\\n     \\\\Bigl(\\n       \\\\lvert 00000\\\\rangle \\n       + \\\\lvert 00011\\\\rangle\\n       \\\\;-\\\\; i\\\\,\\\\lvert 01100\\\\rangle\\n       \\\\;-\\\\; i\\\\,\\\\lvert 01111\\\\rangle\\n       \\\\;-\\\\; i\\\\,\\\\lvert 10000\\\\rangle\\n       \\\\;-\\\\; i\\\\,\\\\lvert 10011\\\\rangle\\n       \\\\;+\\\\; \\\\lvert 11100\\\\rangle\\n       + \\\\lvert 11111\\\\rangle\\n     \\\\Bigr).\\n   $$\\n\\n10. **After $\\\\mathrm{CNOT}(1,4)$**  \\n    **Operation**: Finally, qubit\u202f1 is the control and qubit\u202f4 is the target. Whenever qubit\u202f1=1, flip qubit\u202f4.  \\n    **State now** (final):\\n    $$\\n      \\\\frac{1}{2\\\\sqrt{2}}\\n      \\\\Bigl(\\n        \\\\lvert 00000\\\\rangle\\n        \\\\;+\\\\; \\\\lvert 00011\\\\rangle\\n        \\\\;-\\\\; i\\\\,\\\\lvert 01101\\\\rangle\\n        \\\\;-\\\\; i\\\\,\\\\lvert 01110\\\\rangle\\n        \\\\;-\\\\; i\\\\,\\\\lvert 10000\\\\rangle\\n        \\\\;-\\\\; i\\\\,\\\\lvert 10011\\\\rangle\\n        \\\\;+\\\\; \\\\lvert 11101\\\\rangle\\n        \\\\;+\\\\; \\\\lvert 11110\\\\rangle\\n      \\\\Bigr).\\n    $$\\n\\nThis eight\u2010term superposition (up to any global phase) is the final state of the five\u2010qubit system after all gates have been applied.\\n\\n#### Simulation and Output\\n\\n```python\\n# Prepare the simulator\\nsimulator = Aer.get_backend(\'statevector_simulator\')\\n\\n# Transpile the circuit for the simulator\\ntranspiled_qc = transpile(qc_multi, simulator)\\n\\n# Run the simulation\\njob = simulator.run(transpiled_qc)\\nresult = job.result()\\n\\n# Get the final state vector\\nstatevector = result.get_statevector()\\nprint(statevector)\\n\\n# Plot the state on the Bloch sphere\\nplot_bloch_multivector(statevector)\\n```\\n\\n## Tasks you may do\\n1. Implement Shor\'s Algorithm for finding the prime factors of an integer.\\n2. Implement Grover\'s Algorithm for an unstructured search that finds with high probability the unique input to a black box function that produces a particular output value."},{"id":"intro-to-pytorch","metadata":{"permalink":"/blog/intro-to-pytorch","source":"@site/blog/2023-12-13-intro-to-pytorch/index.md","title":"Introduction to PyTorch","description":"This blog post will be about some basic introductions to PyTorch, including tensors, and how to train your own model in PyTorch.","date":"2023-12-13T00:00:00.000Z","tags":[{"label":"beginners","permalink":"/blog/tags/beginners"},{"label":"nn","permalink":"/blog/tags/nn"},{"label":"pytorch","permalink":"/blog/tags/pytorch"},{"label":"tutorial","permalink":"/blog/tags/tutorial"},{"label":"tired","permalink":"/blog/tags/tired"}],"readingTime":5.82,"hasTruncateMarker":true,"authors":[{"name":"Eason Xie","title":"Some Random Tree","url":"https://easonoob.github.io","imageURL":"https://avatars.githubusercontent.com/u/100521878?v=4","key":"eason"}],"frontMatter":{"slug":"intro-to-pytorch","title":"Introduction to PyTorch","authors":"eason","tags":["beginners","nn","pytorch","tutorial","tired"]},"unlisted":false,"prevItem":{"title":"Introduction to Quantum Computing","permalink":"/blog/intro-to-quantum-computing"},"nextItem":{"title":"The Most Basics of Neural Networks","permalink":"/blog/basic-of-nn"}},"content":"This blog post will be about some basic introductions to PyTorch, including tensors, and how to train your own model in PyTorch.\\n\\n\x3c!--truncate--\x3e\\n\\n## Prerequisites\\n- Basic Maths\\n- Basic Python\\n\\n## Aims\\n1.\\tUnderstand tensors\\n2.\\tUnderstand the steps to build and train a model in PyTorch\\n\\n## Introduction\\n> \u201cArtificial Intelligence, deep learning, machine learning \u2014 whatever you\'re doing if you don\'t understand it \u2014 learn it. Because otherwise you\'re going to be a dinosaur within 3 years.\u201d ~ Mark Cuban.\\n\\n## Tensors\\n[https://pytorch.org/tutorials/beginner/basics/tensorqs_tutorial.html](https://pytorch.org/tutorials/beginner/basics/tensorqs_tutorial.html)\\n\\nTensors are specialized arrays or matrices. It can have as many dimensions as you want. We use tensors to encode the data in the PyTorch, as well as the model parameters. Specifically, if you perform operations on a PyTorch tensor (e.g. addition, multiplication), your action will be saved into PyTorch\u2019s built-in dynamic computation graph, which is necessary for model backpropagation in training.\\n\\nHere\u2019s an example of some tensors: \\n\\n```python\\nimport torch\\n\\n# Create a tensor from a list/array:\\n\\ndata1 = [0, 1, 2, 3, 4, 5] # 1D array\\ndata2 = [[0, 1], [2, 3], [4, 5]] # 2D array\\n\\ntensor1 = torch.tensor(data1)\\ntensor2 = torch.tensor(data2)\\n\\nprint(tensor1.shape) # torch.Size([6])\\nprint(tensor2.shape) # torch.Size([3, 2])\\n\\n# Create a tensor filled with zeros or ones:\\n\\nzeros_tensor = torch.zeros(3, 5, 4, 2, 1, 4) # A 6D tensor\\nones_tensor = torch.ones(3, 5)\\n\\n# Create a random tensor from a specified shape:\\n\\nrandom_tensor1 = torch.randn(10, 20) # A 10 by 20 tensor with normal distribution\\nprint(random_tensor1.shape) # torch.Size([10, 20])\\nprint(random_tensor1[0, :10]) # You can use python array index slicing\\n\\nrandom_tensor2 = torch.randint(0, 100, (5, 10)) # 5 by 10 tensor with integer values ranging from 0 to 100\\nprint(random_tensor2[0, 3:10])\\n\\n# Attributes of a tensor:\\n\\nprint(tensor1.shape)\\nprint(tensor1.dtype) # data type, e.g. float32, int64, float64, Bfloat16\\nprint(tensor1.device) # device the tensor is stored on, e.g. cpu, cuda\\n```\\n\\n## Training a Simple Model in PyTorch\\n\\nThe steps of training a model in PyTorch includes:\\n1.\\tFind a dataset that suits your problem, download it, and create a dataloader.\\n2.\\tDefine your model, you can create your own model module using `nn.Module`.\\n3.\\tDefine your loss function (from `torch.nn`), optimizers (from `torch.optim`), etc.\\n4.\\tDefine your training loop, it can be a function that does a single step and write a loop, or just simply a training loop.\\n5.\\tStart training!\\n\\nFor this example, we are going to use the MNIST dataset, which is built-in in PyTorch, making it very easy to download and use.\\n\\nFirst import the necessary libraries:\\n```python\\nimport torch\\nfrom torch import nn\\nfrom torch.utils.data import DataLoader\\nfrom torchvision import datasets\\nfrom torchvision.transforms import ToTensor\\n```\\n\\n### Finding and defining your dataset\\n\\nPyTorch offers domain-specific libraries such as TorchText, TorchVision, and TorchAudio, all of which include datasets. For this tutorial, we will be using a TorchVision dataset.\\n\\nThe `torchvision.datasets` module contains `Dataset` objects for many real-world vision data like CIFAR, COCO. In this tutorial, we use the FashionMNIST dataset. Every TorchVision `Dataset` includes two arguments: `transform` and `target_transform` to modify the samples and labels respectively.\\n\\n```python\\n# Download training data from the MNIST dataset.\\ntraining_data = datasets.FashionMNIST(\\n    root=\\"data\\",\\n    train=True,\\n    download=True,\\n    transform=ToTensor(), # Convert to PyTorch tensor.\\n)\\n\\n# Download test data from the MNIST dataset.\\ntest_data = datasets.FashionMNIST(\\n    root=\\"data\\",\\n    train=False,\\n    download=True,\\n    transform=ToTensor(),\\n)\\n```\\n\\nWe pass the `Dataset` as an argument to `DataLoader`. This wraps an iterable over our dataset, and supports automatic batching, sampling, shuffling and multiprocess data loading. Here we define a batch size of 64, i.e. each element in the dataloader iterable will return a batch of 64 features and labels.\\n\\n```python\\nbatch_size = 64\\n\\n# Create data loaders.\\ntrain_dataloader = DataLoader(training_data, batch_size=batch_size)\\ntest_dataloader = DataLoader(test_data, batch_size=batch_size)\\n\\nfor X, y in test_dataloader:\\n    print(f\\"Shape of X [N, C, H, W]: {X.shape}\\") # N, C, H, W stands for Batch Size, Channel Size, Height, and Width\\n    print(f\\"Shape of y: {y.shape} {y.dtype}\\")\\n    break\\n```\\n\\n### Create the model\\n\\nTo define a neural network in PyTorch, we create a class that inherits from nn.Module. We define the layers of the network in the `__init__` function and specify how data will pass through the network in the forward function. To accelerate operations in the neural network, we move it to the GPU (`cuda`) or MPS if available.\\n\\n```python\\n# Get cpu, gpu (cuda) device for training.\\ndevice = (\\n    \\"cuda\\" if torch.cuda.is_available() else \\"cpu\\"\\n)\\nprint(f\\"Using {device} device\\")\\n\\n# Define model\\nclass NeuralNetwork(nn.Module):\\n    def __init__(self):\\n        super().__init__()\\n        self.flatten = nn.Flatten() # Flattens to (Batch Size, Channel Size * Height * Width), from a 4D tensor to a 2D tensor\\n        self.linear_relu_stack = nn.Sequential(\\n            nn.Linear(28*28, 512), # Fully-connected hidden layer\\n            nn.ReLU(), # Activation Function\\n            nn.Linear(512, 512),\\n            nn.ReLU(),\\n            nn.Linear(512, 10)\\n        ) # When `linear_relu_stack` is called, it will run all the modules inside in order.\\n\\n    def forward(self, x):\\n        x = self.flatten(x) # flatten to 2D, same as x.view(x.size(0), -1)\\n        logits = self.linear_relu_stack(x) # the nn.Sequential instance\\n        return logits\\n\\n# Create an instance\\nmodel = NeuralNetwork().to(device)\\nprint(model)\\n```\\n\\n[Learn More About Building Neural Networks in PyTorch Here](https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html)\\n\\n### Define the Loss Function and the Optimizer\\n\\nWe are using the Cross Entropy Loss loss function and the Stochastic Gradient Descent (SGD) optimizer for training this model.\\n\\n```python\\nloss_fn = nn.CrossEntropyLoss()\\noptimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\\n```\\n\\n### Define the training loop\\n\\nFirst we define the function to train a single step:\\n\\n```python\\ndef train(dataloader, model, loss_fn, optimizer):\\n    size = len(dataloader.dataset)\\n    model.train()\\n    for batch, (X, y) in enumerate(dataloader):\\n        X, y = X.to(device), y.to(device) # Move tensors to cuda if available\\n\\n        # Compute prediction error\\n        pred = model(X) # Forward pass\\n        loss = loss_fn(pred, y) # Compute loss\\n\\n        # Backpropagation\\n        loss.backward() # Compute gradients\\n        optimizer.step() # Update parameters\\n        optimizer.zero_grad() # Zero the gradients\\n\\n        if batch % 100 == 0:\\n            loss, current = loss.item(), (batch + 1) * len(X)\\n            print(f\\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\\")\\n```\\n\\nThen a function to evaluate (or validate). The goal of this is to check the model\u2019s performance against the test dataset to ensure it is learning, and monitor if it is overfitting.\\n\\n```python\\ndef test(dataloader, model, loss_fn):\\n    size = len(dataloader.dataset)\\n    num_batches = len(dataloader)\\n    model.eval()\\n    test_loss, correct = 0, 0\\n    with torch.no_grad():\\n        for X, y in dataloader:\\n            X, y = X.to(device), y.to(device)\\n            pred = model(X)\\n            test_loss += loss_fn(pred, y).item()\\n            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\\n    test_loss /= num_batches\\n    correct /= size\\n    print(f\\"Test Error: \\\\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\\\n\\")\\n```\\n\\nThe training process is conducted over several iterations (epochs) that go over the entire training dataset. During each epoch, the model learns parameters to make better predictions. We print the model\u2019s accuracy and loss at each epoch; we\u2019d like to see the accuracy increase and the loss decrease with every epoch.\\n\\n```python\\nepochs = 5\\nfor t in range(epochs):\\n    print(f\\"Epoch {t+1}\\\\n-------------------------------\\")\\n    train(train_dataloader, model, loss_fn, optimizer)\\n    test(test_dataloader, model, loss_fn)\\nprint(\\"Done!\\")\\n```\\n\\n[Read More About Model Training Here](https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html)\\n\\n## Tasks\\n\\n1. Plot some graphs to show the training loss, validation loss, and validation accuracy over step.\\n2. Experiment with different learning rate, batch size, and model architecture, how does it affect the results?"},{"id":"basic-of-nn","metadata":{"permalink":"/blog/basic-of-nn","source":"@site/blog/2023-11-13-basic-of-nn/index.md","title":"The Most Basics of Neural Networks","description":"This blog post will be about the most basics of neural networks, for absolute beginners.","date":"2023-11-13T00:00:00.000Z","tags":[{"label":"beginners","permalink":"/blog/tags/beginners"},{"label":"nn","permalink":"/blog/tags/nn"},{"label":"tutorial","permalink":"/blog/tags/tutorial"},{"label":"tired","permalink":"/blog/tags/tired"}],"readingTime":6.525,"hasTruncateMarker":true,"authors":[{"name":"Eason Xie","title":"Some Random Tree","url":"https://easonoob.github.io","imageURL":"https://avatars.githubusercontent.com/u/100521878?v=4","key":"eason"}],"frontMatter":{"slug":"basic-of-nn","title":"The Most Basics of Neural Networks","authors":"eason","tags":["beginners","nn","tutorial","tired"]},"unlisted":false,"prevItem":{"title":"Introduction to PyTorch","permalink":"/blog/intro-to-pytorch"},"nextItem":{"title":"Website Launch!","permalink":"/blog/website-launch"}},"content":"This blog post will be about the most basics of neural networks, for absolute beginners.\\n\\nYou\'ve heard about things like ChatGPT, LlaMa, Midjourney, Dall-E, and Stable Diffusion. But have you ever wondered how exactly does they work? In this blog series, I will explain the neural network, from the absolute basics to advanced, from simple fully-connected networks to Transformers, Quantum Neural Networks, and Graph Neural Networks. If you encountered any questions, feel free to ask by directly through my email or by Discord!\\n\\n\x3c!--truncate--\x3e\\n\\n## Prerequisites\\n- Basic Maths\\n- Basic Python\\n\\n## Aims\\n1.\\tUnderstand the neurons and neural networks\\n2.\\tUnderstand the use of neural networks\\n3.\\tUnderstand forward pass, loss function, backward pass, and weight update\\n\\n## Introduction\\n> \u201cI think the brain is essentially a computer and consciousness is like a computer program. It will cease to run when the computer is turned off. Theoretically, it could be re-created on a neural network, but that would be very difficult, as it would require all one\'s memories.\u201d ~ Stephen Hawking\\n\\n## Important Concepts\\n\\n### Neurons and Neural Networks\\nImagine you are making a system to predict property prices. How would you implement such a system? Maybe it will take the different factors of a property (e.g. size, height, view, surrounding environment, etc.) to calculate the price and outputs it. But how can you calculate the price based on these inputs (factors)?\\n\\nLet\u2019s simplify the problem and assume there is only one factor: the size (area) of the property. Let\u2019s plot a graph with the area on the x-axis, price on the y-axis:\\n\\n![Graph 1](./graph-1.png)\\n\\nThen we can draw a straight fit line on the graph to relate all the data points:\\n:::tip\\nNote that the fit line may not pass through all/any data points!\\n:::\\n\\n![Graph 2](./graph-2.png)\\n\\nHow can you represent the fit line in maths? We can use a simple linear function:\\n\\n$$\\nf(x) = wx + b\\n$$\\n\\nWhere w is the **weight** (the steepness of the linear), b is the **bias** (the y-intercept, or height of the line). In fact, we can represent any straight line on a two-dimensional Cartesian coordinate space and any linear functions with one variable with this function. In the neural network, this function is the simplest form of a **neuron**. But what is a neuron? This is an anatomy of a human neuron cell:\\n\\n![Neuron Anatomy](./neuron-anatomy.png)\\n\\nThe dendrites receive the signals, the cell processes the signals, and the axon terminals send the signals to other neuron cells. By connecting 86 billion of these cells, it forms your brain. We can simulate a neuron in maths with the following function:\\n\\n$$\\nf(x_1, x_2, x_3, x_4, \\\\ldots) = w_1 x_1 + w_2 x_2 + w_3 x_3 + w_4 x_4 + \\\\cdots + b\\n$$\\n\\nThe multiple weights represent the \u201cimportance\u201d of each of the different input x. The b is still the bias. Here\u2019s another diagram to understand the neuron:\\n\\n![Diagram 1](./diagram-1.png)\\n\\nSo, for example, the multiple x can be the different factors of the property, and the weights are parameters that you can tune, representing the importance of that factor contributing to the final price. Wait, what is the f(x) in the above diagram? It is an Activation Function. Its purpose is to introduce non-linearity into the neural network as combining linear functions can only result in a linear function:\\n\\nLet $f(x) = w_1 x + b_1$, $g(x) = w_2 x + b_2$, then $h(x) = g(f(x))$:\\n$$\\nh(x) = g(f(x)) = w_2 (w_1 x + b_1) + b_2\\n$$\\n$$\\nh(x) = w_2 w_1 x + w_2 b_1 + b_2\\n$$\\nLet $a = w_2 w_1$ and $c = w_2 b_1 + b_2$, then:\\n$$\\nh(x) = ax + c\\n$$\\n\\nWhich is a straight line on the graph. But by adding activation functions in between, e.g. $h(x)=g(a(f(x)))$, where $a$ is the activation function, we can create a non-linear (broadly speaking not a straight line on the graph) function or network. Some common activation functions include ReLU, Sigmoid, Tanh, LeakyReLU, GELU, and Softmax. The details of the activation function will be talked about in later blog posts.\\n\\nBy combining and connecting multiple neurons in an orderly manner, you get a neural network. Below is a diagram of a neural network which you\u2019ve probably seen before:\\n\\n![Diagram 2](./diagram-2.png)\\n\\nEach circle (node) in the neural network represents a neuron. Each neuron in the input layer processes a single number and each neuron in the output layer outputs a single number. Neural networks are **universal function approximators**.\\n:::info\\nNote that while the neurons in the hidden layer may seem to have multiple outputs, the outputs are the same.\\n:::\\n\\n### Training of neural networks\\n\\nHowever, how can you calculate the weights and biases given x and y? Below are the processes of training a neural network to find the optimal weights and biases:\\t\\n\\n1.\\t**Initialize the weights and biases** using random numbers with normal or uniform distribution.\\n2.\\t**Forward pass:** Run the model with the input x, and get the model output y.\\n3.\\t**Compute loss:** Compare the model-output y with the real targeted y and calculate the difference using a loss function. \\n4.\\t**Backpropagation/backward pass:** Calculate the gradients of each parameter (weight or bias) using gradient descent with partial derivative. Which means how the output of the model changes with that parameter changing.\\n5.\\t**Update parameters:** Update the parameters using optimizers based on the gradients calculated and the given learning rate hyperparameter (usually between 1e-2 to 1e-6).\\n6.\\t**Repeat Step 2 to 5** with different x and y until the loss is good enough.\\n\\n![Diagram 3](./diagram-3.png)\\n\\n## Code Implementations\\n\\n### A simple neural network with PyTorch\\n\\nLet\u2019s consider a simple formula: y=3x+1. Given an array of x and array of the corresponding y, find 3 and 1 (the weight and the bias). Run the following codes in Google Colab for simplicity.\\n\\n```python\\nimport torch # main library\\nimport torch.nn as nn # neural network modules and functions\\nimport torch.optim as optim # optimizers for neural networks\\nimport matplotlib.pyplot as plt # plotting for analysing the loss\\n\\ntorch.manual_seed(69) # define a manual seed so the results are reproducible\\n```\\n\\nDefine the data x and y:\\n\\n```python\\nx = torch.tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10], dtype=torch.float32)\\ny = 3*x + 1\\n\\nprint(x.tolist())\\nprint(y.tolist())\\n```\\n\\nDefine the model, loss function, and optimizer:\\n\\n```python\\nmodel = nn.Linear(1, 1) # a single neuron\\ncriterion = nn.MSELoss() # Mean squared error (MSE) loss function\\noptimizer = optim.SGD(model.parameters(), lr=1e-4) # Stochastic Gradient Descent (SGD) optimizer\\n```\\n\\nTraining Loop:\\n\\n```python\\nepochs = 100\\nlosses = []\\nfor i in range(epochs):\\n    for iter_x, iter_y in zip(x, y):\\n        iter_x = iter_x.unsqueeze(0) # add another dimension at the end\\n        iter_y = iter_y.unsqueeze(0) # same\\n        optimizer.zero_grad() # zero the gradients\\n        output = model(iter_x) # Forward pass\\n        loss = criterion(output, iter_y) # Compute Loss\\n        loss.backward() # Backward pass\\n        optimizer.step() # Update parameters\\n        print(f\\"Epoch: {i}/{epochs}, Loss: {loss.item()}\\")\\n        losses.append(loss.item())\\n```\\n\\nFinal loss: 0.0001777520083123818\\n\\nPlot the loss graph:\\n\\n```python\\nplt.plot(losses)\\nplt.show() # Plot the loss graph\\n```\\n\\nLoss Graph:\\n\\n![Loss Graph](./result.png)\\n\\nTest the model:\\n\\n```python\\nfor name, param in model.named_parameters():\\n    print(name, param) # Inspect the weight and bias\\n\\nn = 100 # input x\\nx = torch.tensor([n], dtype=torch.float32) # convert to tensor\\ny = model(x) # Forward pass\\nprint(y)\\n```\\n\\n### Follow-up Tasks\\n\\n1. Experiment with different learning rates and epochs and see how it affects the training result.\\n2. Enlarge the dataset (lengthen the x tensor), does it improve the model?\\n3. Experiment with a more complex function than y=3x+1 such as y=x^2. Does it work? If it doesn\u2019t work, how can you solve this issue?\\n4. How can you make the training faster without changing the hardware, data and the model?\\n\\nFeel free to send me an email for your solutions to the above tasks!"},{"id":"website-launch","metadata":{"permalink":"/blog/website-launch","source":"@site/blog/2023-10-06-website-launch/index.md","title":"Website Launch!","description":"Website Launch","date":"2023-10-06T00:00:00.000Z","tags":[{"label":"hola","permalink":"/blog/tags/hola"},{"label":"tired","permalink":"/blog/tags/tired"}],"readingTime":0.055,"hasTruncateMarker":true,"authors":[{"name":"Eason Xie","title":"Some Random Tree","url":"https://easonoob.github.io","imageURL":"https://avatars.githubusercontent.com/u/100521878?v=4","key":"eason"}],"frontMatter":{"slug":"website-launch","title":"Website Launch!","authors":"eason","tags":["hola","tired"]},"unlisted":false,"prevItem":{"title":"The Most Basics of Neural Networks","permalink":"/blog/basic-of-nn"}},"content":"![Website Launch](./website-launch.jpg)\\n\\nThe website is launched today at 00:00 UTC+0!\\n\\n\x3c!--truncate--\x3e"}]}')}}]);