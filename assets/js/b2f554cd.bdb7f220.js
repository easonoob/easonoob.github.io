"use strict";(self.webpackChunkdocusaurus_site=self.webpackChunkdocusaurus_site||[]).push([[5894],{6042:t=>{t.exports=JSON.parse('{"blogPosts":[{"id":"intro-to-qnn","metadata":{"permalink":"/blog/intro-to-qnn","source":"@site/blog/2024-2-9-intro-to-qnn/index.md","title":"Introduction to Quantum Neural Networks","description":"Make a Quantum Neural Network (QNN) in PyTorch.","date":"2024-02-09T00:00:00.000Z","tags":[{"label":"intermediate","permalink":"/blog/tags/intermediate"},{"label":"quantum","permalink":"/blog/tags/quantum"},{"label":"qnn","permalink":"/blog/tags/qnn"},{"label":"pytorch","permalink":"/blog/tags/pytorch"},{"label":"tutorial","permalink":"/blog/tags/tutorial"}],"readingTime":9.51,"hasTruncateMarker":true,"authors":[{"name":"Eason Xie","title":"Website Owner","url":"https://easonoob.github.io","imageURL":"https://avatars.githubusercontent.com/u/100521878?v=4","key":"eason"}],"frontMatter":{"slug":"intro-to-qnn","title":"Introduction to Quantum Neural Networks","authors":"eason","tags":["intermediate","quantum","qnn","pytorch","tutorial"]},"unlisted":false,"nextItem":{"title":"Introduction to Quantum Computing","permalink":"/blog/intro-to-quantum-computing"}},"content":"Make a Quantum Neural Network (QNN) in PyTorch.\\r\\n\\r\\n\x3c!--truncate--\x3e\\r\\n\\r\\nIn the [last blog post](https://easonoob.github.io/blog/intro-to-quantum-computing), we talked about the very basics of quantum computing, and tried a few examples with qiskit. In this post, we will try to build a QNN completely with PyTorch only, without quantum simulation / machine learning libraries like PennyLane or TorchQuantum, so we can know what really happens inside the simulation. Let\'s get started!\\r\\n\\r\\n## Prerequisites\\r\\n- Read and Understand the last post\\r\\n- Basic Python & PyTorch\\r\\n\\r\\n## Simulation\\r\\n\\r\\nImport necessary libraries:\\r\\n\\r\\n```python\\r\\nimport torch, torch.nn as nn, torch.nn.functional as F\\r\\nfrom typing import Union, List\\r\\nimport numpy as np\\r\\nfrom torchvision import datasets, transforms\\r\\nfrom torch.utils.data import DataLoader\\r\\nimport matplotlib.pyplot as plt\\r\\n```\\r\\n\\r\\nWe know a quantum state vector of a single qubit can be represented as $|\\\\psi\\\\rangle = \\\\alpha |0\\\\rangle + \\\\beta |1\\\\rangle$, and numerous qubits can be combined into a large state of $2^n$ propabilities. For the simulation, we will create a single large state vector $h$ which stores all qubits in the system (not very efficient but this is the easiest) with $h \\\\in \\\\mathbb{R}^{b \\\\times \\\\underbrace{2 \\\\times 2 \\\\times \\\\cdots \\\\times 2}_{n \\\\text{ times}}}$ which $b$ is batch size, $n$ is number of qubits so that for each batch, there are $2^n$ propabilities.\\r\\n\\r\\nTo apply the gate to a specific qubit in the state vector, we can simply permute the target qubit dimension to the back, then multiply it with the gate matrix. For double qubits gates, we can permute the control and target to the back, combine the two dimensions, multiply the gate matrix, and reshape back. Here is a function that applied the gate to the state:\\r\\n\\r\\n```python\\r\\ndef apply_gate(state, mat, wires: Union[int, List[int]]):\\r\\n    \\"\\"\\"\\r\\n    Apply the gate matrix/matrices to the state vector using torch.bmm method.\\r\\n    \\r\\n    Args:\\r\\n        state (torch.Tensor): The state vector.\\r\\n        mat (torch.Tensor): The gate matrix/matrices.\\r\\n        wires (int or List[int]): Which qubit(s) the operation is applied to.\\r\\n        \\r\\n    Returns:\\r\\n        torch.Tensor: The updated state vector.\\r\\n    \\"\\"\\"\\r\\n    # Handle input for single qubit as a list for uniformity\\r\\n    if isinstance(wires, int):\\r\\n        wires = [wires]\\r\\n    \\r\\n    # Ensure the matrix is on the same device and dtype as the state\\r\\n    mat = mat.to(state.device).to(state.dtype)\\r\\n    \\r\\n    # Calculate the new order of dimensions for the state to match matrix multiplication needs\\r\\n    num_qubits = len(state.shape) - 1\\r\\n    permute_order = list(range(1, num_qubits + 1))  # Start from 1 to account for batch dimension\\r\\n    for index, wire in enumerate(wires):\\r\\n        permute_order.remove(wire + 1)  # Remove wire from its current place\\r\\n        permute_order.insert(index, wire + 1)  # Insert wire right after the batch dimension\\r\\n\\r\\n    # Permute the state tensor to bring the target wire dimensions next to the batch dimension\\r\\n    permuted_state = state.permute([0] + permute_order)  # Batch dimension remains the first\\r\\n    reshaped_state = permuted_state.reshape(state.shape[0], -1, mat.size(-1))\\r\\n    \\r\\n    # Apply the gate using matrix multiplication\\r\\n    new_state = torch.bmm(reshaped_state, mat) if len(mat.shape) == 3 else reshaped_state @ mat\\r\\n    \\r\\n    # Reshape and permute back to the original shape and order\\r\\n    final_state = new_state.view(state.shape).permute(list(np.argsort([0] + permute_order)))\\r\\n    \\r\\n    return final_state\\r\\n```\\r\\n\\r\\nThe reason for the input to accept batched gate matrices is for the angle encoding later, which encodes inputs into the state.\\r\\n\\r\\n### Define Gate Matrices\\r\\n\\r\\nNow the gate matrix functions:\\r\\n\\r\\n```python\\r\\ndef h_matrix():\\r\\n    return 1 / np.sqrt(2) * torch.tensor([[1, 1], [1, -1]], dtype=torch.complex64, requires_grad=True)\\r\\n\\r\\ndef cnot_matrix():\\r\\n    return torch.tensor([[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 0, 1], [0, 0, 1, 0]], dtype=torch.complex64, requires_grad=True)\\r\\n\\r\\ndef pauli_x_matrix():\\r\\n    return torch.tensor([[0, 1], [1, 0]], dtype=torch.complex64, requires_grad=True)\\r\\n\\r\\ndef pauli_y_matrix():\\r\\n    return torch.tensor([[0, -1j], [1j, 0]], dtype=torch.complex64, requires_grad=True)\\r\\n\\r\\ndef pauli_z_matrix():\\r\\n    return torch.tensor([[1, 0], [0, -1]], dtype=torch.complex64, requires_grad=True)\\r\\n\\r\\ndef rx_matrix(theta):\\r\\n    return torch.tensor([[torch.cos(theta / 2), -1j * torch.sin(theta / 2)],\\r\\n                         [-1j * torch.sin(theta / 2), torch.cos(theta / 2)]], dtype=torch.complex64, requires_grad=True)\\r\\n\\r\\ndef ry_matrix(theta):\\r\\n    return torch.tensor([[torch.cos(theta / 2), -torch.sin(theta / 2)],\\r\\n                         [torch.sin(theta / 2), torch.cos(theta / 2)]], dtype=torch.complex64, requires_grad=True)\\r\\n\\r\\ndef rz_matrix(theta):\\r\\n    return torch.tensor([[torch.exp(-1j * theta / 2), 0],\\r\\n                         [0, torch.exp(1j * theta / 2)]], dtype=torch.complex64, requires_grad=True)\\r\\n\\r\\ndef u3_matrix(theta, phi, lam):\\r\\n    return torch.tensor([[torch.cos(theta / 2), -torch.exp(1j * lam) * torch.sin(theta / 2)],\\r\\n                         [torch.exp(1j * phi) * torch.sin(theta / 2), torch.exp(1j * (phi + lam)) * torch.cos(theta / 2)]], dtype=torch.complex64, requires_grad=True)\\r\\n\\r\\ndef crx_matrix(theta):\\r\\n    return torch.tensor([[1, 0, 0, 0],\\r\\n                         [0, 1, 0, 0],\\r\\n                         [0, 0, torch.cos(theta / 2), -1j * torch.sin(theta / 2)],\\r\\n                         [0, 0, -1j * torch.sin(theta / 2), torch.cos(theta / 2)]], dtype=torch.complex64, requires_grad=True)\\r\\n\\r\\ndef cry_matrix(theta):\\r\\n    return torch.tensor([[1, 0, 0, 0],\\r\\n                         [0, 1, 0, 0],\\r\\n                         [0, 0, torch.cos(theta / 2), -torch.sin(theta / 2)],\\r\\n                         [0, 0, torch.sin(theta / 2), torch.cos(theta / 2)]], dtype=torch.complex64, requires_grad=True)\\r\\n\\r\\ndef crz_matrix(theta):\\r\\n    return torch.tensor([[1, 0, 0, 0],\\r\\n                         [0, 1, 0, 0],\\r\\n                         [0, 0, torch.exp(-1j * theta / 2), 0],\\r\\n                         [0, 0, 0, torch.exp(1j * theta / 2)]], dtype=torch.complex64, requires_grad=True)\\r\\n\\r\\ndef cu3_matrix(theta, phi, lam):\\r\\n    return torch.tensor([[1, 0, 0, 0],\\r\\n                         [0, 1, 0, 0],\\r\\n                         [0, 0, torch.cos(theta / 2), -torch.exp(1j * lam) * torch.sin(theta / 2)],\\r\\n                         [0, 0, torch.exp(1j * phi) * torch.sin(theta / 2), torch.exp(1j * (phi + lam)) * torch.cos(theta / 2)]], dtype=torch.complex64, requires_grad=True)\\r\\n```\\r\\n\\r\\nNow the Gate class, which stores the parameters (if any) and qubits:\\r\\n\\r\\n```python\\r\\nclass Gate(nn.Module):\\r\\n    def __init__(self, gate_matrix_fn, wires: list, n_params: int):\\r\\n        super(Gate, self).__init__()\\r\\n        self.wires = wires\\r\\n        self.n_params = n_params\\r\\n        self.gate_matrix_fn = gate_matrix_fn\\r\\n        self.params = nn.Parameter(torch.randn(n_params, dtype=torch.float32)).uniform_(-np.pi, np.pi) # Unifrom distribution between -pi and pi\\r\\n    \\r\\n    def forward(self, state):\\r\\n        gate_matrix = self.gate_matrix_fn(*self.params)\\r\\n        return apply_gate(state, gate_matrix, self.wires)\\r\\n```\\r\\n\\r\\n### Angle Encoding\\r\\n\\r\\nAngle encoding, as mentioned above, encodes the input tensor into the state vector using rotation gates (rx, ry, u3, etc).\\r\\n\\r\\n```python\\r\\nclass AngleEncoding(nn.Module):\\r\\n    \\"\\"\\"\\r\\n    Example functions list:\\r\\n    [{\\"gate\\": \'rx\', \\"wires\\": 0, \\"input_idx\\": 0},\\r\\n    {\\"gate\\": \'ry\', \\"wires\\": 1, \\"input_idx\\": 1},\\r\\n    {\\"gate\\": \'rz\', \\"wires\\": 2, \\"input_idx\\": 2},]\\r\\n    or\\r\\n    [{\\"gate\\": \'u3\', \\"wires\\": 0, \\"input_idx\\": [0, 1, 2]},\\r\\n    {\\"gate\\": \'rx\', \\"wires\\": 1, \\"input_idx\\": 1},\\r\\n    {\\"gate\\": \'cu3\', \\"wires\\": [1, 2], \\"input_idx\\": [0, 1, 2]},]\\r\\n    \\"\\"\\"\\r\\n    def __init__(self, func_list):\\r\\n        super().__init__()\\r\\n        self.func_list = func_list\\r\\n\\r\\n    def forward(self, state, x):\\r\\n        for info in self.func_list:\\r\\n            params = x[:, [info[\\"input_idx\\"]]] if len(x.shape) > 1 else x[info[\\"input_idx\\"]]\\r\\n            gate = info[\\"gate\\"] + \'_matrix\'\\r\\n            fn = globals()[gate] # Get the function from the globals\\r\\n            mat = torch.stack([fn(*p) for p in params], dim=0) if len(x.shape) > 1 else fn(*params) # Gate matrix\\r\\n            state = apply_gate(\\r\\n                state,\\r\\n                mat=mat,\\r\\n                wires=info[\\"wires\\"],\\r\\n            )\\r\\n        return state\\r\\n```\\r\\n\\r\\n### Measurement\\r\\n\\r\\nFor the measurement, we will measure the expected values of all qubits based on a observable matrix of size $2^n \\\\times 2^n$, which typically is either Pauli-X, Pauli-Y, or Pauli-Z. Mathematically it is $\\\\langle\\\\psi|O|\\\\psi\\\\rangle$.\\r\\n\\r\\n```python\\r\\ndef measure_pauli_expectations(state, observable_matrix):\\r\\n    num_qubits = len(state.shape) - 1\\r\\n    batch_size = state.shape[0]\\r\\n    expected_values = torch.empty((batch_size, num_qubits), dtype=state.real.dtype, device=state.device)\\r\\n    observable_matrix = observable_matrix.to(state.device).to(state.dtype)\\r\\n\\r\\n    # Compute the expectation value for each qubit\\r\\n    for qubit in range(num_qubits):\\r\\n        # Permute to bring the qubit of interest to the last position\\r\\n        dims = list(range(1, num_qubits + 1))\\r\\n        dims.append(dims.pop(qubit))  # Move the qubit index to the end\\r\\n        permuted_state = state.permute([0] + dims)\\r\\n        \\r\\n        # Reshape to combine all other dimensions except the last two\\r\\n        reshaped_state = permuted_state.reshape(batch_size, -1, 2)\\r\\n        \\r\\n        # Apply observable and calculate expectation value\\r\\n        # Here we calculate <psi|O|psi> for the current qubit\\r\\n        measured_state = torch.matmul(reshaped_state, observable_matrix)\\r\\n        probabilities = torch.matmul(measured_state, reshaped_state.transpose(-2, -1)).diagonal(dim1=-2, dim2=-1)\\r\\n        expected_value = probabilities.sum(dim=-1)  # Sum over the states to get the expectation\\r\\n\\r\\n        # Store the computed expected value for the current qubit\\r\\n        expected_values[:, qubit] = expected_value.real\\r\\n\\r\\n    return expected_values\\r\\n```\\r\\n\\r\\n### Model\\r\\n\\r\\nWe will use the MNIST dataset for the testing. We will only use the first 5 numbers (0, 1, 2, 3, 4) and the image size will be reduced to $5 \\\\times 5$ to reduce complexity. The Variational Quantum Circuit (VQC) consists 4 blocks of single and double qubits parameterized gates.\\r\\n\\r\\n```python\\r\\nclass VQC(nn.Module):\\r\\n    def __init__(self):\\r\\n        super(VQC, self).__init__()\\r\\n        self.n_wires = 5 # Number of qubits in the circuit\\r\\n        self.encoding = AngleEncoding([\\r\\n            {\\"input_idx\\": [0], \\"gate\\": \\"ry\\", \\"wires\\": [0]},\\r\\n            {\\"input_idx\\": [1], \\"gate\\": \\"ry\\", \\"wires\\": [1]},\\r\\n            {\\"input_idx\\": [2], \\"gate\\": \\"ry\\", \\"wires\\": [2]},\\r\\n            {\\"input_idx\\": [3], \\"gate\\": \\"ry\\", \\"wires\\": [3]},\\r\\n            {\\"input_idx\\": [4], \\"gate\\": \\"ry\\", \\"wires\\": [4]},\\r\\n            {\\"input_idx\\": [5], \\"gate\\": \\"rz\\", \\"wires\\": [0]},\\r\\n            {\\"input_idx\\": [6], \\"gate\\": \\"rz\\", \\"wires\\": [1]},\\r\\n            {\\"input_idx\\": [7], \\"gate\\": \\"rz\\", \\"wires\\": [2]},\\r\\n            {\\"input_idx\\": [8], \\"gate\\": \\"rz\\", \\"wires\\": [3]},\\r\\n            {\\"input_idx\\": [9], \\"gate\\": \\"rz\\", \\"wires\\": [4]},\\r\\n            {\\"input_idx\\": [10], \\"gate\\": \\"rx\\", \\"wires\\": [0]},\\r\\n            {\\"input_idx\\": [11], \\"gate\\": \\"rx\\", \\"wires\\": [1]},\\r\\n            {\\"input_idx\\": [12], \\"gate\\": \\"rx\\", \\"wires\\": [2]},\\r\\n            {\\"input_idx\\": [13], \\"gate\\": \\"rx\\", \\"wires\\": [3]},\\r\\n            {\\"input_idx\\": [14], \\"gate\\": \\"rx\\", \\"wires\\": [4]},\\r\\n            {\\"input_idx\\": [15], \\"gate\\": \\"ry\\", \\"wires\\": [0]},\\r\\n            {\\"input_idx\\": [16], \\"gate\\": \\"ry\\", \\"wires\\": [1]},\\r\\n            {\\"input_idx\\": [17], \\"gate\\": \\"ry\\", \\"wires\\": [2]},\\r\\n            {\\"input_idx\\": [18], \\"gate\\": \\"ry\\", \\"wires\\": [3]},\\r\\n            {\\"input_idx\\": [19], \\"gate\\": \\"ry\\", \\"wires\\": [4]},\\r\\n            {\\"input_idx\\": [20], \\"gate\\": \\"rz\\", \\"wires\\": [0]},\\r\\n            {\\"input_idx\\": [21], \\"gate\\": \\"rz\\", \\"wires\\": [1]},\\r\\n            {\\"input_idx\\": [22], \\"gate\\": \\"rz\\", \\"wires\\": [2]},\\r\\n            {\\"input_idx\\": [23], \\"gate\\": \\"rz\\", \\"wires\\": [3]},\\r\\n            {\\"input_idx\\": [24], \\"gate\\": \\"rz\\", \\"wires\\": [4]},\\r\\n        ]) # Encode 16 features into 4 qubits\\r\\n        \\r\\n        layers = []\\r\\n        for _ in range(4):\\r\\n            layers.extend([\\r\\n                Gate(cnot_matrix, [0, 1], 0),\\r\\n                Gate(cnot_matrix, [1, 2], 0),\\r\\n                Gate(cnot_matrix, [2, 3], 0),\\r\\n                Gate(cnot_matrix, [3, 4], 0),\\r\\n                Gate(cnot_matrix, [4, 0], 0),\\r\\n                Gate(rx_matrix, [0], 1),\\r\\n                Gate(rx_matrix, [1], 1),\\r\\n                Gate(rx_matrix, [2], 1),\\r\\n                Gate(rx_matrix, [3], 1),\\r\\n                Gate(rx_matrix, [4], 1),\\r\\n                Gate(cu3_matrix, [1, 0], 3),\\r\\n                Gate(cu3_matrix, [2, 1], 3),\\r\\n                Gate(cu3_matrix, [3, 2], 3),\\r\\n                Gate(cu3_matrix, [4, 3], 3),\\r\\n                Gate(cu3_matrix, [0, 4], 3),\\r\\n                Gate(u3_matrix, [0], 3),\\r\\n                Gate(u3_matrix, [1], 3),\\r\\n                Gate(u3_matrix, [2], 3),\\r\\n                Gate(u3_matrix, [3], 3),\\r\\n                Gate(u3_matrix, [4], 3),\\r\\n            ])\\r\\n        self.qnn = nn.Sequential(*layers)\\r\\n    \\r\\n    def forward(self, x):\\r\\n        state = torch.zeros(x.shape[0], 2**self.n_wires, dtype=torch.complex64, device=x.device).reshape(x.shape[0], 2, 2, 2, 2, 2)\\r\\n        state[:, 0, 0, 0, 0, 0] = 1  # Initialize the state to |0001>\\r\\n        state = self.encoding(state, x.view(x.shape[0], -1)) # Encode the input features\\r\\n        state = self.qnn(state)\\r\\n        measured = measure_pauli_expectations(state, pauli_z_matrix())\\r\\n        return F.log_softmax(measured, dim=-1)\\r\\n```\\r\\n\\r\\n### Dataset\\r\\n\\r\\nWe will process the MNIST to include only first 5 digits and resize it to $5 \\\\times 5$.\\r\\n\\r\\n```python\\r\\nclass MNISTDigitsDataset(datasets.MNIST):\\r\\n    def __init__(self, root, train=True, transform=None, target_transform=None, download=False):\\r\\n        super().__init__(root, train=train, transform=transform, target_transform=target_transform, download=download)\\r\\n        # Filter indices for digits 0 to 3\\r\\n        indices = [i for i, label in enumerate(self.targets) if label in [0, 1, 2, 3, 4]]\\r\\n        self.data = self.data[indices]\\r\\n        self.targets = self.targets[indices]\\r\\n\\r\\n    def __getitem__(self, index):\\r\\n        with torch.no_grad():\\r\\n            # Get the image and target label\\r\\n            img, target = self.data[index], int(self.targets[index])\\r\\n            \\r\\n            # Convert image to PIL for transformations\\r\\n            img = transforms.functional.to_pil_image(img)\\r\\n            \\r\\n            # Apply transformations if any\\r\\n            if self.transform:\\r\\n                img = self.transform(img)\\r\\n            \\r\\n            if self.target_transform:\\r\\n                target = self.target_transform(target)\\r\\n        \\r\\n        return img, target\\r\\n\\r\\n    def __len__(self):\\r\\n        return len(self.data)\\r\\n\\r\\ntransform = transforms.Compose([\\r\\n    transforms.Resize((5, 5)),  # Resize the image to 5x5\\r\\n    transforms.ToTensor()       # Convert the image to a PyTorch tensor\\r\\n])\\r\\n\\r\\n# Initialize the dataset\\r\\ntrain_dataset = MNISTDigitsDataset(root=\'./data\', train=True, transform=transform, download=True)\\r\\nvalid_dataset = MNISTDigitsDataset(root=\'./data\', train=False, transform=transform, download=True)\\r\\n\\r\\n# Create the DataLoader\\r\\nbatch_size = 32\\r\\ntrain_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\\r\\nvalid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\\r\\n```\\r\\n\\r\\n### Training Loop\\r\\n\\r\\n```python\\r\\ndevice = torch.device(\'cuda\' if torch.cuda.is_available() else \'cpu\')\\r\\nmodel = VQC().to(device)\\r\\n\\r\\nloss_fn = nn.NLLLoss()\\r\\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\\r\\n\\r\\n# Training loop\\r\\nnum_epochs = 5\\r\\ntrain_losses = []\\r\\nvalid_losses = []\\r\\nvalid_accuracies = []\\r\\nfor epoch in range(num_epochs):\\r\\n    for batch, (images, labels) in enumerate(train_dataloader):\\r\\n        images, labels = images.to(device), labels.to(device)\\r\\n        optimizer.zero_grad()\\r\\n        outputs = model(images)\\r\\n        loss = loss_fn(outputs, labels)\\r\\n        loss.backward()\\r\\n        optimizer.step()\\r\\n        train_losses.append(loss.item())\\r\\n        \\r\\n        if (batch + 1) % 50 == 0:\\r\\n            print(f\\"Epoch {epoch + 1}/{num_epochs}, Iteration: {batch+1}, Loss: {loss.item()}\\")\\r\\n        if batch == 500:\\r\\n            break # Stop early to speed up the training process\\r\\n    \\r\\n    # Validation loop\\r\\n    model.eval()\\r\\n    with torch.no_grad():\\r\\n        total = 0\\r\\n        correct = 0\\r\\n        for batch, (images, labels) in enumerate(valid_dataloader):\\r\\n            images, labels = images.to(device), labels.to(device)\\r\\n            outputs = model(images)\\r\\n            loss = loss_fn(outputs, labels)\\r\\n            valid_losses.append(loss.item())\\r\\n            \\r\\n            _, predicted = torch.max(outputs, 1)\\r\\n            total += labels.size(0)\\r\\n            correct += (predicted == labels).sum().item()\\r\\n\\r\\n            if batch == 100:\\r\\n                break # Stop early to speed up the validation process\\r\\n        \\r\\n        accuracy = correct / total\\r\\n        valid_accuracies.append(accuracy)\\r\\n        print(f\\"Validation accuracy: {accuracy}, Loss: {loss.item()}\\")\\r\\n    model.train()\\r\\n```\\r\\n\\r\\nAnd finally plot the result:\\r\\n\\r\\n```python\\r\\n# plot\\r\\nplt.plot(train_losses, label=\'Training loss\')\\r\\nplt.plot(valid_losses, label=\'Validation loss\')\\r\\nplt.xlabel(\'Iterations\')\\r\\nplt.ylabel(\'Loss\')\\r\\nplt.legend()\\r\\nplt.title(\'Training and Validation Loss\')\\r\\nplt.show()\\r\\n\\r\\nplt.plot(valid_accuracies)\\r\\nplt.xlabel(\'Iterations\')\\r\\nplt.ylabel(\'Accuracy\')\\r\\nplt.title(\'Validation Accuracy\')\\r\\nplt.show()\\r\\n```"},{"id":"intro-to-quantum-computing","metadata":{"permalink":"/blog/intro-to-quantum-computing","source":"@site/blog/2024-1-5-intro-to-quantum-computing/index.md","title":"Introduction to Quantum Computing","description":"The basics of quantum computing and some hands-on trying.","date":"2024-01-05T00:00:00.000Z","tags":[{"label":"beginners","permalink":"/blog/tags/beginners"},{"label":"quantum","permalink":"/blog/tags/quantum"},{"label":"qiskit","permalink":"/blog/tags/qiskit"},{"label":"pytorch","permalink":"/blog/tags/pytorch"},{"label":"tutorial","permalink":"/blog/tags/tutorial"}],"readingTime":9.28,"hasTruncateMarker":true,"authors":[{"name":"Eason Xie","title":"Website Owner","url":"https://easonoob.github.io","imageURL":"https://avatars.githubusercontent.com/u/100521878?v=4","key":"eason"}],"frontMatter":{"slug":"intro-to-quantum-computing","title":"Introduction to Quantum Computing","authors":"eason","tags":["beginners","quantum","qiskit","pytorch","tutorial"]},"unlisted":false,"prevItem":{"title":"Introduction to Quantum Neural Networks","permalink":"/blog/intro-to-qnn"},"nextItem":{"title":"Introduction to PyTorch","permalink":"/blog/intro-to-pytorch"}},"content":"The basics of quantum computing and some hands-on trying.\\r\\n\\r\\n\x3c!--truncate--\x3e\\r\\n\\r\\n## Prerequisites\\r\\n- Basic Maths &mdash; matrix multiplications\\r\\n- Basic quantum mechanics concepts &mdash; Superposition, Wave-Particle Duality, the Uncertainty Principle, and Entanglement\\r\\n- Basic Python\\r\\n- Imagination power\\r\\n\\r\\n## Quantum Mechanics\\r\\n> \u201cThe mathematical framework of quantum theory has passed countless successful tests and is now universally accepted as a consistent and accurate description of all atomic phenomena.\u201d ~ Erwin Schr\xf6dinger\\r\\n\\r\\nWe will not be talking about the advanced maths or concepts of quantum mechanics in this blog post, such as the Uncertainty Principle or the Schr\xf6dinger Equation, but I\'ll assume you know the main principles of Quantum Mechanics, including Superposition, Wave-Particle Duality, the Uncertainty Principle, and Entanglement. The concept of qubits and quantum circuits gates will be explained below.\\r\\n\\r\\n### Qubits\\r\\nIn normal semiconductor computers, each \\"bit\\" can only represent two values: 0 and 1, which can be used in calculations as low and high voltage. However, in quantum computers, \\"qubits\\" (quantum bit) are used. Now instead of representing only 0 **or** 1, it can now represent 0 **and** 1 simultaneously. It carries a \\"state\\" $|\\\\psi\\\\rangle$, which is a vector defined as followed in a qubit:\\r\\n\\r\\n$$\\r\\n|\\\\psi\\\\rangle = \\\\alpha |0\\\\rangle + \\\\beta |1\\\\rangle\\r\\n$$\\r\\n\\r\\nWhere\\r\\n\\r\\n$$\\r\\n|\\\\alpha|^2 + |\\\\beta|^2 = 1\\r\\n$$\\r\\n\\r\\n$\\\\alpha$ and $\\\\beta$ represents the **probabilities** or the qubit in the $|0\\\\rangle$ and $|1\\\\rangle$ states respectively, and are **complex numbers**.\\r\\n\\r\\nNow, if we have two qubits entangled, we can represent $2^2 = 4$ states ($00, 01, 10, 11$), each associated with a probability.\\r\\n\\r\\n$$\\r\\n|\\\\psi\\\\rangle = c_1 |0\\\\rangle + c_2 |1\\\\rangle\\r\\n$$\\r\\n$$\\r\\n|\\\\phi\\\\rangle = c_3 |0\\\\rangle + c_4 |1\\\\rangle\\r\\n$$\\r\\n$$\\r\\n|combined\\\\rangle = c_1c_3 |00\\\\rangle + c_1c_4 |01\\\\rangle + c_2c_3 |10\\\\rangle + c_2c_4 |11\\\\rangle\\r\\n$$\\r\\n\\r\\nThe power of quantum computing is you can represent $2^n$ states if you have $n$ qubits, which is an exponential growth. For example, if you have 20 qubits, you can represent $2^{20} = 1048576$ states, but in a classical computer, it is only 20! If you have 266 qubits, you can represent nearly $10^{80}$ states, which is about the number of atoms in the observable universe! Every single \\"operation\\" you perform on these qubits instantly affects its $2^n$ states. Now we will talk about these \\"operations\\" &mdash; quantum circuit gates.\\r\\n\\r\\n### Gates\\r\\n\\r\\nTo visualize a single qubit, you can use the Bloch Sphere:\\r\\n![Bloch Sphere](bloch_sphere.png)\\r\\nThe z-axis means the probability in being measured in $|0\\\\rangle$ and $|1\\\\rangle$. X-axis and y-axis are also similar.\\r\\n\\r\\n:::info\\r\\n$$\\r\\n|+\\\\rangle = \\\\frac{1}{\\\\sqrt{2}}(|0\\\\rangle + |1\\\\rangle)\\r\\n$$\\r\\n$$\\r\\n|-\\\\rangle = \\\\frac{1}{\\\\sqrt{2}}(|0\\\\rangle - |1\\\\rangle)\\r\\n$$\\r\\n$$\\r\\n|+i\\\\rangle = \\\\frac{1}{\\\\sqrt{2}}(|0\\\\rangle - i|1\\\\rangle)\\r\\n$$\\r\\n$$\\r\\n|-i\\\\rangle = \\\\frac{1}{\\\\sqrt{2}}(|0\\\\rangle - i|1\\\\rangle)\\r\\n$$\\r\\n:::\\r\\n\\r\\nQuantum circuit gates can be visualized as rotating the state (the red arrow) in the bloch sphere. Here are some examples:\\r\\n\\r\\n1. **Pauli-X Gate (NOT Gate)**:\\r\\n   - Rotation: $$ \\\\pi $$ radians (180 degrees) around the X-axis.\\r\\n   - Effect: Transforms the state $$ |0\\\\rangle $$ to $$ |1\\\\rangle $$ and vice versa. On the Bloch sphere, it flips the state from the north pole to the south pole and vice versa.\\r\\n\\r\\n2. **Pauli-Y Gate**:\\r\\n   - Rotation: $$ \\\\pi $$ radians (180 degrees) around the Y-axis.\\r\\n   - Effect: Applies a complex phase and swaps the states $$ |0\\\\rangle $$ and $$ |1\\\\rangle $$. For example, $$ |0\\\\rangle $$ becomes $$ i|1\\\\rangle $$ and $$ |1\\\\rangle $$ becomes $$ -i|0\\\\rangle $$.\\r\\n\\r\\n3. **Pauli-Z Gate**:\\r\\n   - Rotation: $$ \\\\pi $$ radians (180 degrees) around the Z-axis.\\r\\n   - Effect: Leaves the state $$ |0\\\\rangle $$ unchanged and adds a phase of $$ \\\\pi $$ (equivalent to a factor of -1) to the state $$ |1\\\\rangle $$.\\r\\n\\r\\n4. **Hadamard Gate (H Gate)**:\\r\\n   - Rotation: This gate performs a more complex transformation, equivalent to rotating around the axis at 45 degrees from both X and Z axes, followed by a $$ \\\\pi $$ radian rotation around the Y-axis.\\r\\n   - Effect: Creates superpositions from basis states, turning $$ |0\\\\rangle $$ into $$ \\\\frac{|0\\\\rangle + |1\\\\rangle}{\\\\sqrt{2}} $$ and $$ |1\\\\rangle $$ into $$ \\\\frac{|0\\\\rangle - |1\\\\rangle}{\\\\sqrt{2}} $$.\\r\\n\\r\\n5. **S Gate (Phase Gate)**:\\r\\n   - Rotation: $$ \\\\frac{\\\\pi}{2} $$ radians (90 degrees) around the Z-axis.\\r\\n   - Effect: Leaves the state $$ |0\\\\rangle $$ unchanged and multiplies the state $$ |1\\\\rangle $$ by $$ i $$.\\r\\n\\r\\n6. **T Gate**:\\r\\n   - Rotation: $$ \\\\frac{\\\\pi}{4} $$ radians (45 degrees) around the Z-axis.\\r\\n   - Effect: Leaves the state $$ |0\\\\rangle $$ unchanged and multiplies the state $$ |1\\\\rangle $$ by $$ e^{i\\\\pi/4} $$, which is a more subtle phase shift than the S gate.\\r\\n\\r\\nAnd some parameterized gates:\\r\\n\\r\\n7. **RX Gate**:\\r\\n    - Rotation: $$\\\\theta$$ radians around the X-axis. It can be represented as a matrix multiplication between the unitary gate matrix and the quantum states, the gate matrix is:\\r\\n\\r\\n$$\\r\\nR_x(\\\\theta) = \\\\begin{bmatrix}\\r\\n\\\\cos\\\\left(\\\\frac{\\\\theta}{2}\\\\right) & -i\\\\sin\\\\left(\\\\frac{\\\\theta}{2}\\\\right) \\\\\\\\\\r\\n-i\\\\sin\\\\left(\\\\frac{\\\\theta}{2}\\\\right) & \\\\cos\\\\left(\\\\frac{\\\\theta}{2}\\\\right)\\r\\n\\\\end{bmatrix}\\r\\n$$\\r\\n\\r\\n8. **RY Gate**:\\r\\n    - Rotation: $$\\\\theta$$ radians around the y-axis:\\r\\n$$\\r\\nR_y(\\\\theta) = \\\\begin{bmatrix}\\r\\n\\\\cos\\\\left(\\\\frac{\\\\theta}{2}\\\\right) & -\\\\sin\\\\left(\\\\frac{\\\\theta}{2}\\\\right) \\\\\\\\\\r\\n\\\\sin\\\\left(\\\\frac{\\\\theta}{2}\\\\right) & \\\\cos\\\\left(\\\\frac{\\\\theta}{2}\\\\right)\\r\\n\\\\end{bmatrix}\\r\\n$$\\r\\n\\r\\n8. **RZ Gate**:\\r\\n    - Rotation: $$\\\\theta$$ radians around the z-axis:\\r\\n$$\\r\\nR_z(\\\\theta) = \\\\begin{bmatrix}\\r\\ne^{-i\\\\theta/2} & 0 \\\\\\\\\\r\\n0 & e^{i\\\\theta/2}\\r\\n\\\\end{bmatrix}\\r\\n$$\\r\\n\\r\\nAnd much more.\\r\\n\\r\\n:::danger[INFO]\\r\\nEvery gate can be represented as an unitary gate matrix, e.g. the Hadamard Gate:\\r\\n$$\\r\\nH = \\\\frac{1}{\\\\sqrt{2}}\\\\begin{bmatrix}\\r\\n1 & 1 \\\\\\\\\\r\\n1 & -1\\r\\n\\\\end{bmatrix}\\r\\n$$\\r\\n:::\\r\\n\\r\\nAlso, there are double-qubits gate, for example the CNOT (Controlled-NOT) gate:\\r\\n$$\\r\\nCNOT = \\\\begin{bmatrix}\\r\\n1 & 0 & 0 & 0 \\\\\\\\\\r\\n0 & 1 & 0 & 0 \\\\\\\\\\r\\n0 & 0 & 0 & 1 \\\\\\\\\\r\\n0 & 0 & 1 & 0\\r\\n\\\\end{bmatrix}\\r\\n$$\\r\\nControlled gates mean if the first qubit is $|1\\\\rangle$, then the gate (NOT in this case) on second qubit will be performed. There are even triple-qubits gates, like the Toffoli gate (aka CCNOT), which has a $2^n \\\\times 2^n = 8 \\\\times 8$ size gate matrix.\\r\\n\\r\\nIf you have a quantum states, say 10 qubits, and you want to apply a specific gate on a specific qubit, you can use the Kronecker product to construct a $2^n \\\\times 2^n$ gate matrix and apply it to the quantum states, for example you want to apply the Hadamard gate on the 4th qubit:\\r\\n\\r\\n$$\\r\\n|\\\\psi_{t+1}\\\\rangle = |\\\\psi_t\\\\rangle \\\\times (I \\\\otimes I \\\\otimes I \\\\otimes \\\\frac{1}{\\\\sqrt{2}}\\\\begin{bmatrix}1 & 1 \\\\\\\\ 1 & -1\\\\end{bmatrix} \\\\otimes I \\\\otimes I \\\\otimes I \\\\otimes I \\\\otimes I \\\\otimes I)\\r\\n$$\\r\\n\\r\\n:::tip\\r\\n$I$ is the identity matrix, i.e. $\\\\begin{bmatrix}1 & 0 \\\\\\\\ 0 & 1\\\\end{bmatrix}$\\r\\n:::\\r\\n\\r\\nThese quantum gates are crucial in constructing quantum circuits for algorithms, where each type of gate contributes to manipulating the qubit\'s state in a controlled manner to achieve various computational goals.\\r\\n\\r\\n## Simulation with Qiskit\\r\\n\\r\\nQiskit is an open-source framework for quantum computing. It provides the tools for creating quantum circuits, simulating them, and running them on real quantum hardware through IBM\'s cloud services.\\r\\n\\r\\n### Setting Up Your Environment\\r\\n\\r\\nTo start using Qiskit, make sure Python is installed on your machine, then install Qiskit and Qiskit Aer, which includes simulators that run on your local machine.\\r\\n\\r\\n```bash\\r\\npip install qiskit qiskit-aer pylatexenc\\r\\n```\\r\\n\\r\\n### Single Qubit Operations\\r\\n\\r\\n#### Building the Circuit\\r\\n\\r\\nWe\'ll begin with a basic single-qubit circuit to demonstrate sequential gate operations that include rotations and a phase shift.\\r\\n\\r\\n```python\\r\\nfrom qiskit import QuantumCircuit, transpile\\r\\nfrom qiskit_aer import Aer\\r\\nfrom qiskit.visualization import plot_bloch_multivector\\r\\n\\r\\n# Initialize a Quantum Circuit with 1 qubit\\r\\nqc_single = QuantumCircuit(1)\\r\\n\\r\\n# Apply rotation around the X-axis\\r\\nqc_single.rx(3.14159 / 4, 0)  # Pi/4 rotation\\r\\n\\r\\n# Apply rotation around the Y-axis\\r\\nqc_single.ry(3.14159 / 2, 0)  # Pi/2 rotation\\r\\n\\r\\n# Apply a Z gate\\r\\nqc_single.z(0)\\r\\n\\r\\n# Visualize the circuit\\r\\nprint(\\"Single Qubit Circuit:\\")\\r\\nprint(qc_single.draw(output=\'mpl\'))\\r\\n```\\r\\n\\r\\n#### Calculating the Expected State Vector\\r\\n\\r\\nThe state vector undergoes a rotation by $$\\\\pi/4$$ radians around the X-axis, followed by $$\\\\pi/2$$ around the Y-axis, and finally, a phase shift due to the Z gate.\\r\\n\\r\\n#### Simulation and Output\\r\\n\\r\\n```python\\r\\n# Prepare the simulator\\r\\nsimulator = Aer.get_backend(\'statevector_simulator\')\\r\\n\\r\\n# Transpile the circuit for the simulator\\r\\ntranspiled_qc = transpile(qc_single, simulator)\\r\\n\\r\\n# Run the simulation\\r\\njob = simulator.run(transpiled_qc)\\r\\nresult = job.result()\\r\\n\\r\\n# Get the final state vector\\r\\nstatevector = result.get_statevector()\\r\\nprint(statevector)\\r\\n\\r\\n# Plot the state on the Bloch sphere\\r\\nplot_bloch_multivector(statevector)\\r\\n```\\r\\n\\r\\n### Multi-Qubit Operations\\r\\n\\r\\n#### Building the Circuit\\r\\n\\r\\nWe will construct a five-qubit circuit with a combination of entanglement, rotations, and phase shifts to demonstrate the interplay of different quantum gates.\\r\\n\\r\\n```python\\r\\nfrom qiskit import QuantumCircuit, transpile\\r\\nfrom qiskit_aer import Aer\\r\\nfrom qiskit.visualization import plot_bloch_multivector\\r\\nfrom math import pi\\r\\n\\r\\n# Initialize a Quantum Circuit with 5 qubits\\r\\nqc_multi = QuantumCircuit(5)\\r\\n\\r\\n# Apply a Hadamard gate to qubit 0\\r\\nqc_multi.h(0)\\r\\n\\r\\n# Apply a CNOT gate between qubit 0 and qubit 1\\r\\nqc_multi.cx(0, 1)\\r\\n\\r\\n# Apply a rotation around the X-axis by pi/2 on qubit 1\\r\\nqc_multi.rx(pi / 2, 1)\\r\\n\\r\\n# Apply a T gate to qubit 2\\r\\nqc_multi.t(2)\\r\\n\\r\\n# Apply a CNOT gate between qubit 1 and qubit 2\\r\\nqc_multi.cx(1, 2)\\r\\n\\r\\n# Apply a rotation around the Z-axis by pi/4 on qubit 3\\r\\nqc_multi.rz(pi / 4, 3)\\r\\n\\r\\n# Apply a Hadamard gate to qubit 4\\r\\nqc_multi.h(4)\\r\\n\\r\\n# Apply a CNOT gate between qubit 4 and qubit 3\\r\\nqc_multi.cx(4, 3)\\r\\n\\r\\n# Apply a CNOT gate between qubit 1 and qubit 4\\r\\nqc_multi.cx(1, 4)\\r\\n\\r\\n# Visualize the Circuit\\r\\nprint(\\"Five-Qubit Circuit:\\")\\r\\nprint(qc_multi.draw(output=\'mpl\'))\\r\\n```\\r\\n\\r\\n#### Calculating the Expected State Vector\\r\\n\\r\\nNow let\'s calculate the expected state vector after applying the specified sequence of gates:\\r\\n\\r\\n1. **Initial State**: All qubits start in the state $|00000\\\\rangle$.\\r\\n\\r\\n2. **After $ H(0) $**: Creates a superposition on qubit 0.\\r\\n   $\\\\frac{1}{\\\\sqrt{2}} (|00000\\\\rangle + |10000\\\\rangle)$\\r\\n\\r\\n3. **After $ \\\\text{CNOT}(0, 1) $**: Entangles qubits 0 and 1.\\r\\n   $\\\\frac{1}{\\\\sqrt{2}} (|00000\\\\rangle + |11000\\\\rangle)$\\r\\n\\r\\n4. **After $ \\\\text{RX}(1, \\\\pi/2) $**: Applies a $ \\\\pi/2 $ rotation on qubit 1, transforming its basis states.\\r\\n   $\\\\frac{1}{\\\\sqrt{2}} (|00000\\\\rangle + i|11000\\\\rangle)$\\r\\n\\r\\n5. **After $ T(2) $**: Adds a phase of $ \\\\pi/4 $ to the state of qubit 2.\\r\\n   $\\\\frac{1}{\\\\sqrt{2}} (|00000\\\\rangle + ie^{i\\\\pi/4}|11000\\\\rangle)$\\r\\n\\r\\n6. **After $ \\\\text{CNOT}(1, 2) $**: Conditionally flips qubit 2 based on qubit 1.\\r\\n   $\\\\frac{1}{\\\\sqrt{2}} (|00000\\\\rangle + ie^{i\\\\pi/4}|11100\\\\rangle)$\\r\\n\\r\\n7. **After $ \\\\text{RZ}(3, \\\\pi/4) $**: Adds a phase of $ \\\\pi/4 $ to qubit 3.\\r\\n   $\\\\frac{1}{\\\\sqrt{2}} (|00000\\\\rangle + ie^{i\\\\pi/4}|11100\\\\rangle)$ (no change in basis states, just a phase on the amplitude if qubit 3 were in state |1|)\\r\\n\\r\\n8. **After $ H(4) $**: Creates a superposition on qubit 4.\\r\\n   $\\\\frac{1}{2} (|00000\\\\rangle + |00001\\\\rangle + ie^{i\\\\pi/4}|11100\\\\rangle + ie^{i\\\\pi/4}|11101\\\\rangle)$\\r\\n\\r\\n9. **After $ \\\\text{CNOT}(4, 3) $**: Conditionally flips qubit 3 based on qubit 4.\\r\\n   $\\\\frac{1}{2} (|00000\\\\rangle + |00001\\\\rangle + ie^{i\\\\pi/4}|11110\\\\rangle + ie^{i\\\\pi/4}|11111\\\\rangle)$\\r\\n\\r\\n10. **After $ \\\\text{CNOT}(1, 4) $**: Conditionally flips qubit 4 based on qubit 1.\\r\\n    $\\\\frac{1}{2} (|00000\\\\rangle + |00001\\\\rangle + ie^{i\\\\pi/4}|11010\\\\rangle + ie^{i\\\\pi/4}|11011\\\\rangle)$\\r\\n\\r\\n#### Simulation and Output\\r\\n\\r\\n```python\\r\\n# Prepare the simulator\\r\\nsimulator = Aer.get_backend(\'statevector_simulator\')\\r\\n\\r\\n# Transpile the circuit for the simulator\\r\\ntranspiled_qc = transpile(qc_multi, simulator)\\r\\n\\r\\n# Run the simulation\\r\\njob = simulator.run(transpiled_qc)\\r\\nresult = job.result()\\r\\n\\r\\n# Get the final state vector\\r\\nstatevector = result.get_statevector()\\r\\nprint(statevector)\\r\\n\\r\\n# Plot the state on the Bloch sphere\\r\\nplot_bloch_multivector(statevector)\\r\\n```\\r\\n\\r\\n## Tasks you may do\\r\\n1. Implement Shor\'s Algorithm for finding the prime factors of an integer.\\r\\n2. Implement Grover\'s Algorithm for an unstructured search that finds with high probability the unique input to a black box function that produces a particular output value."},{"id":"intro-to-pytorch","metadata":{"permalink":"/blog/intro-to-pytorch","source":"@site/blog/2023-12-13-intro-to-pytorch/index.md","title":"Introduction to PyTorch","description":"This blog post will be about some basic introductions to PyTorch, including tensors, and how to train your own model in PyTorch.","date":"2023-12-13T00:00:00.000Z","tags":[{"label":"beginners","permalink":"/blog/tags/beginners"},{"label":"nn","permalink":"/blog/tags/nn"},{"label":"pytorch","permalink":"/blog/tags/pytorch"},{"label":"tutorial","permalink":"/blog/tags/tutorial"},{"label":"tired","permalink":"/blog/tags/tired"}],"readingTime":5.835,"hasTruncateMarker":true,"authors":[{"name":"Eason Xie","title":"Website Owner","url":"https://easonoob.github.io","imageURL":"https://avatars.githubusercontent.com/u/100521878?v=4","key":"eason"}],"frontMatter":{"slug":"intro-to-pytorch","title":"Introduction to PyTorch","authors":"eason","tags":["beginners","nn","pytorch","tutorial","tired"]},"unlisted":false,"prevItem":{"title":"Introduction to Quantum Computing","permalink":"/blog/intro-to-quantum-computing"},"nextItem":{"title":"The Most Basics of Neural Networks","permalink":"/blog/basic-of-nn"}},"content":"This blog post will be about some basic introductions to PyTorch, including tensors, and how to train your own model in PyTorch.\\r\\n\\r\\n\x3c!--truncate--\x3e\\r\\n\\r\\n## Prerequisites\\r\\n- Basic Maths\\r\\n- Basic Python\\r\\n- Common Sense\\r\\n\\r\\n## Aims\\r\\n1.\\tUnderstand tensors\\r\\n2.\\tUnderstand the steps to build and train a model in PyTorch\\r\\n\\r\\n## Introduction\\r\\n> \u201cArtificial Intelligence, deep learning, machine learning \u2014 whatever you\'re doing if you don\'t understand it \u2014 learn it. Because otherwise you\'re going to be a dinosaur within 3 years.\u201d ~ Mark Cuban.\\r\\n\\r\\n## Tensors\\r\\n[https://pytorch.org/tutorials/beginner/basics/tensorqs_tutorial.html](https://pytorch.org/tutorials/beginner/basics/tensorqs_tutorial.html)\\r\\n\\r\\nTensors are specialized arrays or matrices. It can have as many dimensions as you want. We use tensors to encode the data in the PyTorch, as well as the model parameters. Specifically, if you perform operations on a PyTorch tensor (e.g. addition, multiplication), your action will be saved into PyTorch\u2019s built-in dynamic computation graph, which is necessary for model backpropagation in training.\\r\\n\\r\\nHere\u2019s an example of some tensors: \\r\\n\\r\\n```python\\r\\nimport torch\\r\\n\\r\\n# Create a tensor from a list/array:\\r\\n\\r\\ndata1 = [0, 1, 2, 3, 4, 5] # 1D array\\r\\ndata2 = [[0, 1], [2, 3], [4, 5]] # 2D array\\r\\n\\r\\ntensor1 = torch.tensor(data1)\\r\\ntensor2 = torch.tensor(data2)\\r\\n\\r\\nprint(tensor1.shape) # torch.Size([6])\\r\\nprint(tensor2.shape) # torch.Size([3, 2])\\r\\n\\r\\n# Create a tensor filled with zeros or ones:\\r\\n\\r\\nzeros_tensor = torch.zeros(3, 5, 4, 2, 1, 4) # A 6D tensor\\r\\nones_tensor = torch.ones(3, 5)\\r\\n\\r\\n# Create a random tensor from a specified shape:\\r\\n\\r\\nrandom_tensor1 = torch.randn(10, 20) # A 10 by 20 tensor with normal distribution\\r\\nprint(random_tensor1.shape) # torch.Size([10, 20])\\r\\nprint(random_tensor1[0, :10]) # You can use python array index slicing\\r\\n\\r\\nrandom_tensor2 = torch.randint(0, 100, (5, 10)) # 5 by 10 tensor with integer values ranging from 0 to 100\\r\\nprint(random_tensor2[0, 3:10])\\r\\n\\r\\n# Attributes of a tensor:\\r\\n\\r\\nprint(tensor1.shape)\\r\\nprint(tensor1.dtype) # data type, e.g. float32, int64, float64, Bfloat16\\r\\nprint(tensor1.device) # device the tensor is stored on, e.g. cpu, cuda\\r\\n```\\r\\n\\r\\n## Training a Simple Model in PyTorch\\r\\n\\r\\nThe steps of training a model in PyTorch includes:\\r\\n1.\\tFind a dataset that suits your problem, download it, and create a dataloader.\\r\\n2.\\tDefine your model, you can create your own model module using `nn.Module`.\\r\\n3.\\tDefine your loss function (from `torch.nn`), optimizers (from `torch.optim`), etc.\\r\\n4.\\tDefine your training loop, it can be a function that does a single step and write a loop, or just simply a training loop.\\r\\n5.\\tStart training!\\r\\n\\r\\nFor this example, we are going to use the MNIST dataset, which is built-in in PyTorch, making it very easy to download and use.\\r\\n\\r\\nFirst import the necessary libraries:\\r\\n```python\\r\\nimport torch\\r\\nfrom torch import nn\\r\\nfrom torch.utils.data import DataLoader\\r\\nfrom torchvision import datasets\\r\\nfrom torchvision.transforms import ToTensor\\r\\n```\\r\\n\\r\\n### Finding and defining your dataset\\r\\n\\r\\nPyTorch offers domain-specific libraries such as TorchText, TorchVision, and TorchAudio, all of which include datasets. For this tutorial, we will be using a TorchVision dataset.\\r\\n\\r\\nThe `torchvision.datasets` module contains `Dataset` objects for many real-world vision data like CIFAR, COCO. In this tutorial, we use the FashionMNIST dataset. Every TorchVision `Dataset` includes two arguments: `transform` and `target_transform` to modify the samples and labels respectively.\\r\\n\\r\\n```python\\r\\n# Download training data from the MNIST dataset.\\r\\ntraining_data = datasets.FashionMNIST(\\r\\n    root=\\"data\\",\\r\\n    train=True,\\r\\n    download=True,\\r\\n    transform=ToTensor(), # Convert to PyTorch tensor.\\r\\n)\\r\\n\\r\\n# Download test data from the MNIST dataset.\\r\\ntest_data = datasets.FashionMNIST(\\r\\n    root=\\"data\\",\\r\\n    train=False,\\r\\n    download=True,\\r\\n    transform=ToTensor(),\\r\\n)\\r\\n```\\r\\n\\r\\nWe pass the `Dataset` as an argument to `DataLoader`. This wraps an iterable over our dataset, and supports automatic batching, sampling, shuffling and multiprocess data loading. Here we define a batch size of 64, i.e. each element in the dataloader iterable will return a batch of 64 features and labels.\\r\\n\\r\\n```python\\r\\nbatch_size = 64\\r\\n\\r\\n# Create data loaders.\\r\\ntrain_dataloader = DataLoader(training_data, batch_size=batch_size)\\r\\ntest_dataloader = DataLoader(test_data, batch_size=batch_size)\\r\\n\\r\\nfor X, y in test_dataloader:\\r\\n    print(f\\"Shape of X [N, C, H, W]: {X.shape}\\") # N, C, H, W stands for Batch Size, Channel Size, Height, and Width\\r\\n    print(f\\"Shape of y: {y.shape} {y.dtype}\\")\\r\\n    break\\r\\n```\\r\\n\\r\\n### Create the model\\r\\n\\r\\nTo define a neural network in PyTorch, we create a class that inherits from nn.Module. We define the layers of the network in the `__init__` function and specify how data will pass through the network in the forward function. To accelerate operations in the neural network, we move it to the GPU (`cuda`) or MPS if available.\\r\\n\\r\\n```python\\r\\n# Get cpu, gpu (cuda) device for training.\\r\\ndevice = (\\r\\n    \\"cuda\\" if torch.cuda.is_available() else \\"cpu\\"\\r\\n)\\r\\nprint(f\\"Using {device} device\\")\\r\\n\\r\\n# Define model\\r\\nclass NeuralNetwork(nn.Module):\\r\\n    def __init__(self):\\r\\n        super().__init__()\\r\\n        self.flatten = nn.Flatten() # Flattens to (Batch Size, Channel Size * Height * Width), from a 4D tensor to a 2D tensor\\r\\n        self.linear_relu_stack = nn.Sequential(\\r\\n            nn.Linear(28*28, 512), # Fully-connected hidden layer\\r\\n            nn.ReLU(), # Activation Function\\r\\n            nn.Linear(512, 512),\\r\\n            nn.ReLU(),\\r\\n            nn.Linear(512, 10)\\r\\n        ) # When `linear_relu_stack` is called, it will run all the modules inside in order.\\r\\n\\r\\n    def forward(self, x):\\r\\n        x = self.flatten(x) # flatten to 2D, same as x.view(x.size(0), -1)\\r\\n        logits = self.linear_relu_stack(x) # the nn.Sequential instance\\r\\n        return logits\\r\\n\\r\\n# Create an instance\\r\\nmodel = NeuralNetwork().to(device)\\r\\nprint(model)\\r\\n```\\r\\n\\r\\n[Learn More About Building Neural Networks in PyTorch Here](https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html)\\r\\n\\r\\n### Define the Loss Function and the Optimizer\\r\\n\\r\\nWe are using the Cross Entropy Loss loss function and the Stochastic Gradient Descent (SGD) optimizer for training this model.\\r\\n\\r\\n```python\\r\\nloss_fn = nn.CrossEntropyLoss()\\r\\noptimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\\r\\n```\\r\\n\\r\\n### Define the training loop\\r\\n\\r\\nFirst we define the function to train a single step:\\r\\n\\r\\n```python\\r\\ndef train(dataloader, model, loss_fn, optimizer):\\r\\n    size = len(dataloader.dataset)\\r\\n    model.train()\\r\\n    for batch, (X, y) in enumerate(dataloader):\\r\\n        X, y = X.to(device), y.to(device) # Move tensors to cuda if available\\r\\n\\r\\n        # Compute prediction error\\r\\n        pred = model(X) # Forward pass\\r\\n        loss = loss_fn(pred, y) # Compute loss\\r\\n\\r\\n        # Backpropagation\\r\\n        loss.backward() # Compute gradients\\r\\n        optimizer.step() # Update parameters\\r\\n        optimizer.zero_grad() # Zero the gradients\\r\\n\\r\\n        if batch % 100 == 0:\\r\\n            loss, current = loss.item(), (batch + 1) * len(X)\\r\\n            print(f\\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\\")\\r\\n```\\r\\n\\r\\nThen a function to evaluate (or validate). The goal of this is to check the model\u2019s performance against the test dataset to ensure it is learning, and monitor if it is overfitting.\\r\\n\\r\\n```python\\r\\ndef test(dataloader, model, loss_fn):\\r\\n    size = len(dataloader.dataset)\\r\\n    num_batches = len(dataloader)\\r\\n    model.eval()\\r\\n    test_loss, correct = 0, 0\\r\\n    with torch.no_grad():\\r\\n        for X, y in dataloader:\\r\\n            X, y = X.to(device), y.to(device)\\r\\n            pred = model(X)\\r\\n            test_loss += loss_fn(pred, y).item()\\r\\n            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\\r\\n    test_loss /= num_batches\\r\\n    correct /= size\\r\\n    print(f\\"Test Error: \\\\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\\\n\\")\\r\\n```\\r\\n\\r\\nThe training process is conducted over several iterations (epochs) that go over the entire training dataset. During each epoch, the model learns parameters to make better predictions. We print the model\u2019s accuracy and loss at each epoch; we\u2019d like to see the accuracy increase and the loss decrease with every epoch.\\r\\n\\r\\n```python\\r\\nepochs = 5\\r\\nfor t in range(epochs):\\r\\n    print(f\\"Epoch {t+1}\\\\n-------------------------------\\")\\r\\n    train(train_dataloader, model, loss_fn, optimizer)\\r\\n    test(test_dataloader, model, loss_fn)\\r\\nprint(\\"Done!\\")\\r\\n```\\r\\n\\r\\n[Read More About Model Training Here](https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html)\\r\\n\\r\\n## Tasks\\r\\n\\r\\n1. Plot some graphs to show the training loss, validation loss, and validation accuracy over step.\\r\\n2. Experiment with different learning rate, batch size, and model architecture, how does it affect the results?"},{"id":"basic-of-nn","metadata":{"permalink":"/blog/basic-of-nn","source":"@site/blog/2023-11-13-basic-of-nn/index.md","title":"The Most Basics of Neural Networks","description":"This blog post will be about the most basics of neural networks, for absolute beginners.","date":"2023-11-13T00:00:00.000Z","tags":[{"label":"beginners","permalink":"/blog/tags/beginners"},{"label":"nn","permalink":"/blog/tags/nn"},{"label":"tutorial","permalink":"/blog/tags/tutorial"},{"label":"tired","permalink":"/blog/tags/tired"}],"readingTime":6.54,"hasTruncateMarker":true,"authors":[{"name":"Eason Xie","title":"Website Owner","url":"https://easonoob.github.io","imageURL":"https://avatars.githubusercontent.com/u/100521878?v=4","key":"eason"}],"frontMatter":{"slug":"basic-of-nn","title":"The Most Basics of Neural Networks","authors":"eason","tags":["beginners","nn","tutorial","tired"]},"unlisted":false,"prevItem":{"title":"Introduction to PyTorch","permalink":"/blog/intro-to-pytorch"},"nextItem":{"title":"Website Launch!","permalink":"/blog/website-launch"}},"content":"This blog post will be about the most basics of neural networks, for absolute beginners.\\r\\n\\r\\nYou\'ve heard about things like ChatGPT, LlaMa, Midjourney, Dall-E, and Stable Diffusion. But have you ever wondered how exactly does they work? In this blog series, I will explain the neural network, from the absolute basics to advanced, from simple fully-connected networks to Transformers, Quantum Neural Networks, and Graph Neural Networks. If you encountered any questions, feel free to ask by directly through my email or by Discord!\\r\\n\\r\\n\x3c!--truncate--\x3e\\r\\n\\r\\n## Prerequisites\\r\\n- Basic Maths\\r\\n- Basic Python\\r\\n- Common Sense\\r\\n\\r\\n## Aims\\r\\n1.\\tUnderstand the neurons and neural networks\\r\\n2.\\tUnderstand the use of neural networks\\r\\n3.\\tUnderstand forward pass, loss function, backward pass, and weight update\\r\\n\\r\\n## Introduction\\r\\n> \u201cI think the brain is essentially a computer and consciousness is like a computer program. It will cease to run when the computer is turned off. Theoretically, it could be re-created on a neural network, but that would be very difficult, as it would require all one\'s memories.\u201d ~ Stephen Hawking\\r\\n\\r\\n## Important Concepts\\r\\n\\r\\n### Neurons and Neural Networks\\r\\nImagine you are making a system to predict property prices. How would you implement such a system? Maybe it will take the different factors of a property (e.g. size, height, view, surrounding environment, etc.) to calculate the price and outputs it. But how can you calculate the price based on these inputs (factors)?\\r\\n\\r\\nLet\u2019s simplify the problem and assume there is only one factor: the size (area) of the property. Let\u2019s plot a graph with the area on the x-axis, price on the y-axis:\\r\\n\\r\\n![Graph 1](./graph-1.png)\\r\\n\\r\\nThen we can draw a straight fit line on the graph to relate all the data points:\\r\\n:::tip\\r\\nNote that the fit line may not pass through all/any data points!\\r\\n:::\\r\\n\\r\\n![Graph 2](./graph-2.png)\\r\\n\\r\\nHow can you represent the fit line in maths? We can use a simple linear function:\\r\\n\\r\\n$$\\r\\nf(x) = wx + b\\r\\n$$\\r\\n\\r\\nWhere w is the **weight** (the steepness of the linear), b is the **bias** (the y-intercept, or height of the line). In fact, we can represent any straight line on a two-dimensional Cartesian coordinate space and any linear functions with one variable with this function. In the neural network, this function is the simplest form of a **neuron**. But what is a neuron? This is an anatomy of a human neuron cell:\\r\\n\\r\\n![Neuron Anatomy](./neuron-anatomy.png)\\r\\n\\r\\nThe dendrites receive the signals, the cell processes the signals, and the axon terminals send the signals to other neuron cells. By connecting 86 billion of these cells, it forms your brain. We can simulate a neuron in maths with the following function:\\r\\n\\r\\n$$\\r\\nf(x_1, x_2, x_3, x_4, \\\\ldots) = w_1 x_1 + w_2 x_2 + w_3 x_3 + w_4 x_4 + \\\\cdots + b\\r\\n$$\\r\\n\\r\\nThe multiple weights represent the \u201cimportance\u201d of each of the different input x. The b is still the bias. Here\u2019s another diagram to understand the neuron:\\r\\n\\r\\n![Diagram 1](./diagram-1.png)\\r\\n\\r\\nSo, for example, the multiple x can be the different factors of the property, and the weights are parameters that you can tune, representing the importance of that factor contributing to the final price. Wait, what is the f(x) in the above diagram? It is an Activation Function. Its purpose is to introduce non-linearity into the neural network as combining linear functions can only result in a linear function:\\r\\n\\r\\nLet $f(x) = w_1 x + b_1$, $g(x) = w_2 x + b_2$, then $h(x) = g(f(x))$:\\r\\n$$\\r\\nh(x) = g(f(x)) = w_2 (w_1 x + b_1) + b_2\\r\\n$$\\r\\n$$\\r\\nh(x) = w_2 w_1 x + w_2 b_1 + b_2\\r\\n$$\\r\\nLet $a = w_2 w_1$ and $c = w_2 b_1 + b_2$, then:\\r\\n$$\\r\\nh(x) = ax + c\\r\\n$$\\r\\n\\r\\nWhich is a straight line on the graph. But by adding activation functions in between, e.g. $h(x)=g(a(f(x)))$, where $a$ is the activation function, we can create a non-linear (broadly speaking not a straight line on the graph) function or network. Some common activation functions include ReLU, Sigmoid, Tanh, LeakyReLU, GELU, and Softmax. The details of the activation function will be talked about in later blog posts.\\r\\n\\r\\nBy combining and connecting multiple neurons in an orderly manner, you get a neural network. Below is a diagram of a neural network which you\u2019ve probably seen before:\\r\\n\\r\\n![Diagram 2](./diagram-2.png)\\r\\n\\r\\nEach circle (node) in the neural network represents a neuron. Each neuron in the input layer processes a single number and each neuron in the output layer outputs a single number. Neural networks are **universal function approximators**.\\r\\n:::info\\r\\nNote that while the neurons in the hidden layer may seem to have multiple outputs, the outputs are the same.\\r\\n:::\\r\\n\\r\\n### Training of neural networks\\r\\n\\r\\nHowever, how can you calculate the weights and biases given x and y? Below are the processes of training a neural network to find the optimal weights and biases:\\t\\r\\n\\r\\n1.\\t**Initialize the weights and biases** using random numbers with normal or uniform distribution.\\r\\n2.\\t**Forward pass:** Run the model with the input x, and get the model output y.\\r\\n3.\\t**Compute loss:** Compare the model-output y with the real targeted y and calculate the difference using a loss function. \\r\\n4.\\t**Backpropagation/backward pass:** Calculate the gradients of each parameter (weight or bias) using gradient descent with partial derivative. Which means how the output of the model changes with that parameter changing.\\r\\n5.\\t**Update parameters:** Update the parameters using optimizers based on the gradients calculated and the given learning rate hyperparameter (usually between 1e-2 to 1e-6).\\r\\n6.\\t**Repeat Step 2 to 5** with different x and y until the loss is good enough.\\r\\n\\r\\n![Diagram 3](./diagram-3.png)\\r\\n\\r\\n## Code Implementations\\r\\n\\r\\n### A simple neural network with PyTorch\\r\\n\\r\\nLet\u2019s consider a simple formula: y=3x+1. Given an array of x and array of the corresponding y, find 3 and 1 (the weight and the bias). Run the following codes in Google Colab for simplicity.\\r\\n\\r\\n```python\\r\\nimport torch # main library\\r\\nimport torch.nn as nn # neural network modules and functions\\r\\nimport torch.optim as optim # optimizers for neural networks\\r\\nimport matplotlib.pyplot as plt # plotting for analysing the loss\\r\\n\\r\\ntorch.manual_seed(69) # define a manual seed so the results are reproducible\\r\\n```\\r\\n\\r\\nDefine the data x and y:\\r\\n\\r\\n```python\\r\\nx = torch.tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10], dtype=torch.float32)\\r\\ny = 3*x + 1\\r\\n\\r\\nprint(x.tolist())\\r\\nprint(y.tolist())\\r\\n```\\r\\n\\r\\nDefine the model, loss function, and optimizer:\\r\\n\\r\\n```python\\r\\nmodel = nn.Linear(1, 1) # a single neuron\\r\\ncriterion = nn.MSELoss() # Mean squared error (MSE) loss function\\r\\noptimizer = optim.SGD(model.parameters(), lr=1e-4) # Stochastic Gradient Descent (SGD) optimizer\\r\\n```\\r\\n\\r\\nTraining Loop:\\r\\n\\r\\n```python\\r\\nepochs = 100\\r\\nlosses = []\\r\\nfor i in range(epochs):\\r\\n    for iter_x, iter_y in zip(x, y):\\r\\n        iter_x = iter_x.unsqueeze(0) # add another dimension at the end\\r\\n        iter_y = iter_y.unsqueeze(0) # same\\r\\n        optimizer.zero_grad() # zero the gradients\\r\\n        output = model(iter_x) # Forward pass\\r\\n        loss = criterion(output, iter_y) # Compute Loss\\r\\n        loss.backward() # Backward pass\\r\\n        optimizer.step() # Update parameters\\r\\n        print(f\\"Epoch: {i}/{epochs}, Loss: {loss.item()}\\")\\r\\n        losses.append(loss.item())\\r\\n```\\r\\n\\r\\nFinal loss: 0.0001777520083123818\\r\\nPlot the loss graph:\\r\\n\\r\\n```python\\r\\nplt.plot(losses)\\r\\nplt.show() # Plot the loss graph\\r\\n```\\r\\n\\r\\nLoss Graph:\\r\\n\\r\\n![Loss Graph](./result.png)\\r\\n\\r\\nTest the model:\\r\\n\\r\\n```python\\r\\nfor name, param in model.named_parameters():\\r\\n    print(name, param) # Inspect the weight and bias\\r\\n\\r\\nn = 100 # input x\\r\\nx = torch.tensor([n], dtype=torch.float32) # convert to tensor\\r\\ny = model(x) # Forward pass\\r\\nprint(y)\\r\\n```\\r\\n\\r\\n### Follow-up Tasks\\r\\n\\r\\n1. Experiment with different learning rates and epochs and see how it affects the training result.\\r\\n2. Enlarge the dataset (lengthen the x tensor), does it improve the model?\\r\\n3. Experiment with a more complex function than y=3x+1 such as y=x^2. Does it work? If it doesn\u2019t work, how can you solve this issue?\\r\\n4. How can you make the training faster without changing the hardware, data and the model?\\r\\n\\r\\nFeel free to send me an email for your solutions to the above problems!"},{"id":"website-launch","metadata":{"permalink":"/blog/website-launch","source":"@site/blog/2023-10-06-website-launch/index.md","title":"Website Launch!","description":"Website Launch","date":"2023-10-06T00:00:00.000Z","tags":[{"label":"hola","permalink":"/blog/tags/hola"},{"label":"tired","permalink":"/blog/tags/tired"}],"readingTime":0.05,"hasTruncateMarker":false,"authors":[{"name":"Eason Xie","title":"Website Owner","url":"https://easonoob.github.io","imageURL":"https://avatars.githubusercontent.com/u/100521878?v=4","key":"eason"}],"frontMatter":{"slug":"website-launch","title":"Website Launch!","authors":"eason","tags":["hola","tired"]},"unlisted":false,"prevItem":{"title":"The Most Basics of Neural Networks","permalink":"/blog/basic-of-nn"}},"content":"![Website Launch](./website-launch.jpg)\\r\\n\\r\\nThe website is launched today at 00:00 UTC+0!"}]}')}}]);