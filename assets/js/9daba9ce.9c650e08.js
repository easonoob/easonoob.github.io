"use strict";(self.webpackChunkdocusaurus_site=self.webpackChunkdocusaurus_site||[]).push([[2871],{6076:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>h,frontMatter:()=>i,metadata:()=>s,toc:()=>d});var t=r(4848),o=r(8453);const i={slug:"intro-to-pytorch",title:"Introduction to PyTorch",authors:"eason",tags:["beginners","nn","pytorch","tutorial","tired"]},a=void 0,s={permalink:"/blog/intro-to-pytorch",source:"@site/blog/2023-12-13-intro-to-pytorch/index.md",title:"Introduction to PyTorch",description:"This blog post will be about some basic introductions to PyTorch, including tensors, and how to train your own model in PyTorch.",date:"2023-12-13T00:00:00.000Z",tags:[{label:"beginners",permalink:"/blog/tags/beginners"},{label:"nn",permalink:"/blog/tags/nn"},{label:"pytorch",permalink:"/blog/tags/pytorch"},{label:"tutorial",permalink:"/blog/tags/tutorial"},{label:"tired",permalink:"/blog/tags/tired"}],readingTime:5.835,hasTruncateMarker:!0,authors:[{name:"Eason Xie",title:"Website Owner",url:"https://easonoob.github.io",imageURL:"https://avatars.githubusercontent.com/u/100521878?v=4",key:"eason"}],frontMatter:{slug:"intro-to-pytorch",title:"Introduction to PyTorch",authors:"eason",tags:["beginners","nn","pytorch","tutorial","tired"]},unlisted:!1,prevItem:{title:"Introduction to Quantum Computing",permalink:"/blog/intro-to-quantum-computing"},nextItem:{title:"The Most Basics of Neural Networks",permalink:"/blog/basic-of-nn"}},l={authorsImageUrls:[void 0]},d=[{value:"Prerequisites",id:"prerequisites",level:2},{value:"Aims",id:"aims",level:2},{value:"Introduction",id:"introduction",level:2},{value:"Tensors",id:"tensors",level:2},{value:"Training a Simple Model in PyTorch",id:"training-a-simple-model-in-pytorch",level:2},{value:"Finding and defining your dataset",id:"finding-and-defining-your-dataset",level:3},{value:"Create the model",id:"create-the-model",level:3},{value:"Define the Loss Function and the Optimizer",id:"define-the-loss-function-and-the-optimizer",level:3},{value:"Define the training loop",id:"define-the-training-loop",level:3},{value:"Tasks",id:"tasks",level:2}];function c(e){const n={a:"a",blockquote:"blockquote",code:"code",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",pre:"pre",ul:"ul",...(0,o.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.p,{children:"This blog post will be about some basic introductions to PyTorch, including tensors, and how to train your own model in PyTorch."}),"\n",(0,t.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Basic Maths"}),"\n",(0,t.jsx)(n.li,{children:"Basic Python"}),"\n",(0,t.jsx)(n.li,{children:"Common Sense"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"aims",children:"Aims"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Understand tensors"}),"\n",(0,t.jsx)(n.li,{children:"Understand the steps to build and train a model in PyTorch"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,t.jsxs)(n.blockquote,{children:["\n",(0,t.jsx)(n.p,{children:"\u201cArtificial Intelligence, deep learning, machine learning \u2014 whatever you're doing if you don't understand it \u2014 learn it. Because otherwise you're going to be a dinosaur within 3 years.\u201d ~ Mark Cuban."}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"tensors",children:"Tensors"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.a,{href:"https://pytorch.org/tutorials/beginner/basics/tensorqs_tutorial.html",children:"https://pytorch.org/tutorials/beginner/basics/tensorqs_tutorial.html"})}),"\n",(0,t.jsx)(n.p,{children:"Tensors are specialized arrays or matrices. It can have as many dimensions as you want. We use tensors to encode the data in the PyTorch, as well as the model parameters. Specifically, if you perform operations on a PyTorch tensor (e.g. addition, multiplication), your action will be saved into PyTorch\u2019s built-in dynamic computation graph, which is necessary for model backpropagation in training."}),"\n",(0,t.jsx)(n.p,{children:"Here\u2019s an example of some tensors:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"import torch\r\n\r\n# Create a tensor from a list/array:\r\n\r\ndata1 = [0, 1, 2, 3, 4, 5] # 1D array\r\ndata2 = [[0, 1], [2, 3], [4, 5]] # 2D array\r\n\r\ntensor1 = torch.tensor(data1)\r\ntensor2 = torch.tensor(data2)\r\n\r\nprint(tensor1.shape) # torch.Size([6])\r\nprint(tensor2.shape) # torch.Size([3, 2])\r\n\r\n# Create a tensor filled with zeros or ones:\r\n\r\nzeros_tensor = torch.zeros(3, 5, 4, 2, 1, 4) # A 6D tensor\r\nones_tensor = torch.ones(3, 5)\r\n\r\n# Create a random tensor from a specified shape:\r\n\r\nrandom_tensor1 = torch.randn(10, 20) # A 10 by 20 tensor with normal distribution\r\nprint(random_tensor1.shape) # torch.Size([10, 20])\r\nprint(random_tensor1[0, :10]) # You can use python array index slicing\r\n\r\nrandom_tensor2 = torch.randint(0, 100, (5, 10)) # 5 by 10 tensor with integer values ranging from 0 to 100\r\nprint(random_tensor2[0, 3:10])\r\n\r\n# Attributes of a tensor:\r\n\r\nprint(tensor1.shape)\r\nprint(tensor1.dtype) # data type, e.g. float32, int64, float64, Bfloat16\r\nprint(tensor1.device) # device the tensor is stored on, e.g. cpu, cuda\n"})}),"\n",(0,t.jsx)(n.h2,{id:"training-a-simple-model-in-pytorch",children:"Training a Simple Model in PyTorch"}),"\n",(0,t.jsx)(n.p,{children:"The steps of training a model in PyTorch includes:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Find a dataset that suits your problem, download it, and create a dataloader."}),"\n",(0,t.jsxs)(n.li,{children:["Define your model, you can create your own model module using ",(0,t.jsx)(n.code,{children:"nn.Module"}),"."]}),"\n",(0,t.jsxs)(n.li,{children:["Define your loss function (from ",(0,t.jsx)(n.code,{children:"torch.nn"}),"), optimizers (from ",(0,t.jsx)(n.code,{children:"torch.optim"}),"), etc."]}),"\n",(0,t.jsx)(n.li,{children:"Define your training loop, it can be a function that does a single step and write a loop, or just simply a training loop."}),"\n",(0,t.jsx)(n.li,{children:"Start training!"}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"For this example, we are going to use the MNIST dataset, which is built-in in PyTorch, making it very easy to download and use."}),"\n",(0,t.jsx)(n.p,{children:"First import the necessary libraries:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"import torch\r\nfrom torch import nn\r\nfrom torch.utils.data import DataLoader\r\nfrom torchvision import datasets\r\nfrom torchvision.transforms import ToTensor\n"})}),"\n",(0,t.jsx)(n.h3,{id:"finding-and-defining-your-dataset",children:"Finding and defining your dataset"}),"\n",(0,t.jsx)(n.p,{children:"PyTorch offers domain-specific libraries such as TorchText, TorchVision, and TorchAudio, all of which include datasets. For this tutorial, we will be using a TorchVision dataset."}),"\n",(0,t.jsxs)(n.p,{children:["The ",(0,t.jsx)(n.code,{children:"torchvision.datasets"})," module contains ",(0,t.jsx)(n.code,{children:"Dataset"})," objects for many real-world vision data like CIFAR, COCO. In this tutorial, we use the FashionMNIST dataset. Every TorchVision ",(0,t.jsx)(n.code,{children:"Dataset"})," includes two arguments: ",(0,t.jsx)(n.code,{children:"transform"})," and ",(0,t.jsx)(n.code,{children:"target_transform"})," to modify the samples and labels respectively."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# Download training data from the MNIST dataset.\r\ntraining_data = datasets.FashionMNIST(\r\n    root="data",\r\n    train=True,\r\n    download=True,\r\n    transform=ToTensor(), # Convert to PyTorch tensor.\r\n)\r\n\r\n# Download test data from the MNIST dataset.\r\ntest_data = datasets.FashionMNIST(\r\n    root="data",\r\n    train=False,\r\n    download=True,\r\n    transform=ToTensor(),\r\n)\n'})}),"\n",(0,t.jsxs)(n.p,{children:["We pass the ",(0,t.jsx)(n.code,{children:"Dataset"})," as an argument to ",(0,t.jsx)(n.code,{children:"DataLoader"}),". This wraps an iterable over our dataset, and supports automatic batching, sampling, shuffling and multiprocess data loading. Here we define a batch size of 64, i.e. each element in the dataloader iterable will return a batch of 64 features and labels."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'batch_size = 64\r\n\r\n# Create data loaders.\r\ntrain_dataloader = DataLoader(training_data, batch_size=batch_size)\r\ntest_dataloader = DataLoader(test_data, batch_size=batch_size)\r\n\r\nfor X, y in test_dataloader:\r\n    print(f"Shape of X [N, C, H, W]: {X.shape}") # N, C, H, W stands for Batch Size, Channel Size, Height, and Width\r\n    print(f"Shape of y: {y.shape} {y.dtype}")\r\n    break\n'})}),"\n",(0,t.jsx)(n.h3,{id:"create-the-model",children:"Create the model"}),"\n",(0,t.jsxs)(n.p,{children:["To define a neural network in PyTorch, we create a class that inherits from nn.Module. We define the layers of the network in the ",(0,t.jsx)(n.code,{children:"__init__"})," function and specify how data will pass through the network in the forward function. To accelerate operations in the neural network, we move it to the GPU (",(0,t.jsx)(n.code,{children:"cuda"}),") or MPS if available."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# Get cpu, gpu (cuda) device for training.\r\ndevice = (\r\n    "cuda" if torch.cuda.is_available() else "cpu"\r\n)\r\nprint(f"Using {device} device")\r\n\r\n# Define model\r\nclass NeuralNetwork(nn.Module):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.flatten = nn.Flatten() # Flattens to (Batch Size, Channel Size * Height * Width), from a 4D tensor to a 2D tensor\r\n        self.linear_relu_stack = nn.Sequential(\r\n            nn.Linear(28*28, 512), # Fully-connected hidden layer\r\n            nn.ReLU(), # Activation Function\r\n            nn.Linear(512, 512),\r\n            nn.ReLU(),\r\n            nn.Linear(512, 10)\r\n        ) # When `linear_relu_stack` is called, it will run all the modules inside in order.\r\n\r\n    def forward(self, x):\r\n        x = self.flatten(x) # flatten to 2D, same as x.view(x.size(0), -1)\r\n        logits = self.linear_relu_stack(x) # the nn.Sequential instance\r\n        return logits\r\n\r\n# Create an instance\r\nmodel = NeuralNetwork().to(device)\r\nprint(model)\n'})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.a,{href:"https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html",children:"Learn More About Building Neural Networks in PyTorch Here"})}),"\n",(0,t.jsx)(n.h3,{id:"define-the-loss-function-and-the-optimizer",children:"Define the Loss Function and the Optimizer"}),"\n",(0,t.jsx)(n.p,{children:"We are using the Cross Entropy Loss loss function and the Stochastic Gradient Descent (SGD) optimizer for training this model."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"loss_fn = nn.CrossEntropyLoss()\r\noptimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n"})}),"\n",(0,t.jsx)(n.h3,{id:"define-the-training-loop",children:"Define the training loop"}),"\n",(0,t.jsx)(n.p,{children:"First we define the function to train a single step:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'def train(dataloader, model, loss_fn, optimizer):\r\n    size = len(dataloader.dataset)\r\n    model.train()\r\n    for batch, (X, y) in enumerate(dataloader):\r\n        X, y = X.to(device), y.to(device) # Move tensors to cuda if available\r\n\r\n        # Compute prediction error\r\n        pred = model(X) # Forward pass\r\n        loss = loss_fn(pred, y) # Compute loss\r\n\r\n        # Backpropagation\r\n        loss.backward() # Compute gradients\r\n        optimizer.step() # Update parameters\r\n        optimizer.zero_grad() # Zero the gradients\r\n\r\n        if batch % 100 == 0:\r\n            loss, current = loss.item(), (batch + 1) * len(X)\r\n            print(f"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]")\n'})}),"\n",(0,t.jsx)(n.p,{children:"Then a function to evaluate (or validate). The goal of this is to check the model\u2019s performance against the test dataset to ensure it is learning, and monitor if it is overfitting."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'def test(dataloader, model, loss_fn):\r\n    size = len(dataloader.dataset)\r\n    num_batches = len(dataloader)\r\n    model.eval()\r\n    test_loss, correct = 0, 0\r\n    with torch.no_grad():\r\n        for X, y in dataloader:\r\n            X, y = X.to(device), y.to(device)\r\n            pred = model(X)\r\n            test_loss += loss_fn(pred, y).item()\r\n            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\r\n    test_loss /= num_batches\r\n    correct /= size\r\n    print(f"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n")\n'})}),"\n",(0,t.jsx)(n.p,{children:"The training process is conducted over several iterations (epochs) that go over the entire training dataset. During each epoch, the model learns parameters to make better predictions. We print the model\u2019s accuracy and loss at each epoch; we\u2019d like to see the accuracy increase and the loss decrease with every epoch."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'epochs = 5\r\nfor t in range(epochs):\r\n    print(f"Epoch {t+1}\\n-------------------------------")\r\n    train(train_dataloader, model, loss_fn, optimizer)\r\n    test(test_dataloader, model, loss_fn)\r\nprint("Done!")\n'})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.a,{href:"https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html",children:"Read More About Model Training Here"})}),"\n",(0,t.jsx)(n.h2,{id:"tasks",children:"Tasks"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Plot some graphs to show the training loss, validation loss, and validation accuracy over step."}),"\n",(0,t.jsx)(n.li,{children:"Experiment with different learning rate, batch size, and model architecture, how does it affect the results?"}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(c,{...e})}):c(e)}},8453:(e,n,r)=>{r.d(n,{R:()=>a,x:()=>s});var t=r(6540);const o={},i=t.createContext(o);function a(e){const n=t.useContext(i);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:a(e.components),t.createElement(i.Provider,{value:n},e.children)}}}]);